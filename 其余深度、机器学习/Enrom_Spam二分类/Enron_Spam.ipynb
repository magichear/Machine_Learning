{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3nus_A8CLQWX",
        "outputId": "aeed8105-3526-4b60-fae8-0cf67ffe65d9"
      },
      "outputs": [],
      "source": [
        "%pip install spacy\n",
        "%pip install psutil\n",
        "!python -m spacy download en_core_web_sm\n",
        "\n",
        "# llama3.2-1B有点太大了，这个才12.8MB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "u3R328gALQWY"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import json\n",
        "import math\n",
        "import torch\n",
        "import spacy\n",
        "import psutil\n",
        "import itertools\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from collections import Counter\n",
        "\n",
        "# 数据集命名统一\n",
        "TEXT_PROCESSED = \"processed_text\"\n",
        "LABEL = \"label\"\n",
        "TEXT_VECTORIZATION = \"vectorization_text\"\n",
        "TEXT_VECTORIZATION_PADDING = \"padding_vectorization_text\"\n",
        "FILE_PATH_WTI = \"word_to_idx.json\"\n",
        "SENTENCE_LENGTH = 200"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_N6zoiPxuYN_"
      },
      "source": [
        "#### 云端训练解包"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SYm5gma8uYOD",
        "outputId": "446a8eb9-a57c-470d-c0f0-bf0d5c3419c2"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import shutil\n",
        "drive.mount('/content/drive/')\n",
        "srcA = '/content/drive/MyDrive/sdxxdl_colab/hw2/processed_data_useful.json'\n",
        "srcB = '/content/drive/MyDrive/sdxxdl_colab/hw2/word_to_idx.json'\n",
        "srcC = '/content/drive/MyDrive/sdxxdl_colab/hw2/processed_text.zip'\n",
        "\n",
        "destA = '/content/processed_data_useful.json'\n",
        "destB = '/content/word_to_idx.json'\n",
        "destC = '/content/processed_text.zip'\n",
        "\n",
        "# 移动文件\n",
        "shutil.copy(srcA, destA)\n",
        "shutil.copy(srcB, destB)\n",
        "shutil.copy(srcC, destC)\n",
        "!unzip processed_text.zip\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aSvo-UovuYOD",
        "outputId": "fa9fa166-7928-4021-f315-7158a341b923"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "cloud_directory = '/content/drive/MyDrive/sdxxdl_colab/hw2/models'\n",
        "\n",
        "os.makedirs(cloud_directory, exist_ok=True)\n",
        "\n",
        "for item in os.listdir('/content'):\n",
        "    item_path = os.path.join('/content', item)\n",
        "\n",
        "    if item in ['sample_data', 'drive', 'processed_data_useful.zip', '.config']:\n",
        "        continue\n",
        "\n",
        "    shutil.copy(item_path, os.path.join(cloud_directory, item))\n",
        "\n",
        "print(\"Done\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4brB2PorLQWY",
        "outputId": "72046ade-347f-4698-a142-908b74b9edd7"
      },
      "outputs": [],
      "source": [
        "# 加载数据集\n",
        "df = (\n",
        "    pd.read_csv(\"enron_spam_data.csv\")\n",
        "    .drop(columns=[\"Date\"], axis=1)\n",
        "    .rename(columns={\"Spam/Ham\": \"label\"})\n",
        ")\n",
        "df.dropna(inplace=True)\n",
        "df[\"text\"] = df[\"Subject\"] + \" \" + df[\"Message\"]\n",
        "df = df[[\"text\", \"label\"]]\n",
        "df[\"label\"] = df[\"label\"].map({\"ham\": 0, \"spam\": 1})\n",
        "print(\"数据集样例:\")\n",
        "print(df.head())\n",
        "print(\"数据集大小:\", df.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F20Z5pX7LQWZ"
      },
      "source": [
        "#### 分词\n",
        "\n",
        "~~考虑~~使用~~nltk或~~spaCy进行分词\n",
        "\n",
        "am和pm感觉还是保留一下，有可能垃圾邮件发过来的时间也是一个重要判据（比如可能晚上发更多？）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ihXaz-Q4LQWb"
      },
      "outputs": [],
      "source": [
        "# 这一步耗时很长\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "def spacy_tokenizer(text):\n",
        "    doc = nlp(text.lower())  # 转为小写并分词\n",
        "    # 去除停用词和标点符号\n",
        "    tokens = [\n",
        "        token.text\n",
        "        for token in doc\n",
        "        if not token.is_stop and not token.is_punct and len(token.text) > 1\n",
        "    ]\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "\n",
        "# 应用spaCy分词器到文本列\n",
        "df[TEXT_PROCESSED] = df[\"text\"].apply(spacy_tokenizer)\n",
        "df[[TEXT_PROCESSED, LABEL]].to_csv(\n",
        "    \"processed_text.csv\", index=False, encoding=\"utf-8\"\n",
        ")\n",
        "print(\"分词结果已保存为 processed_text.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bKOURhWfLQWc",
        "outputId": "23f8a7a7-7f91-46aa-af1b-29ee8e77cb53"
      },
      "outputs": [],
      "source": [
        "# 提取预处理好的文本数据\n",
        "df_processed = pd.read_csv(\"processed_text.csv\", encoding=\"utf-8\")\n",
        "print(\"是否存在 NaN 值:\", df_processed[TEXT_PROCESSED].isna().any())\n",
        "\n",
        "# 忽略空值\n",
        "df_processed[TEXT_PROCESSED] = df_processed[TEXT_PROCESSED].fillna(\"\")\n",
        "print(df_processed.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KGGRbFUDLQWc"
      },
      "source": [
        "~~数字太多了，对词表影响好大，测试了前面几个出来几乎全是`unknown`，以及先练练模型，看看后续是否需要增大词表到两万~~\n",
        "\n",
        "一开始把100_000看成了10_000，但也能说明这个对词表的影响不小"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Ket31bBLQWc",
        "outputId": "54ea1705-522c-404a-eabc-f371accec152"
      },
      "outputs": [],
      "source": [
        "PAD = \"<pad>\"\n",
        "UNK = \"<unk>\"\n",
        "\n",
        "# 将所有分词结果合并为一个大字符串（空格分隔），然后一次性分割成单词列表\n",
        "all_tokens = list(\n",
        "    itertools.chain.from_iterable(df_processed[TEXT_PROCESSED].str.split())\n",
        ")\n",
        "\n",
        "# all_tokens = [token for token in all_tokens if not token.isdigit()]  # 去除数字\n",
        "\n",
        "unique_tokens = set(all_tokens)\n",
        "print(f\"无重复的最大词表大小为: {len(unique_tokens)}\")\n",
        "\n",
        "# 统计词频\n",
        "word_counts = Counter(all_tokens)   # Counter 本身会去重\n",
        "vocab = [word for word, _ in word_counts.most_common(100000)]\n",
        "\n",
        "# 添加特殊标记（为填充部分与非词表单词准备）\n",
        "vocab = [PAD, UNK] + vocab\n",
        "word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
        "\n",
        "with open(FILE_PATH_WTI, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(word_to_idx, f, ensure_ascii=False, indent=4)\n",
        "    print(f\"词表已保存到 {FILE_PATH_WTI}\")\n",
        "\n",
        "print(\"[Done] Build vocabulary\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xiGJK1X0LQWd"
      },
      "source": [
        "### 文本向量化\n",
        "\n",
        "需要截断过长的句子，为过短的句子从左边填充直到阈值\n",
        "\n",
        "根据文档提示，这里阈值设定为`200`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W1kxAxI2LQWd",
        "outputId": "e5eb96f8-056a-4269-edd5-1fe2773ea17e"
      },
      "outputs": [],
      "source": [
        "# 把分词结果借助词表转换为数字序列\n",
        "df_processed[TEXT_VECTORIZATION] = df_processed[TEXT_PROCESSED].apply(\n",
        "    lambda x: [word_to_idx.get(token, word_to_idx[UNK]) for token in x.split()]\n",
        ")\n",
        "\n",
        "# 从左到右截，前面的部分可能包含主题等重要内容\n",
        "def pad_sequence(indices):\n",
        "    if len(indices) > SENTENCE_LENGTH:\n",
        "        return indices[:SENTENCE_LENGTH]\n",
        "    else:\n",
        "        return [word_to_idx[PAD]] * (SENTENCE_LENGTH - len(indices)) + indices\n",
        "\n",
        "\n",
        "df_processed[TEXT_VECTORIZATION_PADDING] = df_processed[TEXT_VECTORIZATION].apply(pad_sequence)\n",
        "print(df_processed.head())\n",
        "\n",
        "\n",
        "# 缓存一下\n",
        "df_processed.to_csv(\"processed_data.csv\", index=False, encoding=\"utf-8\")\n",
        "print(\"处理后的数据已保存为 processed_data.csv\")\n",
        "\n",
        "# 仅存储 LABEL 和 TEXT_VECTORIZATION_PADDING 两列\n",
        "df_useful = df_processed[[LABEL, TEXT_VECTORIZATION_PADDING]]\n",
        "\n",
        "nested_data = df_useful.apply(\n",
        "    lambda row: {\n",
        "        \"label\": row[\"label\"],\n",
        "        \"text\": row[TEXT_VECTORIZATION_PADDING],\n",
        "    },\n",
        "    axis=1,\n",
        ").tolist()\n",
        "\n",
        "with open(\"processed_data_useful.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(nested_data, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "print(\"有效数据已保存 processed_data_useful.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gIBWmhFHLQWe",
        "outputId": "3a237c91-360e-465c-c930-bbad5648b16b"
      },
      "outputs": [],
      "source": [
        "with open(FILE_PATH_WTI, \"r\", encoding=\"utf-8\") as f:\n",
        "    word_to_idx = json.load(f)\n",
        "    print(f\"词表已从 {FILE_PATH_WTI} 加载\")\n",
        "\n",
        "with open(\"processed_data_useful.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    nested_data = json.load(f)\n",
        "\n",
        "# 还原为 DataFrame（这里只要顶层两个字段对就可以了）\n",
        "df_loaded = pd.DataFrame(\n",
        "    [\n",
        "        {\n",
        "            \"text\": item[\"text\"],\n",
        "            \"label\": item[\"label\"],\n",
        "        }\n",
        "        for item in nested_data\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(df_loaded.head())\n",
        "# 打乱 df_loaded 的行\n",
        "df_shuffled = df_loaded.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "print(df_shuffled.head())\n",
        "\n",
        "\n",
        "train_df, test_df = train_test_split(df_shuffled, test_size=0.2, random_state=42)\n",
        "train_df, val_df = train_test_split(train_df, test_size=0.1, random_state=42)\n",
        "\n",
        "print(f\"训练集大小: {len(train_df)}\")\n",
        "print(f\"验证集大小: {len(val_df)}\")\n",
        "print(f\"测试集大小: {len(test_df)}\")\n",
        "\n",
        "train_dataset = TensorDataset(\n",
        "    torch.LongTensor(train_df[\"text\"].tolist()),\n",
        "    torch.LongTensor(train_df[\"label\"].tolist()),\n",
        ")\n",
        "\n",
        "val_dataset = TensorDataset(\n",
        "    torch.LongTensor(val_df[\"text\"].tolist()),\n",
        "    torch.LongTensor(val_df[\"label\"].tolist()),\n",
        ")\n",
        "\n",
        "test_dataset = TensorDataset(\n",
        "    torch.LongTensor(test_df[\"text\"].tolist()),\n",
        "    torch.LongTensor(test_df[\"label\"].tolist()),\n",
        ")\n",
        "\n",
        "BATCH_SIZE = 256\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, num_workers=2,pin_memory=True, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE,num_workers=2, pin_memory=True, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IW0CAalcLQWe"
      },
      "source": [
        "## 模型结构"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7sIasdCRLQWe"
      },
      "source": [
        "位置编码的核心目的是**向模型注入序列中元素的位置信息**，使模型能够感知单词在句子中的相对或绝对位置，这对处理具有顺序依赖的序列数据（如文本）至关重要。以最常见的**正弦位置编码**为例，其原理如下：  \n",
        "- 对于一个长度为 $ L $、维度为 $ d_{\\text{model}} $ 的序列，位置编码为每个位置 $ i $ 生成一个 $ d_{\\text{model}} $ 维的向量。  \n",
        "- 利用三角函数的周期性，对偶数维度使用正弦函数，奇数维度使用余弦函数，公式为：  \n",
        "$$\n",
        "\\begin{aligned}\n",
        "PE_{(i, 2j)} &= \\sin\\left(\\frac{i}{10000^{2j / d_{\\text{model}}}}\\right) \\\\\n",
        "PE_{(i, 2j+1)} &= \\cos\\left(\\frac{i}{10000^{2j / d_{\\text{model}}}}\\right)\n",
        "\\end{aligned}\n",
        "$$  \n",
        "其中 $ i $ 是位置（如第 $ i $ 个词），$ j $ 是维度索引。这种编码方式的优势在于：  \n",
        "- 不同位置的编码在高维空间中具有唯一性，且能保持相对位置关系（如位置 $ i $ 和 $ i+k $ 的编码差异仅与 $ k $ 有关）。  \n",
        "- 三角函数的平滑性使模型易于学习和泛化。\n",
        "\n",
        "---\n",
        "\n",
        "RoPE的关键是对Q和K向量进行**位置相关的旋转变换**，使相对位置信息嵌入到点积计算中。具体步骤如下：  \n",
        "\n",
        "将每个维度的实向量表示为复数的实部和虚部：  \n",
        "- 对于位置 $n$ 和维度 $2m$（偶数维度），对应复数的虚部为 $\\sin(\\theta_{m,n})$；  \n",
        "- 维度 $2m+1$（奇数维度）对应实部为 $\\cos(\\theta_{m,n})$，其中频率 $\\theta_{m,n} = n \\cdot \\omega_m$，$\\omega_m = 1/10000^{2m/d_{\\text{model}}}$ 是预定义的频率参数。  \n",
        "\n",
        "对于位置 $n$ 和 $n+k$，相对位置为 $k$。RoPE通过旋转矩阵对Q和K进行变换，使得：  \n",
        "- 当计算 $Q_n$ 与 $K_{n+k}$ 的点积时，结果隐式包含 $k$ 的信息。  \n",
        "数学上，对向量 $v = [v_1, v_2, v_3, v_4, \\dots, v_{d-1}, v_d]$（偶数维度，两两分组），位置 $n$ 的旋转操作可表示为：  \n",
        "$$\n",
        "\\text{rotate}(v, n) = \\begin{bmatrix}\n",
        "v_1 \\cos(\\theta_m) - v_2 \\sin(\\theta_m) \\\\\n",
        "v_1 \\sin(\\theta_m) + v_2 \\cos(\\theta_m) \\\\\n",
        "v_3 \\cos(\\theta_{m+1}) - v_4 \\sin(\\theta_{m+1}) \\\\\n",
        "v_3 \\sin(\\theta_{m+1}) + v_4 \\cos(\\theta_{m+1}) \\\\\n",
        "\\vdots\n",
        "\\end{bmatrix},\n",
        "$$  \n",
        "其中每组维度 $(2m, 2m+1)$ 对应频率 $\\theta_m = n \\cdot \\omega_m$。\n",
        "\n",
        "`RoPE`更适合处理长序列，而邮件通常来说都是短文本，本次还都做了截断，所以经典位置编码的效果和计算效率相对高一些是可以接受的\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPqMGUdpLQWe"
      },
      "source": [
        "#### Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "b8tKChdYLQWe"
      },
      "outputs": [],
      "source": [
        "# 仿照原理实现的原始位置编码\n",
        "class SimplePE(nn.Module):\n",
        "    def __init__(self, d_model, max_len=1500):  # 这里max_len对效率影响不大，由于下面需要比较不同截断长度，这里设置大一点\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)  # 初始化一个形状为 [max_len, d_model] 的全零张量\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)  # 这里表示每个位置的索引\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        param = position * div_term\n",
        "        pe[:, 0::2] = torch.sin(param)  # 所有行，从第一列开始每隔两列取值求sin/cos\n",
        "        pe[:, 1::2] = torch.cos(param)\n",
        "        self.register_buffer(\"pe\", pe.unsqueeze(0))  # 在初始化时直接扩展维度\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.pe[:, :x.size(1), :]\n",
        "\n",
        "# RoPE位置编码（d_model必须为偶数）\n",
        "class RoPE(nn.Module):\n",
        "    def __init__(self, d_model):\n",
        "        super().__init__()\n",
        "        inv_freq = 1.0 / (10000 ** (torch.arange(0, d_model, 2).float() / d_model))\n",
        "        self.register_buffer(\"inv_freq\", inv_freq)\n",
        "\n",
        "    def forward(self, query, key):\n",
        "        seq_len = query.size(1)\n",
        "        pos = torch.arange(seq_len, device=query.device).unsqueeze(1)  # [seq_len, 1]\n",
        "\n",
        "        sin_inp = torch.einsum(\n",
        "            \"i,j->ij\", pos.squeeze(1).float(), self.inv_freq  # [seq_len, d_model//2]\n",
        "        )\n",
        "        emb_sin = torch.sin(sin_inp)  # [seq_len, d_model//2]\n",
        "        emb_cos = torch.cos(sin_inp)  # [seq_len, d_model//2]\n",
        "\n",
        "        # 分奇偶处理\n",
        "        query1, query2 = query[..., ::2], query[..., 1::2]\n",
        "        key1, key2 = key[..., ::2], key[..., 1::2]\n",
        "\n",
        "        query_rotated = torch.cat(\n",
        "            [query1 * emb_cos - query2 * emb_sin, query1 * emb_sin + query2 * emb_cos],\n",
        "            dim=-1,\n",
        "        )\n",
        "        key_rotated = torch.cat(\n",
        "            [key1 * emb_cos - key2 * emb_sin, key1 * emb_sin + key2 * emb_cos], dim=-1\n",
        "        )\n",
        "\n",
        "        return query_rotated, key_rotated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "zYxg1DWNLQWe"
      },
      "outputs": [],
      "source": [
        "class AttentionModel(nn.Module):\n",
        "    def __init__(\n",
        "        self, vocab_size, d_model, nhead, mha_cnt=1, dropout=0.5, pos_encoding_type=\"original\", output_dim=1\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model, padding_idx=0)  # 0对应<pad>，计算时候会忽略掉这些填充块\n",
        "        self.pos_encoding_type = pos_encoding_type\n",
        "\n",
        "        if pos_encoding_type == \"original\":\n",
        "            self.pos_encoder = SimplePE(d_model)\n",
        "        else:\n",
        "            self.pos_encoder = RoPE(d_model)\n",
        "\n",
        "        # 下面传值的时候就可以直接调用其forward\n",
        "        self.mhas = nn.ModuleList([\n",
        "            nn.MultiheadAttention(\n",
        "                embed_dim=d_model,\n",
        "                num_heads=nhead,\n",
        "                dropout=dropout,\n",
        "                batch_first=True,  # 把batch放在第一，保持输入形状为[batch_size, seq_len, d_model]\n",
        "            )\n",
        "            for _ in range(mha_cnt)\n",
        "        ])\n",
        "        \"\"\"\n",
        "        def forward(\n",
        "            self,\n",
        "            query: Tensor,\n",
        "            key: Tensor,\n",
        "            value: Tensor,\n",
        "            key_padding_mask: Optional[Tensor] = None,  <-- 上面embedding已经处理过了\n",
        "            need_weights: bool = True,\n",
        "            attn_mask: Optional[Tensor] = None,\n",
        "            average_attn_weights: bool = True,\n",
        "            is_causal: bool = False,\n",
        "        \"\"\"\n",
        "        self.fc = nn.Linear(d_model, output_dim) # 将d_model映射到输出维度\n",
        "        self.classifier = lambda x: self.fc(x).squeeze()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: [batch_size, seq_len]\n",
        "        x = self.embedding(x)  # [batch_size, seq_len, d_model]\n",
        "\n",
        "        # 位置编码\n",
        "        if self.pos_encoding_type == \"original\":\n",
        "            x = x + self.pos_encoder(x)  # [batch_size, seq_len, d_model]\n",
        "        else:\n",
        "            x, _ = self.pos_encoder(x, x)  # [1, seq_len, d_model]\n",
        "\n",
        "        attn_mask = self.__maskGen(x.size(1), device=x.device)  # 下三角掩码\n",
        "        # 只拿输出，忽略权重（这次如果做可视化分析才需要）\n",
        "        for mha in self.mhas: # 网上说直接叠放是经典transformer设计，以(pe,mha,fc)为一组叠放更像GPT变体\n",
        "            # 这里直接叠放，保持计算效率的同时避免过拟合（因为单层的效果就已经很好了）\n",
        "            x, _ = mha(query=x, key=x, value=x, attn_mask=attn_mask)  # [batch_size, seq_len, d_model]\n",
        "\n",
        "        # 取最后一个隐藏层的输出作为分类依据（包含所有时间步的信息，由于mha还会将序列信息聚合到时间步中，所以这里还附带了序列的全部上下文）\n",
        "        x = x[:, -1, :]  # [batch_size, d_model]\n",
        "        '''\n",
        "        torch.Size([2, 3, 4])\n",
        "        原始张量 x:\n",
        "         tensor([[[ 1.9884,  1.3547, -1.0178, -0.6918],\n",
        "                 [-0.0031,  0.3837, -0.2284,  1.5879],\n",
        "                 [-0.3995, -1.3488, -1.6228,  0.5332]],\n",
        "\n",
        "                [[-0.8753, -0.3659,  0.9125,  0.4637],\n",
        "                 [-0.7732,  0.5641, -0.2950, -0.4699],\n",
        "                 [-0.9908, -0.6429, -0.9626,  0.5282]]])\n",
        "        torch.Size([2, 4])\n",
        "        切片后的张量 x_sliced, 每一批都取最后一层的输出:\n",
        "         tensor([[-0.3995, -1.3488, -1.6228,  0.5332],\n",
        "                [-0.9908, -0.6429, -0.9626,  0.5282]])\n",
        "        '''\n",
        "\n",
        "        # 分类器\n",
        "        logits = self.classifier(x)  # [batch_size]\n",
        "        return logits, None # 多的返回值是注意力分析任务的史山\n",
        "\n",
        "    @staticmethod\n",
        "    def __maskGen(size, device=None):\n",
        "        \"\"\"生成下三角注意力掩码，确保位置编码不会看到未来的信息\"\"\"\n",
        "        mask = (torch.triu(torch.ones(size, size)) == 1).transpose(0, 1)    # 生成全1上三角并转置\n",
        "        mask = (\n",
        "            mask.float()\n",
        "            .masked_fill(mask == 0, float(\"-inf\"))  # 根据文档描述将上三角部分填充为负无穷\n",
        "            .masked_fill(mask == 1, float(0.0))\n",
        "        )\n",
        "        if device is not None:\n",
        "            mask = mask.to(device)  # 将掩码移动到指定设备\n",
        "        return mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IsH2fdxJLQWe"
      },
      "source": [
        "#### RNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "vNFpkmYaLQWe"
      },
      "outputs": [],
      "source": [
        "class RNNModel(nn.Module):\n",
        "    \"\"\"循环计算，不需要位置编码\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self, vocab_size, embed_dim, hidden_dim, num_layers, output_dim, rnn_type=\"RNN\", bidirectional=False, dropout=0.5\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "\n",
        "        # 配合下面的RNN分析\n",
        "        rnn_class = {\"RNN\": nn.RNN, \"LSTM\": nn.LSTM, \"GRU\": nn.GRU}.get(rnn_type, nn.RNN)\n",
        "\n",
        "        self.rnn = rnn_class(\n",
        "            embed_dim,\n",
        "            hidden_dim,\n",
        "            num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout if num_layers > 1 else 0,  # 单层 RNN 不支持 dropout   跟pytorch实现有关，RNN层数大于1时才会生效\n",
        "            bidirectional=bidirectional,\n",
        "        )\n",
        "        # bidirectional: bool = False,  默认单向\n",
        "        self.fc = nn.Linear(hidden_dim * (2 if bidirectional else 1), output_dim) # [hidden_dim * num_directions, output_dim]\n",
        "\n",
        "    def forward(self, x):\n",
        "        # [batch_size, seq_len]\n",
        "        x = self.embedding(x)  # [batch_size, seq_len, embed_dim]\n",
        "        _, hn = self.rnn(x)\n",
        "        # 单向 RNN: [num_layers, batch_size, dim]\n",
        "        # 双向 RNN: [num_layers * 2, batch_size, dim]\n",
        "\n",
        "        # 处理 LSTM 的元组输出（hn 是元组 (hn, cn)）\n",
        "        if isinstance(hn, tuple):\n",
        "            hn = hn[0]  # 只取 hn（隐藏状态），忽略 cn（细胞状态，内部用来处理长期记忆的，本次用不上）\n",
        "\n",
        "        x = hn[-1, :, :]  # 同样取最后一层 [batch_size, hidden_dim]\n",
        "        x = self.fc(x)\n",
        "        logits = x.squeeze()  # [batch_size]  <--- output_dim=1\n",
        "        return logits, None  # 此处是史山，与上面注意力板块适配的"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "MJP2sQ7JLQWe"
      },
      "outputs": [],
      "source": [
        "class ModelTrainer:\n",
        "    def __init__(self, model, train_loader, validation_loader, epochs=200, lr=0.01, model_name=\"AttentionModel\", loss_func=nn.BCEWithLogitsLoss()):\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.model = model.to(self.device)\n",
        "        self.train_loader = train_loader\n",
        "        self.validation_loader = validation_loader\n",
        "        self.epochs = epochs\n",
        "        self.lr = lr\n",
        "\n",
        "        # 默认二元交叉熵损失函数\n",
        "        self.loss_func = loss_func\n",
        "        self.optim = optim.Adam(self.model.parameters(), lr=self.lr)\n",
        "        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "            self.optim, \"min\", patience=3\n",
        "        )\n",
        "        self.max_acc = 0.0\n",
        "        self.model_name = model_name\n",
        "\n",
        "        # 拿来画图的\n",
        "        self.train_losses, self.validation_losses = [], []\n",
        "        self.train_accs, self.validation_accs = [], []\n",
        "        self.train_precisions, self.validation_precisions = [], []\n",
        "        self.train_recalls, self.validation_recalls = [], []\n",
        "        self.train_f1s, self.validation_f1s = [], []\n",
        "\n",
        "    def train(self):\n",
        "        for epoch in range(self.epochs):\n",
        "\n",
        "            # 训练\n",
        "            train_loss, train_acc, train_precision, train_recall, train_f1 = (\n",
        "                self.train_part()\n",
        "            )\n",
        "\n",
        "            # 验证\n",
        "            (\n",
        "                validation_loss,\n",
        "                validation_acc,\n",
        "                validation_precision,\n",
        "                validation_recall,\n",
        "                validation_f1,\n",
        "            ) = self.validation_part()\n",
        "\n",
        "            # 调整学习率\n",
        "            self.scheduler.step(validation_loss)\n",
        "\n",
        "            # 保存最佳模型\n",
        "            if validation_acc > self.max_acc:\n",
        "                self.max_acc = validation_acc\n",
        "                torch.save(self.model.state_dict(), self.model_name)\n",
        "\n",
        "            print(f\"Epoch {epoch + 1}/{self.epochs}\")\n",
        "            print(\n",
        "                f\"Train Loss: {train_loss:.4f}, Acc: {train_acc:.4f}, Precision: {train_precision:.4f}, Recall: {train_recall:.4f}, F1: {train_f1:.4f} | \"\n",
        "                f\"Validation Loss: {validation_loss:.4f}, Acc: {validation_acc:.4f}, Precision: {validation_precision:.4f}, Recall: {validation_recall:.4f}, F1: {validation_f1:.4f}\"\n",
        "            )\n",
        "\n",
        "        return (\n",
        "            self.train_losses,\n",
        "            self.validation_losses,\n",
        "            self.train_accs,\n",
        "            self.validation_accs,\n",
        "            self.train_precisions,\n",
        "            self.validation_precisions,\n",
        "            self.train_recalls,\n",
        "            self.validation_recalls,\n",
        "            self.train_f1s,\n",
        "            self.validation_f1s,\n",
        "        )\n",
        "\n",
        "    def train_part(self):\n",
        "        self.model.train()\n",
        "        loss_sum = 0.0\n",
        "        all_labels = []\n",
        "        all_preds = []\n",
        "        for inputs, labels in self.train_loader:\n",
        "            inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
        "            self.optim.zero_grad()  # 梯度清零\n",
        "            logits, _ = self.model(inputs)  # 调用上面搭建的前向传播\n",
        "            if self.loss_func.__class__.__name__ == \"BCEWithLogitsLoss\":\n",
        "                loss = self.loss_func(logits, labels.float())  # 计算损失\n",
        "                preds = (torch.sigmoid(logits) > 0.5).int().tolist()\n",
        "            else: # CE\n",
        "                loss = self.loss_func(logits, labels.long())\n",
        "                preds = torch.argmax(logits, dim=1).int().tolist()\n",
        "            loss.backward()  # 反向传播，计算梯度\n",
        "            self.optim.step()  # 更新参数\n",
        "\n",
        "            loss_sum += loss.item()\n",
        "            all_labels.extend(labels.tolist())\n",
        "            all_preds.extend(preds)\n",
        "\n",
        "        train_loss = loss_sum / len(self.train_loader)\n",
        "        train_acc = accuracy_score(all_labels, all_preds)\n",
        "        train_precision = precision_score(all_labels, all_preds)\n",
        "        train_recall = recall_score(all_labels, all_preds)\n",
        "        train_f1 = f1_score(all_labels, all_preds)\n",
        "\n",
        "        self.train_losses.append(train_loss)\n",
        "        self.train_accs.append(train_acc)\n",
        "        self.train_precisions.append(train_precision)\n",
        "        self.train_recalls.append(train_recall)\n",
        "        self.train_f1s.append(train_f1)\n",
        "        return train_loss, train_acc, train_precision, train_recall, train_f1\n",
        "\n",
        "    def validation_part(self):\n",
        "        self.model.eval()\n",
        "        validation_loss = 0.0\n",
        "        all_labels = []\n",
        "        all_preds = []\n",
        "        with torch.no_grad():  # 验证阶段不需要计算梯度\n",
        "            for inputs, labels in self.validation_loader:\n",
        "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
        "                logits, _ = self.model(inputs)\n",
        "\n",
        "                if self.loss_func.__class__.__name__ == \"BCEWithLogitsLoss\":\n",
        "                    validation_loss += self.loss_func(logits, labels.float()).item()\n",
        "                    preds = (torch.sigmoid(logits) > 0.5).int().tolist()\n",
        "                else:  # CE\n",
        "                    validation_loss += self.loss_func(logits, labels.long()).item()\n",
        "                    preds = torch.argmax(logits, dim=1).tolist()\n",
        "\n",
        "                all_labels.extend(labels.tolist())\n",
        "                all_preds.extend(preds)\n",
        "\n",
        "        validation_loss = validation_loss / len(self.validation_loader)\n",
        "        validation_acc = accuracy_score(all_labels, all_preds)\n",
        "        validation_precision = precision_score(all_labels, all_preds)\n",
        "        validation_recall = recall_score(all_labels, all_preds)\n",
        "        validation_f1 = f1_score(all_labels, all_preds)\n",
        "\n",
        "        self.validation_losses.append(validation_loss)\n",
        "        self.validation_accs.append(validation_acc)\n",
        "        self.validation_precisions.append(validation_precision)\n",
        "        self.validation_recalls.append(validation_recall)\n",
        "        self.validation_f1s.append(validation_f1)\n",
        "        return (\n",
        "            validation_loss,\n",
        "            validation_acc,\n",
        "            validation_precision,\n",
        "            validation_recall,\n",
        "            validation_f1,\n",
        "        )\n",
        "\n",
        "LOSS_IDX_T = 0\n",
        "LOSS_IDX_V = 1\n",
        "ACC_IDX_T = 2\n",
        "ACC_IDX_V = 3\n",
        "PRECISION_IDX_T = 4\n",
        "PRECISION_IDX_V = 5\n",
        "RECALL_IDX_T = 6\n",
        "RECALL_IDX_V = 7\n",
        "F1_IDX_T = 8\n",
        "F1_IDX_V = 9"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "训练集曲线平稳、验证集曲线的抖动是正常的，数据量太少、参数量太多的情况下模型很容易阶段性过拟合，后续epoch恢复后就会带来抖动"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "B4D-R1a6LQWf"
      },
      "outputs": [],
      "source": [
        "class ModelEvaluator:\n",
        "    def __init__(\n",
        "        self,\n",
        "        model=None,\n",
        "        test_loader=None,\n",
        "        compare_lists=None,\n",
        "    ):\n",
        "        self.model = model\n",
        "        self.test_loader = test_loader\n",
        "        self.compare_lists = compare_lists\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    def evaluate_model(self, analyse=True, model_path=\"AttentionModel.pth\", loss_type=\"BCE\"):\n",
        "        self.model.load_state_dict(torch.load(model_path, map_location=self.device))\n",
        "        self.model.to(self.device)\n",
        "        self.model.eval()\n",
        "\n",
        "        all_labels = []\n",
        "        all_preds = []\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in self.test_loader:\n",
        "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
        "                logits, _ = self.model(inputs)\n",
        "\n",
        "                if loss_type == \"BCE\":\n",
        "                    preds = (torch.sigmoid(logits) > 0.5).int().tolist()\n",
        "                else:\n",
        "                    preds = torch.argmax(logits, dim=1).tolist()\n",
        "\n",
        "                all_labels.extend(labels.tolist())\n",
        "                all_preds.extend(preds)\n",
        "\n",
        "        # 计算评估指标\n",
        "        test_acc = accuracy_score(all_labels, all_preds)\n",
        "        test_precision = precision_score(all_labels, all_preds)\n",
        "        test_recall = recall_score(all_labels, all_preds)\n",
        "        test_f1 = f1_score(all_labels, all_preds)\n",
        "\n",
        "        print(f\"Test Accuracy: {test_acc:.4f}\")\n",
        "        print(f\"Test Precision: {test_precision:.4f}\")\n",
        "        print(f\"Spam Accuracy: {2 * test_acc - test_precision:.4f}\")\n",
        "        print(f\"Test Recall: {test_recall:.4f}\")\n",
        "        print(f\"Test F1 Score: {test_f1:.4f}\")\n",
        "\n",
        "        # 绘制混淆矩阵\n",
        "        cm = confusion_matrix(all_labels, all_preds, labels=[0, 1])\n",
        "        disp = ConfusionMatrixDisplay(\n",
        "            confusion_matrix=cm, display_labels=[\"Ham\", \"Spam\"]\n",
        "        )\n",
        "        disp.plot(cmap=plt.cm.Blues)\n",
        "        plt.title(\"Confusion Matrix\")\n",
        "        plt.show()\n",
        "\n",
        "        if not analyse:\n",
        "            return test_acc, test_precision, test_recall, test_f1\n",
        "\n",
        "    def plot_compare_strategies(\n",
        "        self,\n",
        "        metric_name,\n",
        "        train_idx=0,\n",
        "        val_idx=1,\n",
        "        title=\"\",\n",
        "    ):\n",
        "        plt.figure(figsize=(12, 6))\n",
        "\n",
        "        # 训练曲线\n",
        "        if train_idx is not None:\n",
        "            plt.subplot(1, 2, 1)\n",
        "            for label, results in self.compare_lists:\n",
        "                plt.plot(\n",
        "                    results[train_idx], label=f\"Train {metric_name} ({label})\"\n",
        "                )  # 训练指标\n",
        "            plt.xlabel(\"Epoch\")\n",
        "            plt.ylabel(metric_name)\n",
        "            plt.title(f\"Train {title}\")\n",
        "            plt.legend()\n",
        "\n",
        "        # 验证曲线\n",
        "        if train_idx is not None:\n",
        "            plt.subplot(1, 2, 2)\n",
        "            lb = \"Validation\"\n",
        "            cs = \"Epoch\"\n",
        "        else:\n",
        "            plt.subplot(1, 1, 1)  # 如果没有训练曲线，则调整为单独的图\n",
        "            lb = \"Test\"\n",
        "            cs = \"Case\"\n",
        "\n",
        "        for label, results in self.compare_lists:\n",
        "            plt.plot(\n",
        "                results[val_idx], label=f\"{lb} {metric_name} ({label})\"\n",
        "            )  # 验证指标\n",
        "        plt.xlabel(cs)\n",
        "        plt.ylabel(metric_name)\n",
        "        plt.title(f\"{lb} {title}\")\n",
        "        plt.legend()\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nn4wWQcXLQWf"
      },
      "source": [
        "## 分析"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "FxzH6gAkLQWf"
      },
      "outputs": [],
      "source": [
        "def plot_metrics(compare_lists, setTitle):\n",
        "    # 多组测试封装\n",
        "    plot_compare_strategies = ModelEvaluator(\n",
        "        compare_lists=compare_lists\n",
        "    ).plot_compare_strategies\n",
        "\n",
        "    plot_compare_strategies(\n",
        "        \"Loss\", train_idx=LOSS_IDX_T, val_idx=LOSS_IDX_V, title=setTitle(\"Loss\")\n",
        "    )\n",
        "\n",
        "    plot_compare_strategies(\n",
        "        \"Accuracy\", train_idx=ACC_IDX_T, val_idx=ACC_IDX_V, title=setTitle(\"Accuracy\")\n",
        "    )\n",
        "\n",
        "    plot_compare_strategies(\n",
        "        \"Precision\",\n",
        "        train_idx=PRECISION_IDX_T,\n",
        "        val_idx=PRECISION_IDX_V,\n",
        "        title=setTitle(\"Precision\"),\n",
        "    )\n",
        "\n",
        "    plot_compare_strategies(\n",
        "        \"Recall\", train_idx=RECALL_IDX_T, val_idx=RECALL_IDX_V, title=setTitle(\"Recall\")\n",
        "    )\n",
        "\n",
        "    plot_compare_strategies(\n",
        "        \"F1 Score\", train_idx=F1_IDX_T, val_idx=F1_IDX_V, title=setTitle(\"F1 Score\")\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Ib6-MiT8LQWf"
      },
      "outputs": [],
      "source": [
        "def compare_diff_strategies(evaluates=None, setTitle=None, compare_lists=None):\n",
        "    # 这里直接用plt画图，仿照eval里多策略画图的方法（解耦还是做的不够好）\n",
        "    if compare_lists is None:\n",
        "        compare_lists = []\n",
        "        for evaluate in evaluates:\n",
        "            model_name = evaluate[2]\n",
        "            test_acc, test_precision, test_recall, test_f1 = ModelEvaluator(\n",
        "                evaluate[0], test_loader=test_loader\n",
        "            ).evaluate_model(analyse=False, model_path=evaluate[1])\n",
        "            compare_lists.append([model_name, test_acc, test_precision, test_recall, test_f1])\n",
        "\n",
        "    models = [item[0] for item in compare_lists]\n",
        "    accs = [item[1] for item in compare_lists]\n",
        "    precisions = [item[2] for item in compare_lists]\n",
        "    recalls = [item[3] for item in compare_lists]\n",
        "    f1s = [item[4] for item in compare_lists]\n",
        "\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    metrics = [\"Accuracy\", \"Precision\", \"Recall\", \"F1 Score\"]\n",
        "    metrics_data = [accs, precisions, recalls, f1s]\n",
        "\n",
        "    for idx, (metric, data) in enumerate(zip(metrics, metrics_data)):\n",
        "        plt.subplot(2, 2, idx + 1)\n",
        "        x = range(len(models))\n",
        "        plt.plot(x, data, marker='o', linestyle='-', linewidth=2, markersize=8)\n",
        "        plt.xticks(x, models, rotation=45)\n",
        "        plt.ylabel(metric)\n",
        "        plt.title(f\"Test {setTitle(metric)}\")\n",
        "        for i, val in enumerate(data):\n",
        "            plt.text(i, val, f\"{val:.4f}\", ha='center', va='bottom')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "gOf3iFnPLQWf"
      },
      "outputs": [],
      "source": [
        "vocab_size = len(word_to_idx)\n",
        "d_model = 256   # 较小的维度已经足以完成二分类任务，太高会引入很大的计算开销\n",
        "nhead = 8\n",
        "hidden_dim = 256\n",
        "num_layers = 1\n",
        "output_dim = 1\n",
        "epochs = 15\n",
        "\n",
        "\n",
        "def train_model(model, model_name, loss_func=nn.BCEWithLogitsLoss(), train_loader=train_loader, val_loader=val_loader):\n",
        "    trainer = ModelTrainer(\n",
        "        model=model,\n",
        "        train_loader=train_loader,\n",
        "        validation_loader=val_loader,\n",
        "        epochs=epochs,\n",
        "        lr=0.001,\n",
        "        model_name=model_name,\n",
        "        loss_func=loss_func,\n",
        "    )\n",
        "    return trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "vOkqVIU3USvH"
      },
      "outputs": [],
      "source": [
        "def train_truly(model, model_name, loss_func=nn.BCEWithLogitsLoss(), train_loader=train_loader, val_loader=val_loader):\n",
        "    start_time = time.time()\n",
        "\n",
        "    # 记录初始内存\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.reset_peak_memory_stats()\n",
        "        initial_memory = torch.cuda.memory_allocated()\n",
        "    else:\n",
        "        process = psutil.Process()\n",
        "        initial_memory = process.memory_info().rss  # 初始内存（字节）\n",
        "\n",
        "    results = train_model(model=model, model_name=model_name, loss_func=loss_func, train_loader=train_loader, val_loader=val_loader)\n",
        "    end_time = time.time()\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        peak_memory = torch.cuda.max_memory_allocated()\n",
        "    else:\n",
        "        peak_memory = process.memory_info().rss  # 最终内存（字节）\n",
        "\n",
        "    elapsed_time = end_time - start_time\n",
        "    memory_used = peak_memory - initial_memory\n",
        "    print(f\"Model: {model_name}\")\n",
        "    print(f\"Elapsed Time: {elapsed_time:.2f} seconds\")\n",
        "    print(f\"Memory Used: {memory_used / (1024 ** 2):.2f} MB\\n\")\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RL-RA_reLQWf"
      },
      "source": [
        "#### Attention与RNN比较\n",
        "\n",
        "\n",
        "多头注意力机制将输入 $X$ 投影到 $h$ 个不同的子空间（注意力头），每个子空间的维度为 $d/h$。每个注意力头独立计算（所有数据），最后将结果拼接并通过一个线性层得到最终输出\n",
        "\n",
        "- 每个注意力头的计算复杂度为 $O(n^2\\frac{d}{h})$\n",
        "- 由于有 $h$ 个注意力头，总的计算复杂度为 $h \\times O(n^2\\frac{d}{h}) = O(n^2d)$\n",
        "\n",
        "多头注意力自回归模型的计算复杂度主要由注意力机制决定，为 $O(n^2d)$，其中 $n$ 是序列长度，$d$ 是特征维度。这意味着随着序列长度的增加，计算复杂度呈平方增长，因此在处理长序列时计算量会显著增加\n",
        "\n",
        "---\n",
        "\n",
        "RNN 的基本结构是一个循环单元，在每个时间步，需要计算 $\\mathbf{W}_{xh}\\mathbf{x}_t$ 和 $\\mathbf{W}_{hh}\\mathbf{h}_{t - 1}$，这两个矩阵乘法的复杂度分别为 $O(hd)$ 和 $O(h^2)$。因此，单个时间步的计算复杂度为 $O(hd + h^2)$；对于长度为 $n$ 的序列，需要进行 $n$ 个时间步的计算，因此总的计算复杂度为 $O(n(hd + h^2))$\n",
        "\n",
        "其中 $n$ 是序列长度，$d$ 是输入维度，$h$ 是隐藏状态维度。通常情况下，隐藏状态维度 $h$ 是一个固定的值，因此 RNN 的计算复杂度与序列长度 $n$ 呈线性关系。这使得 RNN 在处理长序列时相对更高效，但由于其循环结构，难以并行计算\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "FnUot7oCLQWf",
        "outputId": "7c3a35b2-3fb2-45d8-9574-9b6db195d7fd"
      },
      "outputs": [],
      "source": [
        "# 训练 AttentionModel\n",
        "attention_model = AttentionModel(vocab_size, d_model, nhead)\n",
        "results_attention = train_truly(\n",
        "    model=attention_model, model_name=\"AttentionModel.pth\"\n",
        ")\n",
        "\n",
        "# 训练 RNNModel\n",
        "rnn_model = RNNModel(\n",
        "    vocab_size, d_model, hidden_dim, num_layers, output_dim, rnn_type=\"RNN\"\n",
        ")\n",
        "results_rnn = train_truly(model=rnn_model, model_name=\"RNNModel.pth\")\n",
        "\n",
        "# 训练 LSTM-RNN\n",
        "lstm_model = RNNModel(\n",
        "    vocab_size, d_model, hidden_dim, num_layers, output_dim, rnn_type=\"LSTM\"\n",
        ")\n",
        "results_lstm = train_truly(model=lstm_model, model_name=\"LSTMModel.pth\")\n",
        "\n",
        "# 训练 GRU-RNN\n",
        "gru_model = RNNModel(\n",
        "    vocab_size, d_model, hidden_dim, num_layers, output_dim, rnn_type=\"GRU\"\n",
        ")\n",
        "results_gru = train_truly(model=gru_model, model_name=\"GRUModel.pth\")\n",
        "\n",
        "# 嵌套列表，包含模型名称和对应的结果\n",
        "compare_lists = [\n",
        "    [\"Attention\", results_attention],\n",
        "    [\"RNN\", results_rnn],\n",
        "    [\"LSTM\", results_lstm],\n",
        "    [\"GRU\", results_gru],\n",
        "]\n",
        "\n",
        "# 绘制对比图\n",
        "def setTitle(metric_name):\n",
        "    return f\"{metric_name} -- Attention vs RNN\"\n",
        "\n",
        "plot_metrics(compare_lists, setTitle)\n",
        "compare_diff_strategies(\n",
        "    [\n",
        "        [attention_model, \"AttentionModel.pth\", \"Attention\"],\n",
        "        [rnn_model, \"RNNModel.pth\", \"RNN\"],\n",
        "        [lstm_model, \"LSTMModel.pth\", \"LSTM\"],\n",
        "        [gru_model, \"GRUModel.pth\", \"GRU\"],\n",
        "    ],\n",
        "    setTitle,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPIHmfPc80gb"
      },
      "source": [
        "#### 模型超参数影响"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OMo7eVku80gb",
        "outputId": "63175ecc-6171-4bd9-df2b-52169e77b448"
      },
      "outputs": [],
      "source": [
        "head_group = [\n",
        "    {\"nhead\": 2, \"d_model\": d_model, \"mha_cnt\": 1},\n",
        "    {\"nhead\": 4, \"d_model\": d_model, \"mha_cnt\": 1},\n",
        "    {\"nhead\": 8, \"d_model\": d_model, \"mha_cnt\": 1},\n",
        "    {\"nhead\": 16, \"d_model\": d_model, \"mha_cnt\": 1},\n",
        "]\n",
        "\n",
        "dim_group = [\n",
        "    {\"nhead\": nhead, \"d_model\": 32, \"mha_cnt\": 1},\n",
        "    {\"nhead\": nhead, \"d_model\": 128, \"mha_cnt\": 1},\n",
        "    {\"nhead\": nhead, \"d_model\": 256, \"mha_cnt\": 1},\n",
        "    {\"nhead\": nhead, \"d_model\": 512, \"mha_cnt\": 1},\n",
        "]\n",
        "\n",
        "mha_group = [\n",
        "    {\"nhead\": nhead, \"d_model\": d_model, \"mha_cnt\": 1},\n",
        "    {\"nhead\": nhead, \"d_model\": d_model, \"mha_cnt\": 2},\n",
        "    {\"nhead\": nhead, \"d_model\": d_model, \"mha_cnt\": 3},\n",
        "    {\"nhead\": nhead, \"d_model\": d_model, \"mha_cnt\": 4},\n",
        "]\n",
        "\n",
        "def param_test(param_group, group_name, pos):\n",
        "    results = []\n",
        "    compare_lists = []\n",
        "    for params in param_group:\n",
        "        model = AttentionModel(\n",
        "            vocab_size=vocab_size,\n",
        "            d_model=params[\"d_model\"],\n",
        "            nhead=params[\"nhead\"],\n",
        "            mha_cnt=params[\"mha_cnt\"],\n",
        "        )\n",
        "        model_name = f\"AttentionModel_{group_name}_nhead{params['nhead']}_dmodel{params['d_model']}_mha{params['mha_cnt']}.pth\"\n",
        "        result = train_truly(model=model, model_name=model_name)\n",
        "        results.append([f\"{group_name}_{params[pos]}\", result])\n",
        "        compare_lists.append([model, model_name, f\"{group_name}_{params[pos]}\"])\n",
        "    return results, compare_lists\n",
        "\n",
        "results_heads, compare_lists_heads = param_test(head_group, \"heads\", \"nhead\")\n",
        "results_dims, compare_lists_dims = param_test(dim_group, \"dims\", \"d_model\")\n",
        "results_mhas, compare_lists_mhas = param_test(mha_group, \"mhas\", \"mha_cnt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "T6OFXgvh80gb",
        "outputId": "208a1aca-6d29-48bf-ae86-ca7bedd4261f"
      },
      "outputs": [],
      "source": [
        "# 注意力头数\n",
        "def setTitle(metric_name):\n",
        "    return f\"{metric_name} -- heads Comparisons\"\n",
        "plot_metrics(results_heads, setTitle)\n",
        "compare_diff_strategies(\n",
        "    evaluates=compare_lists_heads,\n",
        "    setTitle=setTitle,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "iAcDkM-t80gb",
        "outputId": "353d5864-0860-4d1f-d3ec-e7bd06d828df"
      },
      "outputs": [],
      "source": [
        "# 隐藏层维度\n",
        "def setTitle(metric_name):\n",
        "    return f\"{metric_name} -- dims Comparisons\"\n",
        "plot_metrics(results_dims, setTitle)\n",
        "compare_diff_strategies(\n",
        "    evaluates=compare_lists_dims,\n",
        "    setTitle=setTitle,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ddKWDHQ580gb",
        "outputId": "ca880bd0-3580-426a-a0c2-7b52ffff4403"
      },
      "outputs": [],
      "source": [
        "# 注意力层数\n",
        "def setTitle(metric_name):\n",
        "    return f\"{metric_name} -- mhas Comparisons\"\n",
        "plot_metrics(results_mhas, setTitle)\n",
        "compare_diff_strategies(\n",
        "    evaluates=compare_lists_mhas,\n",
        "    setTitle=setTitle,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GefYnk9_LQWf"
      },
      "source": [
        "#### 文本处理影响"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S54hJWlJ80gb",
        "outputId": "2fedca46-af80-43c0-e841-bcf9cee2596a"
      },
      "outputs": [],
      "source": [
        "# 提取预处理好的文本数据\n",
        "df_processed = pd.read_csv(\"processed_text.csv\", encoding=\"utf-8\")\n",
        "print(\"是否存在 NaN 值:\", df_processed[TEXT_PROCESSED].isna().any())\n",
        "\n",
        "# 忽略空值\n",
        "df_processed[TEXT_PROCESSED] = df_processed[TEXT_PROCESSED].fillna(\"\")\n",
        "print(df_processed.head())\n",
        "\n",
        "PAD = \"<pad>\"\n",
        "UNK = \"<unk>\"\n",
        "\n",
        "# 将所有分词结果合并为一个大字符串（空格分隔），然后一次性分割成单词列表\n",
        "all_tokens = list(\n",
        "    itertools.chain.from_iterable(df_processed[TEXT_PROCESSED].str.split())\n",
        ")\n",
        "\n",
        "# all_tokens = [token for token in all_tokens if not token.isdigit()]  # 去除数字\n",
        "\n",
        "unique_tokens = set(all_tokens)\n",
        "print(f\"无重复的最大词表大小为: {len(unique_tokens)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7MuT52t080gb"
      },
      "outputs": [],
      "source": [
        "# 把上面的预处理逻辑与训练、测试代码做个封装，这部分可以直接不看\n",
        "def Attention_all_in_one(sentence_length, vocab_size_T, model_name):\n",
        "    all_tokens = list(\n",
        "        itertools.chain.from_iterable(df_processed[TEXT_PROCESSED].str.split())\n",
        "    )\n",
        "    word_counts = Counter(all_tokens)\n",
        "    vocab = [word for word, _ in word_counts.most_common(vocab_size_T-2)]   # 此处必须减2，不然会越界报错\n",
        "    vocab = [PAD, UNK] + vocab\n",
        "    word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
        "    # 把分词结果借助词表转换为数字序列\n",
        "    df_processed[TEXT_VECTORIZATION] = df_processed[TEXT_PROCESSED].apply(\n",
        "        lambda x: [word_to_idx.get(token, word_to_idx[UNK]) for token in x.split()]\n",
        "    )\n",
        "    # 从左到右截，前面的部分可能包含主题等重要内容\n",
        "    def pad_sequence(indices):\n",
        "        if len(indices) > sentence_length:\n",
        "            return indices[:sentence_length]\n",
        "        else:\n",
        "            return [word_to_idx[PAD]] * (sentence_length - len(indices)) + indices\n",
        "\n",
        "    df_processed[TEXT_VECTORIZATION_PADDING] = df_processed[TEXT_VECTORIZATION].apply(\n",
        "        pad_sequence\n",
        "    )\n",
        "    # 数据集划分\n",
        "    df_useful = df_processed[[LABEL, TEXT_VECTORIZATION_PADDING]]\n",
        "    nested_data = df_useful.apply(\n",
        "        lambda row: {\n",
        "            \"label\": row[\"label\"],\n",
        "            \"text\": row[TEXT_VECTORIZATION_PADDING],\n",
        "        },\n",
        "        axis=1,\n",
        "    ).tolist()\n",
        "\n",
        "    df_loaded = pd.DataFrame(\n",
        "        [\n",
        "            {\n",
        "                \"text\": item[\"text\"],\n",
        "                \"label\": item[\"label\"],\n",
        "            }\n",
        "            for item in nested_data\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    df_shuffled = df_loaded.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "    train_df, test_df = train_test_split(df_shuffled, test_size=0.2, random_state=42)\n",
        "    train_df, val_df = train_test_split(train_df, test_size=0.1, random_state=42)\n",
        "\n",
        "    train_dataset = TensorDataset(\n",
        "        torch.LongTensor(train_df[\"text\"].tolist()),\n",
        "        torch.LongTensor(train_df[\"label\"].tolist()),\n",
        "    )\n",
        "\n",
        "    val_dataset = TensorDataset(\n",
        "        torch.LongTensor(val_df[\"text\"].tolist()),\n",
        "        torch.LongTensor(val_df[\"label\"].tolist()),\n",
        "    )\n",
        "\n",
        "    test_dataset = TensorDataset(\n",
        "        torch.LongTensor(test_df[\"text\"].tolist()),\n",
        "        torch.LongTensor(test_df[\"label\"].tolist()),\n",
        "    )\n",
        "\n",
        "    # 数据加载器\n",
        "    BATCH_SIZE = 256\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "    attention_model = AttentionModel(vocab_size_T, d_model, nhead)\n",
        "    results_attention = train_truly(\n",
        "        model=attention_model, model_name=model_name, train_loader=train_loader, val_loader=val_loader\n",
        "    )\n",
        "    test_acc, test_precision, test_recall, test_f1 = ModelEvaluator(\n",
        "        attention_model, test_loader=test_loader\n",
        "    ).evaluate_model(analyse=False, model_path=model_name)\n",
        "\n",
        "    return results_attention, [model_name, test_acc, test_precision, test_recall, test_f1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "byetTJoU80gc",
        "outputId": "8c9c7e3b-c97b-4c7a-a3f6-6997257f8c8b"
      },
      "outputs": [],
      "source": [
        "print(int(df_processed[TEXT_PROCESSED].str.len().max() * 0.75))\n",
        "results = []\n",
        "compare_lists = []\n",
        "sentence_lengths = [50, 200, 500, 1000]  # 1000跑不动，爆显存了\n",
        "vocab_sizes = [10_000, 50_000, 100_000, len(unique_tokens)-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "27-idwQv80gj",
        "outputId": "c0591214-ed69-4192-905c-40b02073d05d"
      },
      "outputs": [],
      "source": [
        "# 固定词表大小，测试不同截断长度\n",
        "for sentence_length in sentence_lengths:\n",
        "    result, compare_list = Attention_all_in_one(\n",
        "        sentence_length, 100_000, f\"AttentionModel_len{sentence_length}.pth\"\n",
        "    )\n",
        "    results.append([f\"Length {sentence_length}\", result])\n",
        "    compare_lists.append(compare_list)\n",
        "# 绘制对比图\n",
        "def setTitle(metric_name):\n",
        "    return f\"{metric_name} -- Length Comparisons\"\n",
        "\n",
        "plot_metrics(results, setTitle)\n",
        "# 新增了判断逻辑，直接传入compare_lists说明可以直接画图\n",
        "compare_diff_strategies(compare_lists=compare_lists, setTitle=setTitle)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "4mKk5zTrWYa_",
        "outputId": "c08ef881-cff2-47ac-f2a6-68587adf25b5"
      },
      "outputs": [],
      "source": [
        "# 绘制对比图\n",
        "def setTitle(metric_name):\n",
        "    return f\"{metric_name} -- Length Comparisons\"\n",
        "\n",
        "plot_metrics(results, setTitle)\n",
        "# 新增了判断逻辑，直接传入compare_lists说明可以直接画图\n",
        "compare_diff_strategies(compare_lists=compare_lists, setTitle=setTitle)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "N6CvzgBJ80gj",
        "outputId": "0bf2aeb0-7fc9-4f7d-af14-3303358144c2"
      },
      "outputs": [],
      "source": [
        "results = []\n",
        "compare_lists = []\n",
        "# 固定截断长度，测试不同词表大小\n",
        "for vocab_size_T in vocab_sizes:\n",
        "    result, compare_list = Attention_all_in_one(\n",
        "        200, vocab_size_T, f\"AttentionModel_vocab{vocab_size_T}.pth\"\n",
        "    )\n",
        "    results.append([f\"Vocab {vocab_size_T}\", result])\n",
        "    compare_lists.append(compare_list)\n",
        "\n",
        "\n",
        "# 绘制对比图\n",
        "def setTitle(metric_name):\n",
        "    return f\"{metric_name} -- Vocab Comparisons\"\n",
        "\n",
        "\n",
        "plot_metrics(results, setTitle)\n",
        "compare_diff_strategies(compare_lists=compare_lists, setTitle=setTitle)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qyHQooSRLQWg"
      },
      "source": [
        "#### 位置编码分析"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "dZPhZ-a_LQWp",
        "outputId": "86dff3f6-89ef-4170-8d32-7e777d07956d"
      },
      "outputs": [],
      "source": [
        "model_original = AttentionModel(\n",
        "    vocab_size, d_model, nhead, pos_encoding_type=\"original\"\n",
        ")\n",
        "model_rope = AttentionModel(\n",
        "    vocab_size, d_model, nhead, pos_encoding_type=\"rope\"\n",
        ")\n",
        "results_rope = train_truly(\n",
        "    model=model_rope, model_name=\"AttentionModel_RoPE.pth\"\n",
        ")\n",
        "\n",
        "results_original = train_truly(\n",
        "    model=model_original, model_name=\"AttentionModel_Original.pth\"\n",
        ")\n",
        "\n",
        "compare_lists = [\n",
        "    [\"ORIG\", results_original],\n",
        "    [\"RoPE\", results_rope],\n",
        "]\n",
        "\n",
        "def setTitle(metric_name):\n",
        "    return f\"{metric_name} ---- 2 PE Compare\"\n",
        "\n",
        "plot_metrics(compare_lists, setTitle)\n",
        "compare_diff_strategies(\n",
        "    [\n",
        "        [model_original, \"AttentionModel_Original.pth\", \"ORIG\"],\n",
        "        [model_rope, \"AttentionModel_RoPE.pth\", \"RoPE\"],\n",
        "    ],\n",
        "    setTitle,\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "RL-RA_reLQWf",
        "jPIHmfPc80gb",
        "qyHQooSRLQWg"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
