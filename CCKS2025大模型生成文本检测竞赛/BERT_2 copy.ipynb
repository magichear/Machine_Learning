{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0GJ73UXQrWuP"
      },
      "source": [
        "### 加载数据集"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qFjvB8BGOhdV",
        "outputId": "9f62451d-50f7-40d0-938c-b8e46bbcac27"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "import matplotlib\n",
        "import sklearn\n",
        "import transformers\n",
        "\n",
        "!python --version\n",
        "print(f\"os          : {os.name}\")\n",
        "print(f\"torch       : {torch.__version__}\")\n",
        "print(f\"pandas      : {pd.__version__}\")\n",
        "print(f\"matplotlib  : {matplotlib.__version__}\")\n",
        "print(f\"sklearn     : {sklearn.__version__}\")\n",
        "print(f\"transformers: {transformers.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zVNujL7rnacz"
      },
      "outputs": [],
      "source": [
        "# imdb的无监督数据集可以做对比学习或特定领域微调（如果允许）\n",
        "#!git clone https://hf-mirror.com/datasets/stanfordnlp/imdb ./datasets/imdb\n",
        "# !git clone https://hf-mirror.com/datasets/stanfordnlp/sst2 ./datasets/sst2\n",
        "\n",
        "# model  全库\n",
        "!git clone https://hf-mirror.com/google-bert/bert-base-uncased ./models/bert-base-uncased"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oG173qWWwbP3",
        "outputId": "16fe6ad4-09fe-4936-e2c7-830afc1f2b59"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# 创建保存模型文件的目录\n",
        "local_dir = \"./models/bert-base-uncased\"\n",
        "os.makedirs(local_dir, exist_ok=True)\n",
        "\n",
        "# 使用 wget 下载文件\n",
        "os.system(f\"wget -P {local_dir} https://hf-mirror.com/google-bert/bert-base-uncased/resolve/main/config.json\")\n",
        "os.system(f\"wget -P {local_dir} https://hf-mirror.com/google-bert/bert-base-uncased/resolve/main/pytorch_model.bin\")\n",
        "os.system(f\"wget -P {local_dir} https://hf-mirror.com/google-bert/bert-base-uncased/resolve/main/tokenizer.json\")\n",
        "os.system(f\"wget -P {local_dir} https://hf-mirror.com/google-bert/bert-base-uncased/resolve/main/tokenizer_config.json\")\n",
        "os.system(f\"wget -P {local_dir} https://hf-mirror.com/google-bert/bert-base-uncased/resolve/main/vocab.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "K_d1cDKv0IIq",
        "outputId": "a44dd09f-3b02-4184-c749-8b449945f11e"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import shutil\n",
        "drive.mount(\"/content/drive/\")\n",
        "srcC = \"./models/bert-base-uncased/bert-base-uncased-e2e.bin\"\n",
        "destC = \"/content/drive/MyDrive/sdxxdl/bert-base-uncased-e2e.bin\"\n",
        "shutil.copy(srcC, destC)\n",
        "#shutil.copy(destC, srcC)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5uRbpPGFnacz"
      },
      "outputs": [],
      "source": [
        "# IMDB需要\n",
        "#%pip install fastparquet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "hx2zk4aSw7OU"
      },
      "outputs": [],
      "source": [
        "%mkdir datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BEhf0WoTnac0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "import torch\n",
        "import string\n",
        "import random\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, get_linear_schedule_with_warmup, BertModel, BertForMaskedLM, DataCollatorForLanguageModeling\n",
        "from torch.optim import AdamW   # transformers 里的 AdamW 不再被推荐\n",
        "\n",
        "IMDB_PATH = \"./data/imdb/plain_text/\"\n",
        "IMDB_TRAIN = IMDB_PATH + \"train-00000-of-00001.parquet\"\n",
        "IMDB_TEST = IMDB_PATH + \"test-00000-of-00001.parquet\"\n",
        "IMDB_UNSUPERVISED = IMDB_PATH + \"unsupervised-00000-of-00001.parquet\"   # 无标签数据，可以拿来做语料训练\n",
        "\n",
        "TRAIN_PATH = \"datasets/train.jsonl\"\n",
        "TEST_PATH = \"datasets/test.jsonl\"\n",
        "\n",
        "MODEL_PATH = \"./models/bert-base-uncased\"\n",
        "MODEL_E2E_NAME = MODEL_PATH+\"/bert-base-uncased-e2e.bin\"\n",
        "MODEL_PRE_NAME = MODEL_PATH+\"/bert-base-uncased-mlm.bin\"\n",
        "MODEL_PRE_PATH = MODEL_PATH+\"/bert-base-uncased-mlm\"\n",
        "# 分类标签数\n",
        "NUM_LABELS = 2\n",
        "classes= {\n",
        "    1: \"Machine\",\n",
        "    0: \"Human\"\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zNvLk_tuwbP4"
      },
      "source": [
        "机器相关配置：队友们在运行前必须先根据机器情况配置此处"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "aG2OfuEkwbP4"
      },
      "outputs": [],
      "source": [
        "# 这两项十分消耗显存，参考：colab的T4显卡只能运行 128*128\n",
        "MAX_LENGTH = 128  # 一般不超过512\n",
        "BATCH_SIZE = 128\n",
        "BATCH_SIZE_TEST = BATCH_SIZE << 3 # 测试不会消耗太多资源\n",
        "# 线程数（一般一个运数据，一个跑）\n",
        "NUM_WORKERS = 2\n",
        "# 学习率\n",
        "LEARNING_RATE = 5e-5\n",
        "EPOCHS = 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TQWRaOnSwbP4",
        "outputId": "557620eb-dda5-4e09-dba5-e113910e293f"
      },
      "outputs": [],
      "source": [
        "print(BATCH_SIZE << 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2n5-4c_8nac0",
        "outputId": "33dbf4f0-6541-4165-f138-05ca87eef2fa"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi\n",
        "device = torch.device(\"cuda\")\n",
        "print(f\"Using device: {torch.cuda.get_device_name(0)}\")\n",
        "if torch.cuda.get_device_capability(0)[0] >= 7:\n",
        "    print(\"[INFO] 支持混合精度\")\n",
        "else:\n",
        "    print(\"[WARNING] 不支持混合精度\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CudB51zBnac0"
      },
      "source": [
        "### 数据预处理"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "D:\\python\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "from tqdm import tqdm\n",
        "from bge import BGE  # 引入 BGE 类\n",
        "\n",
        "# 数据预处理函数\n",
        "def clean_text(text, is_uncased=True):\n",
        "    import re\n",
        "    import string\n",
        "    text = re.sub(r'<.*?>', '', text)                                 # 去除HTML标签\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))  # 去除标点符号\n",
        "    if is_uncased:\n",
        "        text = text.lower()                                           # 转为小写\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()                          # 去除多余空格\n",
        "    return text\n",
        "\n",
        "# 加载数据集\n",
        "def preprocess_dataset(df):\n",
        "    df[\"text\"] = df[\"text\"].apply(clean_text)\n",
        "    return df\n",
        "\n",
        "# 使用 BGE 嵌入进行聚类处理\n",
        "def cluster_large_model_text(df, bge_model, output_path):\n",
        "    # 筛选大模型文本（标签为 1 的样本）\n",
        "    large_model_texts = df[df[\"label\"] == 1][\"text\"].tolist()\n",
        "\n",
        "    # 使用 BGE 嵌入生成向量\n",
        "    embeddings = bge_model.embed_texts(large_model_texts)\n",
        "    np.save(\"bge_vectors.npy\", np.vstack(embeddings))  # 保存嵌入向量\n",
        "    embeddings = np.vstack(embeddings)  # 转换为二维数组\n",
        "\n",
        "    # 使用 K-means 聚类\n",
        "    kmeans = KMeans(n_clusters=7, random_state=42)\n",
        "    cluster_labels = kmeans.fit_predict(embeddings)\n",
        "\n",
        "    # 更新标签\n",
        "    df.loc[df[\"label\"] == 1, \"label\"] = cluster_labels + 1  # 聚类结果从 1 到 7\n",
        "\n",
        "    # 将更新后的数据写入新的文件\n",
        "    df.to_json(output_path, orient=\"records\", lines=True)\n",
        "    print(f\"聚类完成，数据已保存到 {output_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "生成嵌入中:   3%|▎         | 520/14907 [00:47<19:00, 12.62it/s]"
          ]
        }
      ],
      "source": [
        "TRAIN_PATH = \"datasets/train.jsonl\"  # 数据路径\n",
        "OUTPUT_PATH = \"datasets/clustered_train.jsonl\"  # 输出文件路径\n",
        "\n",
        "# 加载 JSONL 数据集\n",
        "data = pd.read_json(TRAIN_PATH, lines=True)\n",
        "data = preprocess_dataset(data)\n",
        "    \n",
        "# 初始化 BGE 模型\n",
        "bge_model = BGE(model_path=\"D:/Study_Work/Electronic_data/CS/AAAUniversity/Machine_Learning/sdxxylysj/Lab3/src/datasets/models/bge-base-en-v1.5\")\n",
        "    \n",
        "# 聚类处理并保存结果\n",
        "cluster_large_model_text(data, bge_model, OUTPUT_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "qFZG8Wffnac0"
      },
      "outputs": [],
      "source": [
        "def clean_text(text, is_uncased=True):\n",
        "    text = re.sub(r'<.*?>', '', text)                                 # 去除HTML标签\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))  # 去除标点符号\n",
        "    if is_uncased:\n",
        "        text = text.lower()                                           # 转为小写（注意：uncased模型需要，其余模型不必）\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()                          # 去除多余空格\n",
        "    return text\n",
        "def preprocess_dataset(df, ds_type=\"DETECT\", is_MLM=False):\n",
        "    row_name=\"sentence\" if ds_type == \"SST2\" else \"text\"    # 本次任务与IMDB均为text，仅SST2为sentence\n",
        "    df[\"text\"] = df[row_name].apply(clean_text)\n",
        "    if not is_MLM:\n",
        "        data_list = df[[\"text\", \"label\"]].to_dict(orient=\"records\")\n",
        "        print(df[\"label\"].value_counts(normalize=True))\n",
        "    else:\n",
        "        data_list = df[\"text\"].to_list()\n",
        "    return data_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_XiMTeb2nac0",
        "outputId": "06fad565-5ea9-4e84-e2c3-620929532e04"
      },
      "outputs": [],
      "source": [
        "def init_data(is_MLM=False):\n",
        "    with open(TRAIN_PATH, \"r\", encoding=\"utf-8\") as file:\n",
        "        train_raw = [json.loads(line) for line in file]\n",
        "    with open(TEST_PATH, \"r\", encoding=\"utf-8\") as file:\n",
        "        test_raw = [json.loads(line) for line in file]\n",
        "\n",
        "    train_df = pd.DataFrame(train_raw)\n",
        "    test_df = pd.DataFrame(test_raw)\n",
        "\n",
        "    train_text_lengths = train_df[\"text\"].apply(len)\n",
        "    test_text_lengths = test_df[\"text\"].apply(len)\n",
        "    print(f\"avg train text: {train_text_lengths.mean():.2f}\")\n",
        "    print(f\"avg test text: {test_text_lengths.mean():.2f}\")\n",
        "\n",
        "    print(train_df.head())\n",
        "    print(test_df.head())\n",
        "\n",
        "    train_list_ = preprocess_dataset(train_df)\n",
        "    test_data = preprocess_dataset(test_df, is_MLM=True)\n",
        "    \n",
        "    train_processed_lengths = [len(item[\"text\"]) for item in train_list_]\n",
        "    test_processed_lengths = [len(text) for text in test_data]\n",
        "    print(f\"avg train text(final): {sum(train_processed_lengths) / len(train_processed_lengths):.2f}\")\n",
        "    print(f\"avg test text(final): {sum(test_processed_lengths) / len(test_processed_lengths):.2f}\")\n",
        "\n",
        "    labels = [item[\"label\"] for item in train_list_]\n",
        "\n",
        "    # 训练：测试+验证 = 8:2\n",
        "    train_data, valid_data = train_test_split(\n",
        "        train_list_, test_size=0.2, random_state=42, stratify=labels\n",
        "    )\n",
        "\n",
        "    # 无监督训练用不到测试集，把原有的训练集和验证集覆盖，保留正式数据划分好的测试集\n",
        "    if is_MLM:\n",
        "        combined_data = preprocess_dataset(pd.read_parquet(IMDB_UNSUPERVISED), ds_type=\"IMDB\", is_MLM=True)\n",
        "        train_data, valid_data = train_test_split(\n",
        "            combined_data, test_size=0.2, random_state=42\n",
        "        )\n",
        "    random.shuffle(train_data)\n",
        "    random.shuffle(valid_data)\n",
        "\n",
        "    print(f\"训练集长度: {len(train_data)}; 验证集长度: {len(valid_data)}; 测试集长度: {len(test_data)}\")\n",
        "\n",
        "    return train_data, valid_data, test_data\n",
        "if __name__ == \"__main__\":\n",
        "    init_data()   # train.jsonl的标签分布为 1:0.532393 | 0:0.467607"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "mEGrKHXnnac0"
      },
      "outputs": [],
      "source": [
        "# data_list是经过预处理的列表，每项包含 text 和 label\n",
        "class DetectDataset(Dataset):\n",
        "    def __init__(self, data_list, tokenizer, max_length=MAX_LENGTH, is_MLM=False):\n",
        "        self.data = data_list\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.is_MLM = is_MLM\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if not self.is_MLM:\n",
        "            text = self.data[idx][\"text\"]\n",
        "        else:\n",
        "            text = self.data[idx]   # MLM 只有一列\n",
        "\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            add_special_tokens=True,    # 加上 [CLS]（开头） 和 [SEP]（结尾），不够的用[PAD]填充\n",
        "            max_length=self.max_length, # 输入序列的最大长度\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,            # 启用截断\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        if not self.is_MLM:\n",
        "            return {key: val.squeeze(0) for key, val in encoding.items()}, torch.tensor(self.data[idx][\"label\"])\n",
        "\n",
        "        return {key: val.squeeze(0) for key, val in encoding.items()}\n",
        "\n",
        "# 这个运行挺快的，就不缓存了\n",
        "def load_data(tokenizer, is_MLM=False):\n",
        "    train_data, val_data, test_data = init_data(is_MLM)\n",
        "    train_dataset = DetectDataset(train_data, tokenizer, is_MLM=is_MLM)\n",
        "    val_dataset = DetectDataset(val_data, tokenizer, is_MLM=is_MLM)\n",
        "    test_dataset = DetectDataset(test_data, tokenizer, is_MLM=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE_TEST)\n",
        "\n",
        "    # 自动处理掩码，随机掩码（默认15%的）token并使用原始input_ids作为labels（非掩码位置设为-100以忽略损失）\n",
        "    collate_fn = None if not is_MLM else DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn, num_workers=NUM_WORKERS, pin_memory=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE_TEST, collate_fn=collate_fn, num_workers=2, pin_memory=True)\n",
        "    # for batch in train_loader:\n",
        "    #    inputs, labels = batch\n",
        "    #    print(\"Inputs:\", inputs)\n",
        "    #    print(\"Labels:\", labels)\n",
        "    #    break\n",
        "\n",
        "    return train_loader, val_loader, test_loader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1ubGdNewbP6"
      },
      "source": [
        "初始化分词器与数据加载器"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y5OI-Qu3nac0",
        "outputId": "9b4583e2-2a41-4a38-a68c-89bc990ffa7c"
      },
      "outputs": [],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(MODEL_PATH)\n",
        "train_loader, valid_loader, test_loader = load_data(tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vdlDDCMTwbP6"
      },
      "source": [
        "### 训练器与测试器\n",
        "\n",
        "在`colab`上训练时每个`batch`都至少输出一条信息，不然会因为长时间无响应而断开连接"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "1x0PySoCnac1"
      },
      "outputs": [],
      "source": [
        "class ModelTrainer:\n",
        "    def __init__(self, model, train_loader, val_loader, epochs=3, lr=2e-5, is_MLM=False):\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.model = model.to(self.device)\n",
        "        self.train_loader = train_loader\n",
        "        self.val_loader = val_loader\n",
        "        self.is_MLM = is_MLM\n",
        "        self.epochs = epochs\n",
        "        self.lr = lr\n",
        "\n",
        "        # AdamW 是加入了权重衰减的Adam\n",
        "        self.optimizer = AdamW(self.model.parameters(), lr=lr)\n",
        "        # 学习率从0增加到设定的最大值然后逐渐线性下降\n",
        "        self.scheduler = get_linear_schedule_with_warmup(\n",
        "            self.optimizer,\n",
        "            num_warmup_steps=2,\n",
        "            num_training_steps=len(train_loader)*epochs\n",
        "        )\n",
        "        self.loss_func = nn.CrossEntropyLoss()\n",
        "        # 拓展：混合精度训练\n",
        "        self.scaler = torch.amp.GradScaler(device=self.device.type)\n",
        "\n",
        "        # 训练记录\n",
        "        self.train_losses, self.val_losses = [], []\n",
        "        self.train_accs, self.val_accs = [], []\n",
        "        self.train_precisions, self.val_precisions = [], []\n",
        "        self.train_recalls, self.val_recalls = [], []\n",
        "        self.train_f1s, self.val_f1s = [], []\n",
        "        self.best_accuracy = 0.0\n",
        "        self.min_loss = float(\"inf\")\n",
        "        self.save_name = MODEL_E2E_NAME\n",
        "        self.bear_cnt = 0\n",
        "        self.epc=1\n",
        "        self.mlm_loss_train = []\n",
        "        self.mlm_loss_valid = []\n",
        "\n",
        "        # print(f\"[DEBUG] train loader length:{len(self.train_loader)}\")\n",
        "\n",
        "    def train(self):\n",
        "        for epoch in range(self.epochs):\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.empty_cache()\n",
        "            # 训练\n",
        "            # print(f\"[DEBUG] train\")\n",
        "            train_loss, train_acc, train_precision, train_recall, train_f1 = self.__train_part()\n",
        "\n",
        "            # 验证\n",
        "            # print(f\"[DEBUG] valid\")\n",
        "            val_loss, val_acc, val_precision, val_recall, val_f1 = self._valid_part()\n",
        "            self.epc+=1\n",
        "\n",
        "            # 保存最佳模型\n",
        "            if not self.is_MLM and val_acc > self.best_accuracy:\n",
        "                self.best_accuracy = val_acc\n",
        "                self.bear_cnt=0\n",
        "                torch.save(self.model.state_dict(), self.save_name)\n",
        "            if val_loss < self.min_loss:\n",
        "                self.min_loss = val_loss\n",
        "                self.bear_cnt = 0\n",
        "            else:\n",
        "                self.bear_cnt += 1\n",
        "                if self.bear_cnt >= 3:\n",
        "                    print(\"[INFO] Early stop\")\n",
        "                    break\n",
        "\n",
        "            print(f\"Epoch {epoch + 1}/{self.epochs}\")\n",
        "            print(\n",
        "                f\"Train Loss: {train_loss:.4f}, \"\n",
        "                f\"Acc: {f'{train_acc:.4f}'             if train_acc       is not None else 'N/A'}, \"\n",
        "                f\"Precision: {f'{train_precision:.4f}' if train_precision is not None else 'N/A'}, \"\n",
        "                f\"Recall: {f'{train_recall:.4f}'       if train_recall    is not None else 'N/A'}, \"\n",
        "                f\"F1: {f'{train_f1:.4f}'               if train_f1        is not None else 'N/A'} | \"\n",
        "                f\"Val Loss: {val_loss:.4f}, \"\n",
        "                f\"Acc: {f'{val_acc:.4f}'               if val_acc         is not None else 'N/A'}, \"\n",
        "                f\"Precision: {f'{val_precision:.4f}'   if val_precision   is not None else 'N/A'}, \"\n",
        "                f\"Recall: {f'{val_recall:.4f}'         if val_recall      is not None else 'N/A'}, \"\n",
        "                f\"F1: {f'{val_f1:.4f}'                 if val_f1          is not None else 'N/A'}\"\n",
        "            )\n",
        "\n",
        "        return (\n",
        "            self.train_losses,\n",
        "            self.val_losses,\n",
        "            self.train_accs,\n",
        "            self.val_accs,\n",
        "            self.train_precisions,\n",
        "            self.val_precisions,\n",
        "            self.train_recalls,\n",
        "            self.val_recalls,\n",
        "            self.train_f1s,\n",
        "            self.val_f1s,\n",
        "        )\n",
        "\n",
        "    def __train_part(self):\n",
        "        # print(f\"[DEBUG] jump into __train_part\")\n",
        "        self.model.train()\n",
        "        # print(f\"[DEBUG] {self.model.training}\")\n",
        "        total_loss = 0.0\n",
        "        all_labels = []\n",
        "        all_preds = []\n",
        "        cc=1\n",
        "        for batch in self.train_loader:\n",
        "            print(f\"[DEBUG] batch/epoch: {cc}/{self.epc}\")\n",
        "            cc+=1\n",
        "            if not self.is_MLM:\n",
        "                inputs, labels = batch\n",
        "                # print(f\"[DEBUG] {inputs}\")\n",
        "                # print(f\"[DEBUG] {labels}\")\n",
        "                labels = labels.to(self.device)\n",
        "            else:\n",
        "                inputs = batch\n",
        "                labels = inputs[\"input_ids\"].to(self.device)\n",
        "\n",
        "            input_ids = inputs[\"input_ids\"].to(self.device)\n",
        "            attention_mask = inputs[\"attention_mask\"].to(self.device)\n",
        "\n",
        "            # print(f\"[DEBUG] {input_ids.shape}\")\n",
        "            # print(f\"[DEBUG] {attention_mask.shape}\")\n",
        "            # 拓展： 混合精度前向传播\n",
        "            with torch.amp.autocast(device_type=self.device.type):\n",
        "                if not self.is_MLM:\n",
        "                    outputs = self.model(input_ids=input_ids,attention_mask=attention_mask)\n",
        "                    # print(f\"[DEBUG] {outputs}\")\n",
        "                    loss = self.loss_func(outputs, labels)\n",
        "                else:\n",
        "                    outputs = self.model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "                    loss = outputs.loss # MLM内含loss\n",
        "\n",
        "            # print(f\"[DEBUG] {loss.item()}\")\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "            #loss.backward()\n",
        "            self.scaler.scale(loss).backward()  # 反向传播前对loss乘一个缩放因子，避免梯度过小导致的下溢\n",
        "            self.scaler.unscale_(self.optimizer)  # 反缩放，梯度裁剪通常是基于原始梯度值进行的\n",
        "            # 梯度裁剪 <-- 防止爆炸、稳定训练、加速收敛\n",
        "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
        "            #self.optimizer.step()\n",
        "            self.scaler.step(self.optimizer)\n",
        "            self.scaler.update()  # 更新缩放因子，为下次迭代做准备\n",
        "            self.scheduler.step()\n",
        "\n",
        "            if self.is_MLM:\n",
        "                self.mlm_loss_train.append(loss.item())\n",
        "            total_loss += loss.item()\n",
        "            if not self.is_MLM:\n",
        "                tmp = torch.softmax(outputs, dim=1)\n",
        "                preds = torch.argmax(tmp, dim=1).cpu().numpy()\n",
        "                all_labels.extend(labels.cpu().numpy())\n",
        "                all_preds.extend(preds)\n",
        "\n",
        "        train_loss = total_loss / len(self.train_loader)\n",
        "\n",
        "        if not self.is_MLM:\n",
        "            train_acc = accuracy_score(all_labels, all_preds)\n",
        "            train_precision = precision_score(all_labels, all_preds, average=\"weighted\")\n",
        "            train_recall = recall_score(all_labels, all_preds, average=\"weighted\")\n",
        "            train_f1 = f1_score(all_labels, all_preds, average=\"weighted\")\n",
        "\n",
        "            # 保存记录\n",
        "            self.train_losses.append(train_loss)\n",
        "            self.train_accs.append(train_acc)\n",
        "            self.train_precisions.append(train_precision)\n",
        "            self.train_recalls.append(train_recall)\n",
        "            self.train_f1s.append(train_f1)\n",
        "            return train_loss, train_acc, train_precision, train_recall, train_f1\n",
        "        else:\n",
        "            return train_loss, None, None, None, None\n",
        "\n",
        "    def _valid_part(self):\n",
        "        self.model.eval()\n",
        "        total_loss = 0.0\n",
        "        all_labels = []\n",
        "        all_preds = []\n",
        "        cc=1\n",
        "        with torch.no_grad():\n",
        "            for batch in self.val_loader:\n",
        "                print(f\"[DEBUG] batch/epoch: {cc}/{self.epc}\")\n",
        "                cc+=1\n",
        "                if not self.is_MLM:\n",
        "                    inputs, labels = batch\n",
        "                    labels = labels.to(self.device)\n",
        "                else:\n",
        "                    inputs = batch\n",
        "                    labels = inputs[\"input_ids\"].to(self.device)\n",
        "\n",
        "                input_ids = inputs[\"input_ids\"].to(self.device)\n",
        "                attention_mask = inputs[\"attention_mask\"].to(self.device)\n",
        "\n",
        "                # 拓展： 混合精度前向传播\n",
        "                with torch.amp.autocast(device_type=self.device.type):\n",
        "                    if not self.is_MLM:\n",
        "                        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "                        loss = self.loss_func(outputs, labels)\n",
        "                    else:\n",
        "                        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "                        loss = outputs.loss\n",
        "\n",
        "                if self.is_MLM:\n",
        "                    self.mlm_loss_valid.append(loss.item())\n",
        "\n",
        "                total_loss += loss.item()\n",
        "                if not self.is_MLM:\n",
        "                    tmp = torch.softmax(outputs, dim=1)\n",
        "                    preds = torch.argmax(tmp, dim=1).cpu().numpy()\n",
        "                    all_labels.extend(labels.cpu().numpy())\n",
        "                    all_preds.extend(preds)\n",
        "\n",
        "        val_loss = total_loss / len(self.val_loader)\n",
        "\n",
        "        if not self.is_MLM:\n",
        "            val_acc = accuracy_score(all_labels, all_preds)\n",
        "            val_precision = precision_score(all_labels, all_preds, average=\"weighted\")\n",
        "            val_recall = recall_score(all_labels, all_preds, average=\"weighted\")\n",
        "            val_f1 = f1_score(all_labels, all_preds, average=\"weighted\")\n",
        "\n",
        "            self.val_losses.append(val_loss)\n",
        "            self.val_accs.append(val_acc)\n",
        "            self.val_precisions.append(val_precision)\n",
        "            self.val_recalls.append(val_recall)\n",
        "            self.val_f1s.append(val_f1)\n",
        "\n",
        "            return val_loss, val_acc, val_precision, val_recall, val_f1\n",
        "        else:\n",
        "            return val_loss, None, None, None, None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "e58nHX3Tnac1"
      },
      "outputs": [],
      "source": [
        "class ModelEvaluator:\n",
        "    def __init__(\n",
        "        self,\n",
        "        test_loader=None,\n",
        "        compare_lists=None,\n",
        "    ):\n",
        "        self.test_loader = test_loader\n",
        "        self.compare_lists = compare_lists\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.fpr= None\n",
        "        self.tpr= None\n",
        "        self.roc_auc= None\n",
        "\n",
        "    def evaluate_model(self, model, analyse=True):\n",
        "        '''\n",
        "        当处于分析模式时，不绘制混淆矩阵，直接返回评估指标\n",
        "        特别注意：本方法要求测试集带有标签\n",
        "        '''\n",
        "        model.to(self.device)\n",
        "        model.eval()\n",
        "\n",
        "        all_labels = []\n",
        "        all_preds = []\n",
        "        cc=0\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in self.test_loader:\n",
        "                print(f\"[DEBUG] Testing batch:{cc}\")\n",
        "                cc+=1\n",
        "                inputs = {key: val.to(self.device) for key, val in inputs.items()}\n",
        "                labels = labels.to(self.device)\n",
        "                outputs= model(input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"])\n",
        "                tmp = torch.softmax(outputs, dim=1)\n",
        "                preds = torch.argmax(tmp, dim=1).tolist()\n",
        "\n",
        "                all_labels.extend(labels.tolist())\n",
        "                all_preds.extend(preds)\n",
        "\n",
        "        # 计算评估指标\n",
        "        test_acc = accuracy_score(all_labels, all_preds)\n",
        "        test_precision = precision_score(all_labels, all_preds)\n",
        "        test_recall = recall_score(all_labels, all_preds)\n",
        "        test_f1 = f1_score(all_labels, all_preds)\n",
        "        self.fpr, self.tpr, _ = roc_curve(all_labels, all_preds)\n",
        "        self.roc_auc = auc(self.fpr, self.tpr)\n",
        "\n",
        "        print(f\"Test Accuracy: {test_acc:.4f}\")\n",
        "        print(f\"Test Precision: {test_precision:.4f}\")\n",
        "        print(f\"Test Recall: {test_recall:.4f}\")\n",
        "        print(f\"Test F1 Score: {test_f1:.4f}\")\n",
        "        if analyse:\n",
        "            return test_acc, test_precision, test_recall, test_f1\n",
        "\n",
        "        # 绘制混淆矩阵\n",
        "        cm = confusion_matrix(all_labels, all_preds, labels=[i for i in range(NUM_LABELS)])\n",
        "        disp = ConfusionMatrixDisplay(\n",
        "            confusion_matrix=cm, display_labels=[classes[i] for i in range(NUM_LABELS)]\n",
        "        )\n",
        "        disp.plot(cmap=plt.cm.Blues)\n",
        "        plt.title(\"Confusion Matrix\")\n",
        "        plt.show()\n",
        "\n",
        "    def detect(self, model):\n",
        "        \"\"\"\n",
        "        评估无标签测试集\n",
        "        \"\"\"\n",
        "        model.to(self.device)\n",
        "        model.eval()\n",
        "\n",
        "        all_preds = []\n",
        "        cc = 0\n",
        "        with torch.no_grad():\n",
        "            for inputs in self.test_loader:\n",
        "                print(f\"[DEBUG] Testing batch:{cc}\")\n",
        "                cc += 1\n",
        "                inputs = {key: val.to(self.device) for key, val in inputs.items()}\n",
        "                outputs = model(input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"])\n",
        "                tmp = torch.softmax(outputs, dim=1)\n",
        "                preds = torch.argmax(tmp, dim=1).tolist()\n",
        "\n",
        "                all_preds.extend(preds)\n",
        "\n",
        "        print(f\"Predicted Labels: {all_preds}\")\n",
        "        return all_preds\n",
        "\n",
        "    def collect_errors(self, model, tokenizer):\n",
        "        model.to(self.device)\n",
        "        model.eval()\n",
        "\n",
        "        errors = []  # 存储错误分类的样本\n",
        "        cc=1\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in self.test_loader:\n",
        "                print(f\"[DEBUG] Testing batch:{cc}\")\n",
        "                cc+=1\n",
        "                inputs = {key: val.to(self.device) for key, val in inputs.items()}\n",
        "                labels = labels.to(self.device)\n",
        "\n",
        "                # 模型预测\n",
        "                outputs = model(input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"])\n",
        "                probs = torch.softmax(outputs, dim=1)\n",
        "                preds = torch.argmax(probs, dim=1)\n",
        "\n",
        "                # 收集错误分类的样本\n",
        "                for i in range(len(labels)):\n",
        "                    if preds[i] != labels[i]:\n",
        "                        text = tokenizer.decode(inputs[\"input_ids\"][i], skip_special_tokens=True)\n",
        "                        errors.append({\n",
        "                            \"text\": text,\n",
        "                            \"label\": labels[i].item(),\n",
        "                            \"pred\": preds[i].item()\n",
        "                        })\n",
        "\n",
        "        return errors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HoPM0B8Vnac1"
      },
      "source": [
        "### 模型结构\n",
        "\n",
        "直接将 `bert-base-uncased` 作为基础编码器，接上分类层（全连接层+softmax/sigmoid激活）\n",
        "\n",
        "这里只返回`logits`，具体\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BafzyRExnac1"
      },
      "outputs": [],
      "source": [
        "# 本部分为bert输出维度展示，可不用运行\n",
        "bert_out = BertModel.from_pretrained(MODEL_PATH).config.hidden_size\n",
        "print(f\"bert_out: {bert_out}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "V2CJcccvnac1"
      },
      "outputs": [],
      "source": [
        "class BertClassifier(nn.Module):\n",
        "    def __init__(self, mlm=False):\n",
        "        super(BertClassifier, self).__init__()\n",
        "        model_path = MODEL_PATH if not mlm else MODEL_PRE_PATH\n",
        "        self.bert = BertModel.from_pretrained(model_path)\n",
        "\n",
        "        self.dropout = nn.Dropout(0.6)\n",
        "        self.classifier = nn.Linear(self.bert.config.hidden_size, NUM_LABELS)  # 分类层\n",
        "        #self.activation = nn.Sigmoid() # 二分类\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        # BERT 编码器\n",
        "        outputs = self.bert(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask # 区别有效区域与填充区域\n",
        "        )\n",
        "        # 基于 [CLS] token 的隐藏状态，经过一个额外的全连接层和 tanh 激活函数后的结果\n",
        "        cls_output = outputs.pooler_output\n",
        "\n",
        "        # Dropout + 分类层\n",
        "        #cls_output = self.dropout(cls_output)\n",
        "        logits = self.classifier(cls_output)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uqQ7xncqnac1"
      },
      "outputs": [],
      "source": [
        "# 理论上应该也可以直接调用这个，不过自己编的分类器更灵活一些\n",
        "# model = BertForSequenceClassification.from_pretrained(MODEL_PATH, num_labels=NUM_LABELS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "iDLAYyPOnac1"
      },
      "outputs": [],
      "source": [
        "def compare_eval(initial_metrics, final_metrics):\n",
        "    # 对比前后的性能变化\n",
        "    metrics = [\"Accuracy\", \"Precision\", \"Recall\", \"F1 Score\"]\n",
        "    initial_values = [initial_metrics[0], initial_metrics[1], initial_metrics[2], initial_metrics[3]]\n",
        "    final_values = [final_metrics[0], final_metrics[1], final_metrics[2], final_metrics[3]]\n",
        "\n",
        "    x = range(len(metrics))\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.bar(x, initial_values, width=0.4, label=\"Before Fine-tuning\", align=\"center\")\n",
        "    plt.bar([i + 0.4 for i in x], final_values, width=0.4, label=\"After Fine-tuning\", align=\"center\")\n",
        "    plt.xticks([i + 0.2 for i in x], metrics)\n",
        "    plt.ylabel(\"Score\")\n",
        "    plt.title(\"Performance Comparison Before and After Fine-tuning\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "def train_show(trainer):\n",
        "    plt.figure(figsize=(15, 10))\n",
        "\n",
        "    # 准确率\n",
        "    plt.subplot(2, 3, 1)\n",
        "    plt.plot(trainer.train_accs, label=\"Train Accuracy\")\n",
        "    plt.plot(trainer.val_accs, label=\"Validation Accuracy\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.title(\"Training and Validation Accuracy\")\n",
        "    plt.legend()\n",
        "\n",
        "    # 精确率\n",
        "    plt.subplot(2, 3, 2)\n",
        "    plt.plot(trainer.train_precisions, label=\"Train Precision\")\n",
        "    plt.plot(trainer.val_precisions, label=\"Validation Precision\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Precision\")\n",
        "    plt.title(\"Training and Validation Precision\")\n",
        "    plt.legend()\n",
        "\n",
        "    # 损失\n",
        "    plt.subplot(2, 3, 3)\n",
        "    plt.plot(trainer.train_losses, label=\"Train Loss\")\n",
        "    plt.plot(trainer.val_losses, label=\"Validation Loss\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.title(\"Training and Validation Loss\")\n",
        "    plt.legend()\n",
        "\n",
        "    # 召回率\n",
        "    plt.subplot(2, 3, 4)\n",
        "    plt.plot(trainer.train_recalls, label=\"Train Recall\")\n",
        "    plt.plot(trainer.val_recalls, label=\"Validation Recall\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Recall\")\n",
        "    plt.title(\"Training and Validation Recall\")\n",
        "    plt.legend()\n",
        "\n",
        "    # F1\n",
        "    plt.subplot(2, 3, 5)\n",
        "    plt.plot(trainer.train_f1s, label=\"Train F1\")\n",
        "    plt.plot(trainer.val_f1s, label=\"Validation F1\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"F1 Score\")\n",
        "    plt.title(\"Training and Validation F1 Score\")\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLVFHsYhnac1"
      },
      "source": [
        "#### 端到端训练\n",
        "\n",
        "加载预训练权重，更新整个网络的参数\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HJaHKu16qQ3F",
        "outputId": "e29f8f55-c5fe-44e7-82bf-5cc89e197c42"
      },
      "outputs": [],
      "source": [
        "# 初始化模型和数据\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = BertClassifier()\n",
        "model.to(device)\n",
        "tokenizer = BertTokenizer.from_pretrained(MODEL_PATH)\n",
        "\n",
        "trainer = ModelTrainer(\n",
        "    model=model,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=valid_loader,\n",
        "    epochs=EPOCHS,\n",
        "    lr=LEARNING_RATE,\n",
        ")\n",
        "\n",
        "evaluator = ModelEvaluator(test_loader=valid_loader)\n",
        "print(\"端到端微调前的性能：\")\n",
        "initial_metrics = evaluator.evaluate_model(model=model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Q2HZexx_nac1",
        "outputId": "046f799d-527d-4d14-c1e9-b706ebf309e3"
      },
      "outputs": [],
      "source": [
        "if torch.cuda.is_available():\n",
        "  torch.cuda.empty_cache()\n",
        "trainer.train()\n",
        "train_show(trainer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "a0B6fJb1qVZK",
        "outputId": "32a76447-65f2-4a09-fcf6-7df6d02d7178"
      },
      "outputs": [],
      "source": [
        "if torch.cuda.is_available():\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "print(\"端到端微调后的性能：\")\n",
        "final_metrics = evaluator.evaluate_model(model=model)\n",
        "\n",
        "# 对比端到端训练前后性能变化\n",
        "compare_eval(initial_metrics, final_metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LuB40DMBwbP8",
        "outputId": "cf157343-1653-4b64-f969-75d4fae95faa"
      },
      "outputs": [],
      "source": [
        "# 评估测试集\n",
        "evaluator.test_loader= test_loader\n",
        "predicted_labels = evaluator.detect(model=model)\n",
        "\n",
        "# 将预测结果逐行写入 txt 文件\n",
        "output_file = \"./result/submit.txt\"\n",
        "with open(output_file, \"w\", encoding=\"utf-8\") as file:\n",
        "    for label in predicted_labels:\n",
        "        file.write(f\"{label}\\n\")\n",
        "\n",
        "print(f\"预测结果已写入文件: {output_file}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zx0hl59znac1"
      },
      "source": [
        "#### 先预训练后微调"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKdxK-eAnac2"
      },
      "source": [
        "##### 预训练"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "43oF3Ctfnac2"
      },
      "outputs": [],
      "source": [
        "if torch.cuda.is_available():\n",
        "  torch.cuda.empty_cache()\n",
        "# 加载预训练模型\n",
        "is_MLM = True\n",
        "model = BertForMaskedLM.from_pretrained(MODEL_PATH)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(MODEL_PATH)\n",
        "\n",
        "train_loader, val_loader, test_loader = load_data(tokenizer, is_MLM)\n",
        "\n",
        "mlm_trainer = ModelTrainer(model=model, train_loader=train_loader, val_loader=val_loader, epochs=2, lr=LEARNING_RATE, is_MLM=is_MLM)\n",
        "\n",
        "# 损失在3.5以下（通用预训练MLM损失）比较正常\n",
        "mlm_trainer.train()\n",
        "\n",
        "model.save_pretrained(MODEL_PRE_PATH, safe_serialization=False)\n",
        "tokenizer.save_pretrained(MODEL_PRE_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0D-1z5UyKT6O"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8, 16))\n",
        "plt.plot(mlm_trainer.mlm_loss_train[30:], label=\"Train Loss\")\n",
        "plt.plot(mlm_trainer.mlm_loss_valid[30:], label=\"Validation Loss\")\n",
        "plt.xlabel(\"Batch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Training and Validation Loss\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8oAtM1THnac2"
      },
      "source": [
        "##### 微调"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wEZQXoIynac2"
      },
      "outputs": [],
      "source": [
        "def layerwise_lr_decay(model, lr=5e-5, decay=0.8):\n",
        "    # 分类头参数（学习率较高）\n",
        "    # classifier_params = list(model.classifier.parameters()) + list(model.dropout.parameters())\n",
        "    classifier_params = list(model.classifier.parameters())\n",
        "\n",
        "    # BERT本体参数（学习率较低）\n",
        "    bert_params = []\n",
        "    bert_lr = lr *0.5\n",
        "    for i, layer in enumerate(model.bert.encoder.layer[::-1]):  # 从最后一层开始\n",
        "        bert_params.append({\"params\": layer.parameters(), \"lr\": bert_lr})\n",
        "        bert_lr *= decay  # 每层学习率衰减\n",
        "\n",
        "    # 分类头学习率较高，放在前面确保优化器优先处理\n",
        "    return [{\"params\": classifier_params, \"lr\": lr}] + bert_params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D0F7WkcDnac2"
      },
      "outputs": [],
      "source": [
        "# 初始化模型和数据\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = BertClassifier(mlm=True)\n",
        "model.to(device)\n",
        "tokenizer = BertTokenizer.from_pretrained(MODEL_PRE_PATH)\n",
        "train_loader, val_loader, test_loader = load_data(tokenizer)\n",
        "\n",
        "params = layerwise_lr_decay(model)\n",
        "optimizer = AdamW(params, weight_decay=0.01)\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer, num_warmup_steps=2, num_training_steps=len(train_loader) * EPOCHS\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RcUqJd0p-F8b"
      },
      "outputs": [],
      "source": [
        "# 初始化训练器\n",
        "trainer = ModelTrainer(\n",
        "    model=model,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    epochs=EPOCHS,\n",
        "    lr=5e-5,\n",
        ")\n",
        "trainer.optimizer = optimizer  # 覆盖默认优化器 <-- 引入分层学习率\n",
        "trainer.scheduler = scheduler\n",
        "trainer.save_name = MODEL_PRE_NAME\n",
        "\n",
        "evaluator = ModelEvaluator(test_loader=test_loader)\n",
        "print(\"微调前的性能：\")\n",
        "initial_metrics = evaluator.evaluate_model(model=model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X2WYxiDanac2"
      },
      "outputs": [],
      "source": [
        "if torch.cuda.is_available():\n",
        "    import time\n",
        "    torch.cuda.empty_cache()\n",
        "    start_time = time.time()\n",
        "    torch.cuda.reset_peak_memory_stats()\n",
        "    initial_memory = torch.cuda.memory_allocated()\n",
        "\n",
        "# 开始训练\n",
        "trainer.train()\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    end_time = time.time()\n",
        "    peak_memory = torch.cuda.max_memory_allocated()\n",
        "    elapsed_time = end_time - start_time\n",
        "    memory_used = peak_memory - initial_memory\n",
        "    print(f\"Elapsed Time: {elapsed_time:.2f} seconds\")\n",
        "    print(f\"Memory Used: {memory_used / (1024 ** 2):.2f} MB\\n\")\n",
        "\n",
        "train_show(trainer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uST4F36I-N4V"
      },
      "outputs": [],
      "source": [
        "if torch.cuda.is_available():\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "# 测试模型效果\n",
        "print(\"微调后的性能：\")\n",
        "final_metrics = evaluator.evaluate_model(model=model)\n",
        "\n",
        "# 对比微调前后性能变化\n",
        "compare_eval(initial_metrics, final_metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JaQXaCvC9_U_"
      },
      "source": [
        "### 对比"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o9oEjXhM-C_6"
      },
      "outputs": [],
      "source": [
        "if torch.cuda.is_available():\n",
        "  torch.cuda.empty_cache()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = BertClassifier()\n",
        "model.load_state_dict(torch.load(MODEL_E2E_NAME, map_location=device))\n",
        "model.to(device)\n",
        "tokenizer = BertTokenizer.from_pretrained(MODEL_PATH)\n",
        "_, _, test_loader = load_data(tokenizer, is_check=True)\n",
        "evaluator = ModelEvaluator(test_loader=test_loader)\n",
        "print(\"端到端的性能：\")\n",
        "e2e=evaluator.evaluate_model(model, is_check=True)\n",
        "fpr_e2e, tpr_e2e, roc_auc_e2e = evaluator.fpr, evaluator.tpr, evaluator.roc_auc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7A17hYRX-FFj"
      },
      "outputs": [],
      "source": [
        "model = BertClassifier(mlm=True)\n",
        "model.load_state_dict(torch.load(MODEL_PRE_NAME, map_location=device))\n",
        "model.to(device)\n",
        "tokenizer = BertTokenizer.from_pretrained(MODEL_PRE_PATH)\n",
        "_, _, test_loader = load_data(tokenizer, is_check=True)\n",
        "evaluator = ModelEvaluator(test_loader=test_loader)\n",
        "print(\"预训练&微调的性能：\")\n",
        "mlm = evaluator.evaluate_model(model=model, is_check=True)\n",
        "fpr_mlm, tpr_mlm, roc_auc_mlm = evaluator.fpr, evaluator.tpr, evaluator.roc_auc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NbZvvxwj-IA_"
      },
      "outputs": [],
      "source": [
        "compare_eval(e2e, mlm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bK7BpDMgDz2y"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr_e2e, tpr_e2e, label=f\"End-to-End (AUC = {roc_auc_e2e:.2f})\")\n",
        "plt.plot(fpr_mlm, tpr_mlm, label=f\"Pre-trained & Fine-tuned (AUC = {roc_auc_mlm:.2f})\")\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.plot([0, 1], [0, 1], \"k--\", lw=2, label=\"Random Guessing\")\n",
        "# 图形设置\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel(\"FP Rate\")\n",
        "plt.ylabel(\"TP Rate\")\n",
        "plt.title(\"ROC-AUC Curve Comparison\")\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VgS546eVHQsl"
      },
      "outputs": [],
      "source": [
        "if torch.cuda.is_available():\n",
        "  torch.cuda.empty_cache()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = BertClassifier()\n",
        "model.load_state_dict(torch.load(MODEL_E2E_NAME, map_location=device))\n",
        "model.to(device)\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(MODEL_PATH)\n",
        "_, _, test_loader = load_data(tokenizer, is_check=True)\n",
        "#lengt=1000\n",
        "#test_dataset = test_loader.dataset  # 获取原始数据集\n",
        "#test_subset = Subset(test_dataset, list(range(lengt)))\n",
        "#test_loader = DataLoader(test_subset, batch_size=test_loader.batch_size, shuffle=False)\n",
        "\n",
        "\n",
        "evaluator = ModelEvaluator(test_loader=test_loader)\n",
        "\n",
        "errors = evaluator.collect_errors(model=model, tokenizer=tokenizer)\n",
        "print(f\"使用的测试集大小：{len(test_loader.dataset)} | 错误分类样本数量: {len(errors)}\")\n",
        "for error in errors[:10]:\n",
        "    print(f\"Text: {error['text']}\")\n",
        "    print(f\"True Label: {error['label']}, Pred: {error['pred']}\")\n",
        "    print(\"-\" * 50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hr4UPWr_hFHn"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = BertClassifier()\n",
        "model.load_state_dict(torch.load(MODEL_PRE_NAME, map_location=device))\n",
        "model.to(device)\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(MODEL_PRE_PATH)\n",
        "_, _, test_loader = load_data(tokenizer, is_check=True)\n",
        "#lengt=1000\n",
        "#test_dataset = test_loader.dataset  # 获取原始数据集\n",
        "#test_subset = Subset(test_dataset, list(range(lengt)))\n",
        "#test_loader = DataLoader(test_subset, batch_size=test_loader.batch_size, shuffle=False)\n",
        "\n",
        "\n",
        "evaluator = ModelEvaluator(test_loader=test_loader)\n",
        "\n",
        "errors = evaluator.collect_errors(model=model, tokenizer=tokenizer)\n",
        "print(f\"使用的测试集大小：{len(test_loader.dataset)} | 错误分类样本数量: {len(errors)}\")\n",
        "for error in errors[:10]:\n",
        "    print(f\"Text: {error['text']}\")\n",
        "    print(f\"True Label: {error['label']}, Pred: {error['pred']}\")\n",
        "    print(\"-\" * 50)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "YLVFHsYhnac1",
        "TKdxK-eAnac2"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
