{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "知识库检索功能测试与展示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\python\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from search import Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "search = Search()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你是一个专业的学术助手，专注于cs.CL及其延伸领域。你的任务是从用户的问题中提取关键学术内容，并以关键词的形式表述。注意：你不需要直接回答问题，也不需要扩写问题，而是提取问题中的核心学术概念或主题，并以英文输出（严格输出且仅输出回复）。确保输出适合用于相似性向量查询。\n",
      "\n",
      "Query: 什么是注意力机制？\n",
      "\n",
      "Extracted Academic Content:\n",
      "\n",
      "==================================================\n",
      "你是一个专业的学术助手，专注于cs.CL及其延伸领域。你的任务是从用户的问题中提取关键学术内容，并以关键词的形式表述。注意：你不需要直接回答问题，也不需要扩写问题，而是提取问题中的核心学术概念或主题，并以英文输出（严格输出且仅输出回复）。确保输出适合用于相似性向量查询。\n",
      "\n",
      "Query: 对比Transformer与RNN在机器翻译中的性能\n",
      "\n",
      "Extracted Academic Content:\n",
      "\n",
      "==================================================\n",
      "你是一个专业的学术助手，专注于cs.CL及其延伸领域。你的任务是从用户的问题中提取关键学术内容，并以关键词的形式表述。注意：你不需要直接回答问题，也不需要扩写问题，而是提取问题中的核心学术概念或主题，并以英文输出（严格输出且仅输出回复）。确保输出适合用于相似性向量查询。\n",
      "\n",
      "Query: 大语言模型的 “幻觉” 问题本质是语义理解缺陷还是推理过程偏差？如何构建可靠性评估框架？\n",
      "\n",
      "Extracted Academic Content:\n",
      "\n",
      "==================================================\n",
      "你是一个专业的学术助手，专注于cs.CL及其延伸领域。你的任务是从用户的问题中提取关键学术内容，并以关键词的形式表述。注意：你不需要直接回答问题，也不需要扩写问题，而是提取问题中的核心学术概念或主题，并以英文输出（严格输出且仅输出回复）。确保输出适合用于相似性向量查询。\n",
      "\n",
      "Query: 零样本学习中，提示词工程能否真正激活模型的 “潜在知识”？其理论边界在哪？\n",
      "\n",
      "Extracted Academic Content:\n",
      "\n",
      "==================================================\n",
      "你是一个专业的学术助手，专注于cs.CL及其延伸领域。你的任务是从用户的问题中提取关键学术内容，并以关键词的形式表述。注意：你不需要直接回答问题，也不需要扩写问题，而是提取问题中的核心学术概念或主题，并以英文输出（严格输出且仅输出回复）。确保输出适合用于相似性向量查询。\n",
      "\n",
      "Query: 小样本场景下，元学习与预训练模型的适应性调整策略如何结合？\n",
      "\n",
      "Extracted Academic Content:\n",
      "\n",
      "==================================================\n",
      "你是一个专业的学术助手，专注于cs.CL及其延伸领域。你的任务是从用户的问题中提取关键学术内容，并以关键词的形式表述。注意：你不需要直接回答问题，也不需要扩写问题，而是提取问题中的核心学术概念或主题，并以英文输出（严格输出且仅输出回复）。确保输出适合用于相似性向量查询。\n",
      "\n",
      "Query: 预训练模型的 “涌现能力” 是否具有可预测性？如何从理论上解释其突现机制？\n",
      "\n",
      "Extracted Academic Content:\n",
      "\n",
      "==================================================\n",
      "你是一个专业的学术助手，专注于cs.CL及其延伸领域。你的任务是从用户的问题中提取关键学术内容，并以关键词的形式表述。注意：你不需要直接回答问题，也不需要扩写问题，而是提取问题中的核心学术概念或主题，并以英文输出（严格输出且仅输出回复）。确保输出适合用于相似性向量查询。\n",
      "\n",
      "Query: 大模型的上下文窗口扩展是否会引入 “长距离语义稀释”？如何设计高效的长期依赖建模机制？\n",
      "\n",
      "Extracted Academic Content:\n",
      "\n",
      "==================================================\n",
      "你是一个专业的学术助手，专注于cs.CL及其延伸领域。你的任务是从用户的问题中提取关键学术内容，并以关键词的形式表述。注意：你不需要直接回答问题，也不需要扩写问题，而是提取问题中的核心学术概念或主题，并以英文输出（严格输出且仅输出回复）。确保输出适合用于相似性向量查询。\n",
      "\n",
      "Query: 句法语义一体化建模中，成分结构与依存结构能否统一表示？\n",
      "\n",
      "Extracted Academic Content:\n",
      "\n",
      "==================================================\n",
      "你是一个专业的学术助手，专注于cs.CL及其延伸领域。你的任务是从用户的问题中提取关键学术内容，并以关键词的形式表述。注意：你不需要直接回答问题，也不需要扩写问题，而是提取问题中的核心学术概念或主题，并以英文输出（严格输出且仅输出回复）。确保输出适合用于相似性向量查询。\n",
      "\n",
      "Query: Transformer模型的自注意力机制是否存在 “信息泄露” 风险？如何设计防护措施？\n",
      "\n",
      "Extracted Academic Content:\n",
      "\n",
      "==================================================\n",
      "你是一个专业的学术助手，专注于cs.CL及其延伸领域。你的任务是从用户的问题中提取关键学术内容，并以关键词的形式表述。注意：你不需要直接回答问题，也不需要扩写问题，而是提取问题中的核心学术概念或主题，并以英文输出（严格输出且仅输出回复）。确保输出适合用于相似性向量查询。\n",
      "\n",
      "Query: Transformer上的文本分类和基于CNN的图片分类遇到的问题及解决方案的异同？\n",
      "\n",
      "Extracted Academic Content:\n",
      "\n",
      "==================================================\n",
      "你是一个专业的学术助手，专注于cs.CL及其延伸领域。你的任务是从用户的问题中提取关键学术内容，并以关键词的形式表述。注意：你不需要直接回答问题，也不需要扩写问题，而是提取问题中的核心学术概念或主题，并以英文输出（严格输出且仅输出回复）。确保输出适合用于相似性向量查询。\n",
      "\n",
      "Query: 什么是对抗攻击，常用黑盒文本对抗攻击手段有哪些？\n",
      "\n",
      "Extracted Academic Content:\n",
      "\n",
      "==================================================\n",
      "你是一个专业的学术助手，专注于cs.CL及其延伸领域。你的任务是从用户的问题中提取关键学术内容，并以关键词的形式表述。注意：你不需要直接回答问题，也不需要扩写问题，而是提取问题中的核心学术概念或主题，并以英文输出（严格输出且仅输出回复）。确保输出适合用于相似性向量查询。\n",
      "\n",
      "Query: 什么是对抗攻击，常用白盒文本对抗攻击手段有哪些？\n",
      "\n",
      "Extracted Academic Content:\n",
      "\n",
      "==================================================\n",
      "你是一个专业的学术助手，专注于cs.CL及其延伸领域。你的任务是从用户的问题中提取关键学术内容，并以关键词的形式表述。注意：你不需要直接回答问题，也不需要扩写问题，而是提取问题中的核心学术概念或主题，并以英文输出（严格输出且仅输出回复）。确保输出适合用于相似性向量查询。\n",
      "\n",
      "Query: FGSM对抗攻击方法是否能以及如何迁移到文本对抗攻击领域？\n",
      "\n",
      "Extracted Academic Content:\n",
      "\n",
      "==================================================\n",
      "你是一个专业的学术助手，专注于cs.CL及其延伸领域。你的任务是从用户的问题中提取关键学术内容，并以关键词的形式表述。注意：你不需要直接回答问题，也不需要扩写问题，而是提取问题中的核心学术概念或主题，并以英文输出（严格输出且仅输出回复）。确保输出适合用于相似性向量查询。\n",
      "\n",
      "Query: 解释一下如何使用BERT生成黑盒对抗攻击样本\n",
      "\n",
      "Extracted Academic Content:\n",
      "\n",
      "==================================================\n",
      "你是一个专业的学术助手，专注于cs.CL及其延伸领域。你的任务是从用户的问题中提取关键学术内容，并以关键词的形式表述。注意：你不需要直接回答问题，也不需要扩写问题，而是提取问题中的核心学术概念或主题，并以英文输出（严格输出且仅输出回复）。确保输出适合用于相似性向量查询。\n",
      "\n",
      "Query: 低资源语言的形态丰富性（如黏着语）如何影响少样本词义消歧？基于类型学的迁移学习是否有效？\n",
      "\n",
      "Extracted Academic Content:\n",
      "\n",
      "==================================================\n",
      "你是一个专业的学术助手，专注于cs.CL及其延伸领域。你的任务是从用户的问题中提取关键学术内容，并以关键词的形式表述。注意：你不需要直接回答问题，也不需要扩写问题，而是提取问题中的核心学术概念或主题，并以英文输出（严格输出且仅输出回复）。确保输出适合用于相似性向量查询。\n",
      "\n",
      "Query: BGE模型如何将文本向量化\n",
      "\n",
      "Extracted Academic Content:\n",
      "\n",
      "==================================================\n",
      "你是一个专业的学术助手，专注于cs.CL及其延伸领域。你的任务是从用户的问题中提取关键学术内容，并以关键词的形式表述。注意：你不需要直接回答问题，也不需要扩写问题，而是提取问题中的核心学术概念或主题，并以英文输出（严格输出且仅输出回复）。确保输出适合用于相似性向量查询。\n",
      "\n",
      "Query: 在机器翻译中，如何处理专有名词和术语的翻译问题？\n",
      "\n",
      "Extracted Academic Content:\n",
      "\n",
      "==================================================\n",
      "你是一个专业的学术助手，专注于cs.CL及其延伸领域。你的任务是从用户的问题中提取关键学术内容，并以关键词的形式表述。注意：你不需要直接回答问题，也不需要扩写问题，而是提取问题中的核心学术概念或主题，并以英文输出（严格输出且仅输出回复）。确保输出适合用于相似性向量查询。\n",
      "\n",
      "Query: 在机器翻译中，如何提高对长句和复杂句的翻译质量？\n",
      "\n",
      "Extracted Academic Content:\n",
      "\n",
      "==================================================\n",
      "你是一个专业的学术助手，专注于cs.CL及其延伸领域。你的任务是从用户的问题中提取关键学术内容，并以关键词的形式表述。注意：你不需要直接回答问题，也不需要扩写问题，而是提取问题中的核心学术概念或主题，并以英文输出（严格输出且仅输出回复）。确保输出适合用于相似性向量查询。\n",
      "\n",
      "Query: 如何构建一个高效的文本语义相似度计算模型？\n",
      "\n",
      "Extracted Academic Content:\n",
      "\n",
      "==================================================\n",
      "你是一个专业的学术助手，专注于cs.CL及其延伸领域。你的任务是从用户的问题中提取关键学术内容，并以关键词的形式表述。注意：你不需要直接回答问题，也不需要扩写问题，而是提取问题中的核心学术概念或主题，并以英文输出（严格输出且仅输出回复）。确保输出适合用于相似性向量查询。\n",
      "\n",
      "Query: 在多语言对话系统中，如何实现语言的无缝切换和交流？\n",
      "\n",
      "Extracted Academic Content:\n",
      "\n",
      "==================================================\n",
      "你是一个专业的学术助手，专注于cs.CL及其延伸领域。你的任务是从用户的问题中提取关键学术内容，并以关键词的形式表述。注意：你不需要直接回答问题，也不需要扩写问题，而是提取问题中的核心学术概念或主题，并以英文输出（严格输出且仅输出回复）。确保输出适合用于相似性向量查询。\n",
      "\n",
      "Query: 在诗歌生成任务中，如何让机器生成符合诗歌格律和意境的作品？\n",
      "\n",
      "Extracted Academic Content:\n",
      "\n",
      "==================================================\n",
      "你是一个专业的学术助手，专注于cs.CL及其延伸领域。你的任务是从用户的问题中提取关键学术内容，并以关键词的形式表述。注意：你不需要直接回答问题，也不需要扩写问题，而是提取问题中的核心学术概念或主题，并以英文输出（严格输出且仅输出回复）。确保输出适合用于相似性向量查询。\n",
      "\n",
      "Query: 在文本纠错任务中，如何提高对拼写错误、语法错误的检测和纠正能力？\n",
      "\n",
      "Extracted Academic Content:\n",
      "\n",
      "==================================================\n",
      "你是一个专业的学术助手，专注于cs.CL及其延伸领域。你的任务是从用户的问题中提取关键学术内容，并以关键词的形式表述。注意：你不需要直接回答问题，也不需要扩写问题，而是提取问题中的核心学术概念或主题，并以英文输出（严格输出且仅输出回复）。确保输出适合用于相似性向量查询。\n",
      "\n",
      "Query: 在机器翻译中，如何更好地处理源语言与目标语言之间的语义差异和文化差异？\n",
      "\n",
      "Extracted Academic Content:\n",
      "\n",
      "==================================================\n",
      "你是一个专业的学术助手，专注于cs.CL及其延伸领域。你的任务是从用户的问题中提取关键学术内容，并以关键词的形式表述。注意：你不需要直接回答问题，也不需要扩写问题，而是提取问题中的核心学术概念或主题，并以英文输出（严格输出且仅输出回复）。确保输出适合用于相似性向量查询。\n",
      "\n",
      "Query: 在文本生成任务中，如何平衡生成文本的多样性、正确性和连贯性？\n",
      "\n",
      "Extracted Academic Content:\n",
      "\n",
      "==================================================\n",
      "你是一个专业的学术助手，专注于cs.CL及其延伸领域。你的任务是从用户的问题中提取关键学术内容，并以关键词的形式表述。注意：你不需要直接回答问题，也不需要扩写问题，而是提取问题中的核心学术概念或主题，并以英文输出（严格输出且仅输出回复）。确保输出适合用于相似性向量查询。\n",
      "\n",
      "Query: 在对话系统中，如何提高对用户意图的理解准确率并生成更加自然流畅的回复？\n",
      "\n",
      "Extracted Academic Content:\n",
      "\n",
      "==================================================\n",
      "你是一个专业的学术助手，专注于cs.CL及其延伸领域。你的任务是从用户的问题中提取关键学术内容，并以关键词的形式表述。注意：你不需要直接回答问题，也不需要扩写问题，而是提取问题中的核心学术概念或主题，并以英文输出（严格输出且仅输出回复）。确保输出适合用于相似性向量查询。\n",
      "\n",
      "Query: 如何构建一个鲁棒的文本分类模型，以应对文本数据的噪声和不平衡问题？\n",
      "\n",
      "Extracted Academic Content:\n",
      "\n",
      "==================================================\n",
      "你是一个专业的学术助手，专注于cs.CL及其延伸领域。你的任务是从用户的问题中提取关键学术内容，并以关键词的形式表述。注意：你不需要直接回答问题，也不需要扩写问题，而是提取问题中的核心学术概念或主题，并以英文输出（严格输出且仅输出回复）。确保输出适合用于相似性向量查询。\n",
      "\n",
      "Query: 如何设计一种能够准确识别文本中隐喻表达的模型，并将其应用于情感分析或文本理解？\n",
      "\n",
      "Extracted Academic Content:\n",
      "\n",
      "==================================================\n",
      "你是一个专业的学术助手，专注于cs.CL及其延伸领域。你的任务是从用户的问题中提取关键学术内容，并以关键词的形式表述。注意：你不需要直接回答问题，也不需要扩写问题，而是提取问题中的核心学术概念或主题，并以英文输出（严格输出且仅输出回复）。确保输出适合用于相似性向量查询。\n",
      "\n",
      "Query: 针对低资源语言，如何利用迁移学习来提高自然语言处理任务的性能？\n",
      "\n",
      "Extracted Academic Content:\n",
      "\n",
      "==================================================\n",
      "你是一个专业的学术助手，专注于cs.CL及其延伸领域。你的任务是从用户的问题中提取关键学术内容，并以关键词的形式表述。注意：你不需要直接回答问题，也不需要扩写问题，而是提取问题中的核心学术概念或主题，并以英文输出（严格输出且仅输出回复）。确保输出适合用于相似性向量查询。\n",
      "\n",
      "Query: 在自然语言处理中，如何处理文本数据中的歧义问题？\n",
      "\n",
      "Extracted Academic Content:\n",
      "\n",
      "==================================================\n",
      "你是一个专业的学术助手，专注于cs.CL及其延伸领域。你的任务是从用户的问题中提取关键学术内容，并以关键词的形式表述。注意：你不需要直接回答问题，也不需要扩写问题，而是提取问题中的核心学术概念或主题，并以英文输出（严格输出且仅输出回复）。确保输出适合用于相似性向量查询。\n",
      "\n",
      "Query: 如何提高对古文字的识别和释读能力？\n",
      "\n",
      "Extracted Academic Content:\n",
      "\n",
      "==================================================\n",
      "你是一个专业的学术助手，专注于cs.CL及其延伸领域。你的任务是从用户的问题中提取关键学术内容，并以关键词的形式表述。注意：你不需要直接回答问题，也不需要扩写问题，而是提取问题中的核心学术概念或主题，并以英文输出（严格输出且仅输出回复）。确保输出适合用于相似性向量查询。\n",
      "\n",
      "Query: 在文本风格迁移任务中，如何准确地将文本从一种风格转换为另一种风格？\n",
      "\n",
      "Extracted Academic Content:\n",
      "\n",
      "==================================================\n",
      "你是一个专业的学术助手，专注于cs.CL及其延伸领域。你的任务是从用户的问题中提取关键学术内容，并以关键词的形式表述。注意：你不需要直接回答问题，也不需要扩写问题，而是提取问题中的核心学术概念或主题，并以英文输出（严格输出且仅输出回复）。确保输出适合用于相似性向量查询。\n",
      "\n",
      "Query: 如何利用计算语言学技术对文学作品进行风格分析？\n",
      "\n",
      "Extracted Academic Content:\n",
      "\n",
      "==================================================\n",
      "你是一个专业的学术助手，专注于cs.CL及其延伸领域。你的任务是从用户的问题中提取关键学术内容，并以关键词的形式表述。注意：你不需要直接回答问题，也不需要扩写问题，而是提取问题中的核心学术概念或主题，并以英文输出（严格输出且仅输出回复）。确保输出适合用于相似性向量查询。\n",
      "\n",
      "Query: 如何设计一种能够准确识别文本中事件的模型？\n",
      "\n",
      "Extracted Academic Content:\n",
      "\n",
      "==================================================\n",
      "你是一个专业的学术助手，专注于cs.CL及其延伸领域。你的任务是从用户的问题中提取关键学术内容，并以关键词的形式表述。注意：你不需要直接回答问题，也不需要扩写问题，而是提取问题中的核心学术概念或主题，并以英文输出（严格输出且仅输出回复）。确保输出适合用于相似性向量查询。\n",
      "\n",
      "Query: 如何通过计算语言学方法挖掘文学作品中的叙事结构和情节发展？\n",
      "\n",
      "Extracted Academic Content:\n",
      "\n",
      "==================================================\n",
      "你是一个专业的学术助手，专注于cs.CL及其延伸领域。你的任务是从用户的问题中提取关键学术内容，并以关键词的形式表述。注意：你不需要直接回答问题，也不需要扩写问题，而是提取问题中的核心学术概念或主题，并以英文输出（严格输出且仅输出回复）。确保输出适合用于相似性向量查询。\n",
      "\n",
      "Query: 如何构建一个高效的文本语义相似度计算模型，以支持信息检索和文本匹配？\n",
      "\n",
      "Extracted Academic Content:\n",
      "\n",
      "==================================================\n",
      "你是一个专业的学术助手，专注于cs.CL及其延伸领域。你的任务是从用户的问题中提取关键学术内容，并以关键词的形式表述。注意：你不需要直接回答问题，也不需要扩写问题，而是提取问题中的核心学术概念或主题，并以英文输出（严格输出且仅输出回复）。确保输出适合用于相似性向量查询。\n",
      "\n",
      "Query: 深度学习中的 “灾难性遗忘” 在 NLP 任务中表现为何种形式？\n",
      "\n",
      "Extracted Academic Content:\n",
      "\n",
      "==================================================\n",
      "你是一个专业的学术助手，专注于cs.CL及其延伸领域。你的任务是从用户的问题中提取关键学术内容，并以关键词的形式表述。注意：你不需要直接回答问题，也不需要扩写问题，而是提取问题中的核心学术概念或主题，并以英文输出（严格输出且仅输出回复）。确保输出适合用于相似性向量查询。\n",
      "\n",
      "Query: 阅读障碍者的语言处理缺陷能否通过深度学习模型复现？\n",
      "\n",
      "Extracted Academic Content:\n",
      "\n",
      "==================================================\n",
      "你是一个专业的学术助手，专注于cs.CL及其延伸领域。你的任务是从用户的问题中提取关键学术内容，并以关键词的形式表述。注意：你不需要直接回答问题，也不需要扩写问题，而是提取问题中的核心学术概念或主题，并以英文输出（严格输出且仅输出回复）。确保输出适合用于相似性向量查询。\n",
      "\n",
      "Query: 如何进行代码生成？\n",
      "\n",
      "Extracted Academic Content:\n",
      "\n",
      "==================================================\n",
      "你是一个专业的学术助手，专注于cs.CL及其延伸领域。你的任务是从用户的问题中提取关键学术内容，并以关键词的形式表述。注意：你不需要直接回答问题，也不需要扩写问题，而是提取问题中的核心学术概念或主题，并以英文输出（严格输出且仅输出回复）。确保输出适合用于相似性向量查询。\n",
      "\n",
      "Query: 语系差异显著的语言（如汉藏语系 vs. 印欧语系）在句法表征上的本质区别如何影响跨语言迁移？\n",
      "\n",
      "Extracted Academic Content:\n",
      "\n",
      "==================================================\n",
      "你是一个专业的学术助手，专注于cs.CL及其延伸领域。你的任务是从用户的问题中提取关键学术内容，并以关键词的形式表述。注意：你不需要直接回答问题，也不需要扩写问题，而是提取问题中的核心学术概念或主题，并以英文输出（严格输出且仅输出回复）。确保输出适合用于相似性向量查询。\n",
      "\n",
      "Query: 在多语言文本分类任务中，如何利用多任务学习来提高分类性能？\n",
      "\n",
      "Extracted Academic Content:\n",
      "\n",
      "==================================================\n",
      "你是一个专业的学术助手，专注于cs.CL及其延伸领域。你的任务是从用户的问题中提取关键学术内容，并以关键词的形式表述。注意：你不需要直接回答问题，也不需要扩写问题，而是提取问题中的核心学术概念或主题，并以英文输出（严格输出且仅输出回复）。确保输出适合用于相似性向量查询。\n",
      "\n",
      "Query: 如何区分大语言模型生成文本与人类文本？\n",
      "\n",
      "Extracted Academic Content:\n",
      "\n",
      "==================================================\n",
      "你是一个专业的学术助手，专注于cs.CL及其延伸领域。你的任务是从用户的问题中提取关键学术内容，并以关键词的形式表述。注意：你不需要直接回答问题，也不需要扩写问题，而是提取问题中的核心学术概念或主题，并以英文输出（严格输出且仅输出回复）。确保输出适合用于相似性向量查询。\n",
      "\n",
      "Query: 对于由多个大模型生成文本与人类文本混合的数据集（数据集仅含两种标签），如何区分出大模型生成的文本？\n",
      "\n",
      "Extracted Academic Content:\n",
      "\n",
      "==================================================\n",
      "你是一个专业的学术助手，专注于cs.CL及其延伸领域。你的任务是从用户的问题中提取关键学术内容，并以关键词的形式表述。注意：你不需要直接回答问题，也不需要扩写问题，而是提取问题中的核心学术概念或主题，并以英文输出（严格输出且仅输出回复）。确保输出适合用于相似性向量查询。\n",
      "\n",
      "Query: 不同大模型生成文本的检测方法是否存在差异？如何针对性地进行检测？\n",
      "\n",
      "Extracted Academic Content:\n",
      "\n",
      "==================================================\n",
      "你是一个专业的学术助手，专注于cs.CL及其延伸领域。你的任务是从用户的问题中提取关键学术内容，并以关键词的形式表述。注意：你不需要直接回答问题，也不需要扩写问题，而是提取问题中的核心学术概念或主题，并以英文输出（严格输出且仅输出回复）。确保输出适合用于相似性向量查询。\n",
      "\n",
      "Query: 在大语言模型生成文本检测任务中，如何设计有效的特征提取方法，以区分大模型生成文本与人工编写文本的细微差别？\n",
      "\n",
      "Extracted Academic Content:\n",
      "\n",
      "==================================================\n",
      "你是一个专业的学术助手，专注于cs.CL及其延伸领域。你的任务是从用户的问题中提取关键学术内容，并以关键词的形式表述。注意：你不需要直接回答问题，也不需要扩写问题，而是提取问题中的核心学术概念或主题，并以英文输出（严格输出且仅输出回复）。确保输出适合用于相似性向量查询。\n",
      "\n",
      "Query: 是否可以利用对抗训练等技术提高大模型生成文本的可检测性，如果可以应该如何实现？\n",
      "\n",
      "Extracted Academic Content:\n",
      "\n",
      "==================================================\n",
      "你是一个专业的学术助手，专注于cs.CL及其延伸领域。你的任务是从用户的问题中提取关键学术内容，并以关键词的形式表述。注意：你不需要直接回答问题，也不需要扩写问题，而是提取问题中的核心学术概念或主题，并以英文输出（严格输出且仅输出回复）。确保输出适合用于相似性向量查询。\n",
      "\n",
      "Query: 如何构建大规模、高质量的大模型生成文本检测数据集？\n",
      "\n",
      "Extracted Academic Content:\n",
      "\n",
      "==================================================\n",
      "你是一个专业的学术助手，专注于cs.CL及其延伸领域。你的任务是从用户的问题中提取关键学术内容，并以关键词的形式表述。注意：你不需要直接回答问题，也不需要扩写问题，而是提取问题中的核心学术概念或主题，并以英文输出（严格输出且仅输出回复）。确保输出适合用于相似性向量查询。\n",
      "\n",
      "Query: 如何应对大模型生成文本不断优化带来的检测挑战？\n",
      "\n",
      "Extracted Academic Content:\n",
      "\n",
      "==================================================\n",
      "你是一个专业的学术助手，专注于cs.CL及其延伸领域。你的任务是从用户的问题中提取关键学术内容，并以关键词的形式表述。注意：你不需要直接回答问题，也不需要扩写问题，而是提取问题中的核心学术概念或主题，并以英文输出（严格输出且仅输出回复）。确保输出适合用于相似性向量查询。\n",
      "\n",
      "Query: 如何构建一个能够自适应不同领域和风格的文本生成模型？\n",
      "\n",
      "Extracted Academic Content:\n",
      "\n",
      "==================================================\n",
      "你是一个专业的学术助手，专注于cs.CL及其延伸领域。你的任务是从用户的问题中提取关键学术内容，并以关键词的形式表述。注意：你不需要直接回答问题，也不需要扩写问题，而是提取问题中的核心学术概念或主题，并以英文输出（严格输出且仅输出回复）。确保输出适合用于相似性向量查询。\n",
      "\n",
      "Query: 在文本分类任务中，如何处理文本数据中的噪声和异常值，以提高模型的性能与鲁棒性？\n",
      "\n",
      "Extracted Academic Content:\n",
      "\n",
      "==================================================\n",
      "你是一个专业的学术助手，专注于cs.CL及其延伸领域。你的任务是从用户的问题中提取关键学术内容，并以关键词的形式表述。注意：你不需要直接回答问题，也不需要扩写问题，而是提取问题中的核心学术概念或主题，并以英文输出（严格输出且仅输出回复）。确保输出适合用于相似性向量查询。\n",
      "\n",
      "Query: 什么是知识图谱，如何对文本数据构建知识图谱？\n",
      "\n",
      "Extracted Academic Content:\n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "for i in range(50):\n",
    "    identity, template = search.queryAutoGen(i)\n",
    "    print(identity)\n",
    "    print(template)\n",
    "    print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_datas = []\n",
    "\n",
    "with open(\"extracted_querys.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    for line in file:\n",
    "        stripped_line = line.strip()\n",
    "        if stripped_line:\n",
    "            extracted_datas.append(stripped_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|原始问题|改写后|\n",
      "|---|---|\n",
      "| 什么是注意力机制？ | attention mechanism |\n",
      "| 对比Transformer与RNN在机器翻译中的性能 | Transformer, RNN, machine translation, performance comparison |\n",
      "| 大语言模型的 “幻觉” 问题本质是语义理解缺陷还是推理过程偏差？如何构建可靠性评估框架？ | large language models, hallucination problem, semantic understanding, reasoning bias, reliability evaluation framework |\n",
      "| 零样本学习中，提示词工程能否真正激活模型的 “潜在知识”？其理论边界在哪？ | zero-shot learning, prompt engineering, latent knowledge activation, theoretical boundaries |\n",
      "| 小样本场景下，元学习与预训练模型的适应性调整策略如何结合？ | few-shot learning, meta-learning, pre-trained models, adaptation strategies |\n",
      "| 预训练模型的 “涌现能力” 是否具有可预测性？如何从理论上解释其突现机制？ | pre-trained models, emergent capabilities, predictability, theoretical explanation |\n",
      "| 大模型的上下文窗口扩展是否会引入 “长距离语义稀释”？如何设计高效的长期依赖建模机制？ | large models, context window expansion, long-distance semantic dilution, long-term dependency modeling |\n",
      "| 句法语义一体化建模中，成分结构与依存结构能否统一表示？ | syntactic-semantic integrated modeling, constituent structure, dependency structure, unified representation |\n",
      "| Transformer模型的自注意力机制是否存在 “信息泄露” 风险？如何设计防护措施？ | Transformer model, self-attention mechanism, information leakage, protective measures |\n",
      "| Transformer上的文本分类和基于CNN的图片分类遇到的问题及解决方案的异同？ | Transformer text classification, CNN image classification, problems, solutions, similarities and differences |\n",
      "| 什么是对抗攻击，常用黑盒文本对抗攻击手段有哪些？ | adversarial attacks, black-box text adversarial methods |\n",
      "| 什么是对抗攻击，常用白盒文本对抗攻击手段有哪些？ | adversarial attacks, white-box text adversarial methods |\n",
      "| FGSM对抗攻击方法是否能以及如何迁移到文本对抗攻击领域？ | FGSM adversarial attack, text adversarial attacks, migration |\n",
      "| 解释一下如何使用BERT生成黑盒对抗攻击样本 | BERT, black-box adversarial attack, sample generation |\n",
      "| 低资源语言的形态丰富性（如黏着语）如何影响少样本词义消歧？基于类型学的迁移学习是否有效？ | low-resource languages, morphological richness, agglutinative languages, few-shot word sense disambiguation, typology-based transfer learning |\n",
      "| BGE模型如何将文本向量化 | BGE model, text vectorization |\n",
      "| 在机器翻译中，如何处理专有名词和术语的翻译问题？ | machine translation, proper nouns, terminology translation |\n",
      "| 在机器翻译中，如何提高对长句和复杂句的翻译质量？ | machine translation, long sentences, complex sentences, translation quality |\n",
      "| 如何构建一个高效的文本语义相似度计算模型？ | text semantic similarity, efficient model construction |\n",
      "| 在多语言对话系统中，如何实现语言的无缝切换和交流？ | multilingual dialogue systems, seamless language switching, cross-lingual communication |\n",
      "| 在诗歌生成任务中，如何让机器生成符合诗歌格律和意境的作品？ | poetry generation, prosody, artistic conception |\n",
      "| 在文本纠错任务中，如何提高对拼写错误、语法错误的检测和纠正能力？ | text correction, spelling errors, grammatical errors, detection and correction |\n",
      "| 在机器翻译中，如何更好地处理源语言与目标语言之间的语义差异和文化差异？ | machine translation, semantic differences, cultural differences |\n",
      "| 在文本生成任务中，如何平衡生成文本的多样性、正确性和连贯性？ | text generation, diversity, correctness, coherence |\n",
      "| 在对话系统中，如何提高对用户意图的理解准确率并生成更加自然流畅的回复？ | dialogue systems, user intent understanding, response generation, naturalness, fluency |\n",
      "| 如何构建一个鲁棒的文本分类模型，以应对文本数据的噪声和不平衡问题？ | robust text classification, noise, data imbalance |\n",
      "| 如何设计一种能够准确识别文本中隐喻表达的模型，并将其应用于情感分析或文本理解？ | metaphor detection, model design, sentiment analysis, text understanding |\n",
      "| 针对低资源语言，如何利用迁移学习来提高自然语言处理任务的性能？ | low-resource languages, transfer learning, natural language processing, performance improvement |\n",
      "| 在自然语言处理中，如何处理文本数据中的歧义问题？ | natural language processing, ambiguity resolution |\n",
      "| 如何提高对古文字的识别和释读能力？ | ancient scripts, recognition, interpretation |\n",
      "| 在文本风格迁移任务中，如何准确地将文本从一种风格转换为另一种风格？ | text style transfer, style conversion |\n",
      "| 如何利用计算语言学技术对文学作品进行风格分析？ | computational linguistics, literary analysis, style analysis |\n",
      "| 如何设计一种能够准确识别文本中事件的模型？ | event recognition, model design |\n",
      "| 如何通过计算语言学方法挖掘文学作品中的叙事结构和情节发展？ | computational linguistics, narrative structure, plot development |\n",
      "| 如何构建一个高效的文本语义相似度计算模型，以支持信息检索和文本匹配？ | text semantic similarity, efficient model, information retrieval, text matching |\n",
      "| 深度学习中的 “灾难性遗忘” 在 NLP 任务中表现为何种形式？ | catastrophic forgetting, deep learning, NLP |\n",
      "| 阅读障碍者的语言处理缺陷能否通过深度学习模型复现？ | dyslexia, language processing deficits, deep learning models |\n",
      "| 如何进行代码生成？ | code generation |\n",
      "| 语系差异显著的语言（如汉藏语系 vs. 印欧语系）在句法表征上的本质区别如何影响跨语言迁移？ | language families, syntax representation, cross-lingual transfer |\n",
      "| 在多语言文本分类任务中，如何利用多任务学习来提高分类性能？ | multilingual text classification, multi-task learning |\n",
      "| 如何区分大语言模型生成文本与人类文本？ | large language models, text generation, human text distinction |\n",
      "| 对于由多个大模型生成文本与人类文本混合的数据集（数据集仅含两种标签），如何区分出大模型生成的文本？ | large language models, text generation, human text, multi-datasets, text distinction |\n",
      "| 不同大模型生成文本的检测方法是否存在差异？如何针对性地进行检测？ | large language models, text generation detection, method differences |\n",
      "| 在大语言模型生成文本检测任务中，如何设计有效的特征提取方法，以区分大模型生成文本与人工编写文本的细微差别？ | large language models, text generation detection, feature extraction, subtle differences |\n",
      "| 是否可以利用对抗训练等技术提高大模型生成文本的可检测性，如果可以应该如何实现？ | adversarial training, large language models, text detectability |\n",
      "| 如何构建大规模、高质量的大模型生成文本检测数据集？ | large-scale dataset, high-quality data, text generation detection |\n",
      "| 如何应对大模型生成文本不断优化带来的检测挑战？ | large language models, text generation optimization, detection challenges |\n",
      "| 如何构建一个能够自适应不同领域和风格的文本生成模型？ | adaptive text generation, domain adaptation, style adaptation |\n",
      "| 在文本分类任务中，如何处理文本数据中的噪声和异常值，以提高模型的性能与鲁棒性？ | text classification, noise handling, outlier detection, model performance, robustness |\n",
      "| 什么是知识图谱，如何对文本数据构建知识图谱？ | knowledge graph, text data, construction |\n"
     ]
    }
   ],
   "source": [
    "print(\"|原始问题|改写后|\")\n",
    "print(\"|---|---|\")\n",
    "for extrac, orig in zip(extracted_datas, search.prompts.questions):\n",
    "    print(f\"| {orig} | {extrac} |\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.faiss_search import FaissSearch\n",
    "\n",
    "fs = FaissSearch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "生成嵌入中: 100%|██████████| 1/1 [00:00<00:00, 37.88it/s]\n",
      "生成嵌入中: 100%|██████████| 1/1 [00:00<00:00, 31.34it/s]\n",
      "生成嵌入中: 100%|██████████| 1/1 [00:00<00:00, 21.38it/s]\n",
      "生成嵌入中: 100%|██████████| 1/1 [00:00<00:00, 23.89it/s]\n",
      "生成嵌入中: 100%|██████████| 1/1 [00:00<00:00, 32.10it/s]\n",
      "生成嵌入中: 100%|██████████| 1/1 [00:00<00:00, 34.45it/s]\n",
      "生成嵌入中: 100%|██████████| 1/1 [00:00<00:00, 29.24it/s]\n",
      "生成嵌入中: 100%|██████████| 1/1 [00:00<00:00, 29.06it/s]\n",
      "生成嵌入中: 100%|██████████| 1/1 [00:00<00:00, 28.10it/s]\n",
      "生成嵌入中: 100%|██████████| 1/1 [00:00<00:00, 27.73it/s]\n",
      "生成嵌入中: 100%|██████████| 1/1 [00:00<00:00, 43.47it/s]\n",
      "生成嵌入中: 100%|██████████| 1/1 [00:00<00:00, 27.67it/s]\n",
      "生成嵌入中: 100%|██████████| 1/1 [00:00<00:00, 30.46it/s]\n",
      "生成嵌入中: 100%|██████████| 1/1 [00:00<00:00, 31.36it/s]\n",
      "生成嵌入中: 100%|██████████| 1/1 [00:00<00:00, 19.52it/s]\n",
      "生成嵌入中: 100%|██████████| 1/1 [00:00<00:00, 50.81it/s]\n",
      "生成嵌入中: 100%|██████████| 1/1 [00:00<00:00, 30.32it/s]\n",
      "生成嵌入中: 100%|██████████| 1/1 [00:00<00:00, 37.63it/s]\n",
      "生成嵌入中: 100%|██████████| 1/1 [00:00<00:00, 32.42it/s]\n",
      "生成嵌入中: 100%|██████████| 1/1 [00:00<00:00, 34.37it/s]\n",
      "生成嵌入中: 100%|██████████| 1/1 [00:00<00:00, 27.53it/s]\n",
      "生成嵌入中: 100%|██████████| 1/1 [00:00<00:00, 27.66it/s]\n",
      "生成嵌入中: 100%|██████████| 1/1 [00:00<00:00, 24.11it/s]\n",
      "生成嵌入中: 100%|██████████| 1/1 [00:00<00:00, 30.44it/s]\n",
      "生成嵌入中: 100%|██████████| 1/1 [00:00<00:00, 20.93it/s]\n",
      "生成嵌入中: 100%|██████████| 1/1 [00:00<00:00, 31.80it/s]\n",
      "生成嵌入中: 100%|██████████| 1/1 [00:00<00:00, 29.91it/s]\n",
      "生成嵌入中: 100%|██████████| 1/1 [00:00<00:00, 26.24it/s]\n",
      "生成嵌入中: 100%|██████████| 1/1 [00:00<00:00, 31.48it/s]\n",
      "生成嵌入中: 100%|██████████| 1/1 [00:00<00:00, 45.22it/s]\n",
      "生成嵌入中: 100%|██████████| 1/1 [00:00<00:00, 18.15it/s]\n",
      "生成嵌入中: 100%|██████████| 1/1 [00:00<00:00, 31.54it/s]\n",
      "生成嵌入中: 100%|██████████| 1/1 [00:00<00:00, 24.87it/s]\n",
      "生成嵌入中: 100%|██████████| 1/1 [00:00<00:00, 28.84it/s]\n",
      "生成嵌入中: 100%|██████████| 1/1 [00:00<00:00, 43.67it/s]\n",
      "生成嵌入中: 100%|██████████| 1/1 [00:00<00:00, 31.17it/s]\n",
      "生成嵌入中: 100%|██████████| 1/1 [00:00<00:00, 20.24it/s]\n",
      "生成嵌入中: 100%|██████████| 1/1 [00:00<00:00, 26.83it/s]\n",
      "生成嵌入中: 100%|██████████| 1/1 [00:00<00:00, 39.69it/s]\n",
      "生成嵌入中: 100%|██████████| 1/1 [00:00<00:00, 28.08it/s]\n",
      "生成嵌入中: 100%|██████████| 1/1 [00:00<00:00, 29.06it/s]\n",
      "生成嵌入中: 100%|██████████| 1/1 [00:00<00:00, 22.16it/s]\n",
      "生成嵌入中: 100%|██████████| 1/1 [00:00<00:00, 29.11it/s]\n",
      "生成嵌入中: 100%|██████████| 1/1 [00:00<00:00, 28.39it/s]\n",
      "生成嵌入中: 100%|██████████| 1/1 [00:00<00:00, 30.76it/s]\n",
      "生成嵌入中: 100%|██████████| 1/1 [00:00<00:00, 25.08it/s]\n",
      "生成嵌入中: 100%|██████████| 1/1 [00:00<00:00, 27.59it/s]\n",
      "生成嵌入中: 100%|██████████| 1/1 [00:00<00:00, 30.31it/s]\n",
      "生成嵌入中: 100%|██████████| 1/1 [00:00<00:00, 34.33it/s]\n",
      "生成嵌入中: 100%|██████████| 1/1 [00:00<00:00, 30.43it/s]\n"
     ]
    }
   ],
   "source": [
    "all_results = []\n",
    "for extrated_data in extracted_datas:\n",
    "    fres = fs.search_top_k_ids([extrated_data], k=1000)\n",
    "    all_results.append(fres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum relevant results: 169\n",
      "Minimum relevant results: 0\n",
      "Average relevant results: 17.88\n"
     ]
    }
   ],
   "source": [
    "relevance = 0.4\n",
    "import numpy as np\n",
    "\n",
    "# 初始化统计结果列表\n",
    "relevant_counts = []\n",
    "\n",
    "# 遍历所有 extracted_datas\n",
    "for i, extracted_data in enumerate(extracted_datas):\n",
    "    fres = all_results[i]\n",
    "    relevant_count = 0\n",
    "\n",
    "    # 遍历搜索结果，统计距离在阈值内的数量\n",
    "    for result in fres:\n",
    "        if result[\"distance\"] <= relevance:\n",
    "            relevant_count += 1\n",
    "\n",
    "    # 将统计结果添加到列表中\n",
    "    relevant_counts.append(relevant_count)\n",
    "\n",
    "# 输出结果\n",
    "print(f\"Maximum relevant results: {np.max(relevant_counts)}\")\n",
    "print(f\"Minimum relevant results: {np.min(relevant_counts)}\")\n",
    "print(f\"Average relevant results: {np.mean(relevant_counts):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "生成嵌入中: 100%|██████████| 1/1 [00:00<00:00,  7.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "cost 0.1972 seconds\n",
      "\n",
      "\n",
      "You must reply in English. You are a professional academic assistant specializing in the cs.CL domain. Strictly based on the provided literature information, answer the user's query. You may slightly expand and refine the response using existing knowledge, but avoid fabricating information. The response language must be English. Ensure the response is concise and limited to 500 words. Include a citation with the corresponding literature link (like [text](f\"https://arxiv.org/abs/{{id}}\")) and adjust the response style based on the temperature parameter (from 0.0 to 1.0). If the temperature is low (e.g., 0.2), provide a strict and precise answer. If the temperature is high (e.g., 0.8), provide a more creative and exploratory answer.\n",
      "\n",
      "Temperature: 0.2\n",
      "Question: 什么是注意力机制？\n",
      "Relevant Literature: \n",
      "ID: 1810.10126, Distance: 0.3811461925506592, Content: {'title': 'Area Attention', 'doi': None, 'categories': 'cs.LG cs.AI cs.CL stat.ML', 'abstract': '  Existing attention mechanisms are trained to attend to individual items in a\\ncollection (the memory) with a predefined, fixed granularity, e.g., a word\\ntoken or an image grid. We propose area attention: a way to attend to areas in\\nthe memory, where each area contains a group of items that are structurally\\nadjacent, e.g., spatially for a 2D memory such as images, or temporally for a\\n1D memory such as natural language sentences. Importantly, the shape and the\\nsize of an area are dynamically determined via learning, which enables a model\\nto attend to information with varying granularity. Area attention can easily\\nwork with existing model architectures such as multi-head attention for\\nsimultaneously attending to multiple areas in the memory. We evaluate area\\nattention on two tasks: neural machine translation (both character and\\ntoken-level) and image captioning, and improve upon strong (state-of-the-art)\\nbaselines in all the cases. These improvements are obtainable with a basic form\\nof area attention that is parameter free.\\n', 'publish_date': 'Tue, 23 Oct 2018 23:14:27 GMT'}\n",
      "ID: 2211.03495, Distance: 0.40938127040863037, Content: {'title': 'How Much Does Attention Actually Attend? Questioning the Importance of\\n  Attention in Pretrained Transformers', 'doi': None, 'categories': 'cs.CL cs.LG', 'abstract': '  The attention mechanism is considered the backbone of the widely-used\\nTransformer architecture. It contextualizes the input by computing\\ninput-specific attention matrices. We find that this mechanism, while powerful\\nand elegant, is not as important as typically thought for pretrained language\\nmodels. We introduce PAPA, a new probing method that replaces the\\ninput-dependent attention matrices with constant ones -- the average attention\\nweights over multiple inputs. We use PAPA to analyze several established\\npretrained Transformers on six downstream tasks. We find that without any\\ninput-dependent attention, all models achieve competitive performance -- an\\naverage relative drop of only 8% from the probing baseline. Further, little or\\nno performance drop is observed when replacing half of the input-dependent\\nattention matrices with constant (input-independent) ones. Interestingly, we\\nshow that better-performing models lose more from applying our method than\\nweaker models, suggesting that the utilization of the input-dependent attention\\nmechanism might be a factor in their success. Our results motivate research on\\nsimpler alternatives to input-dependent attention, as well as on methods for\\nbetter utilization of this mechanism in the Transformer architecture.\\n', 'publish_date': 'Mon, 7 Nov 2022 12:37:54 GMT'}\n",
      "ID: 1904.05873, Distance: 0.4193165898323059, Content: {'title': 'An Empirical Study of Spatial Attention Mechanisms in Deep Networks', 'doi': None, 'categories': 'cs.CV cs.CL cs.LG', 'abstract': '  Attention mechanisms have become a popular component in deep neural networks,\\nyet there has been little examination of how different influencing factors and\\nmethods for computing attention from these factors affect performance. Toward a\\nbetter general understanding of attention mechanisms, we present an empirical\\nstudy that ablates various spatial attention elements within a generalized\\nattention formulation, encompassing the dominant Transformer attention as well\\nas the prevalent deformable convolution and dynamic convolution modules.\\nConducted on a variety of applications, the study yields significant findings\\nabout spatial attention in deep networks, some of which run counter to\\nconventional understanding. For example, we find that the query and key content\\ncomparison in Transformer attention is negligible for self-attention, but vital\\nfor encoder-decoder attention. A proper combination of deformable convolution\\nwith key content only saliency achieves the best accuracy-efficiency tradeoff\\nin self-attention. Our results suggest that there exists much room for\\nimprovement in the design of attention mechanisms.\\n', 'publish_date': 'Thu, 11 Apr 2019 17:58:37 GMT'}\n",
      "ID: 1905.09856, Distance: 0.4276387691497803, Content: {'title': 'Copy this Sentence', 'doi': None, 'categories': 'cs.LG cs.CL stat.ML', 'abstract': '  Attention is an operation that selects some largest element from some set,\\nwhere the notion of largest is defined elsewhere. Applying this operation to\\nsequence to sequence mapping results in significant improvements to the task at\\nhand. In this paper we provide the mathematical definition of attention and\\nexamine its application to sequence to sequence models. We highlight the exact\\ncorrespondences between machine learning implementations of attention and our\\nmathematical definition. We provide clear evidence of effectiveness of\\nattention mechanisms evaluating models with varying degrees of attention on a\\nvery simple task: copying a sentence. We find that models that make greater use\\nof attention perform much better on sequence to sequence mapping tasks,\\nconverge faster and are more stable.\\n', 'publish_date': 'Thu, 23 May 2019 18:25:35 GMT'}\n",
      "ID: 2211.07714, Distance: 0.43440112471580505, Content: {'title': 'Revisiting Attention Weights as Explanations from an Information\\n  Theoretic Perspective', 'doi': None, 'categories': 'cs.CL cs.AI cs.LG', 'abstract': '  Attention mechanisms have recently demonstrated impressive performance on a\\nrange of NLP tasks, and attention scores are often used as a proxy for model\\nexplainability. However, there is a debate on whether attention weights can, in\\nfact, be used to identify the most important inputs to a model. We approach\\nthis question from an information theoretic perspective by measuring the mutual\\ninformation between the model output and the hidden states. From extensive\\nexperiments, we draw the following conclusions: (i) Additive and Deep attention\\nmechanisms are likely to be better at preserving the information between the\\nhidden states and the model output (compared to Scaled Dot-product); (ii)\\nablation studies indicate that Additive attention can actively learn to explain\\nthe importance of its input hidden representations; (iii) when attention values\\nare nearly the same, the rank order of attention values is not consistent with\\nthe rank order of the mutual information(iv) Using Gumbel-Softmax with a\\ntemperature lower than one, tends to produce a more skewed attention score\\ndistribution compared to softmax and hence is a better choice for explainable\\ndesign; (v) some building blocks are better at preserving the correlation\\nbetween the ordered list of mutual information and attention weights order (for\\ne.g., the combination of BiLSTM encoder and Additive attention). Our findings\\nindicate that attention mechanisms do have the potential to function as a\\nshortcut to model explanations when they are carefully combined with other\\nmodel elements.\\n', 'publish_date': 'Mon, 31 Oct 2022 12:53:20 GMT'}\n",
      "\n",
      "Answer:\n",
      "\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "生成嵌入中: 100%|██████████| 1/1 [00:00<00:00, 12.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "cost 0.1602 seconds\n",
      "\n",
      "\n",
      "You must reply in English. You are a professional academic assistant specializing in the cs.CL domain. Strictly based on the provided literature information, answer the user's query. You may slightly expand and refine the response using existing knowledge, but avoid fabricating information. The response language must be English. Ensure the response is concise and limited to 500 words. Include a citation with the corresponding literature link (like [text](f\"https://arxiv.org/abs/{{id}}\")) and adjust the response style based on the temperature parameter (from 0.0 to 1.0). If the temperature is low (e.g., 0.2), provide a strict and precise answer. If the temperature is high (e.g., 0.8), provide a more creative and exploratory answer.\n",
      "\n",
      "Temperature: 0.2\n",
      "Question: 对比Transformer与RNN在机器翻译中的性能\n",
      "Relevant Literature: \n",
      "ID: 2502.00617, Distance: 0.33056533336639404, Content: {'title': 'Efficient Language Modeling for Low-Resource Settings with Hybrid\\n  RNN-Transformer Architectures', 'doi': None, 'categories': 'cs.CL', 'abstract': \"  Transformer-based language models have recently been at the forefront of\\nactive research in text generation. However, these models' advances come at the\\nprice of prohibitive training costs, with parameter counts in the billions and\\ncompute requirements measured in petaflop/s-decades. In this paper, we\\ninvestigate transformer-based architectures for improving model performance in\\na low-data regime by selectively replacing attention layers with feed-forward\\nand quasi-recurrent neural network layers. We test these architectures on the\\nstandard Enwik8 and Wikitext-103 corpora. Our results show that our reduced\\narchitectures outperform existing models with a comparable number of\\nparameters, and obtain comparable performance to larger models while\\nsignificantly reducing the number of parameters.\\n\", 'publish_date': 'Sun, 2 Feb 2025 01:05:09 GMT'}\n",
      "ID: 2403.01985, Distance: 0.3450363874435425, Content: {'title': \"Transformers for Low-Resource Languages: Is F\\\\'eidir Linn!\", 'doi': None, 'categories': 'cs.CL cs.AI', 'abstract': \"  The Transformer model is the state-of-the-art in Machine Translation.\\nHowever, in general, neural translation models often under perform on language\\npairs with insufficient training data. As a consequence, relatively few\\nexperiments have been carried out using this architecture on low-resource\\nlanguage pairs. In this study, hyperparameter optimization of Transformer\\nmodels in translating the low-resource English-Irish language pair is\\nevaluated. We demonstrate that choosing appropriate parameters leads to\\nconsiderable performance improvements. Most importantly, the correct choice of\\nsubword model is shown to be the biggest driver of translation performance.\\nSentencePiece models using both unigram and BPE approaches were appraised.\\nVariations on model architectures included modifying the number of layers,\\ntesting various regularisation techniques and evaluating the optimal number of\\nheads for attention. A generic 55k DGT corpus and an in-domain 88k public admin\\ncorpus were used for evaluation. A Transformer optimized model demonstrated a\\nBLEU score improvement of 7.8 points when compared with a baseline RNN model.\\nImprovements were observed across a range of metrics, including TER, indicating\\na substantially reduced post editing effort for Transformer optimized models\\nwith 16k BPE subword models. Bench-marked against Google Translate, our\\ntranslation engines demonstrated significant improvements. The question of\\nwhether or not Transformers can be used effectively in a low-resource setting\\nof English-Irish translation has been addressed. Is f\\\\'eidir linn - yes we can.\\n\", 'publish_date': 'Mon, 4 Mar 2024 12:29:59 GMT'}\n",
      "ID: 1807.03819, Distance: 0.3667772114276886, Content: {'title': 'Universal Transformers', 'doi': None, 'categories': 'cs.CL cs.LG stat.ML', 'abstract': '  Recurrent neural networks (RNNs) sequentially process data by updating their\\nstate with each new data point, and have long been the de facto choice for\\nsequence modeling tasks. However, their inherently sequential computation makes\\nthem slow to train. Feed-forward and convolutional architectures have recently\\nbeen shown to achieve superior results on some sequence modeling tasks such as\\nmachine translation, with the added advantage that they concurrently process\\nall inputs in the sequence, leading to easy parallelization and faster training\\ntimes. Despite these successes, however, popular feed-forward sequence models\\nlike the Transformer fail to generalize in many simple tasks that recurrent\\nmodels handle with ease, e.g. copying strings or even simple logical inference\\nwhen the string or formula lengths exceed those observed at training time. We\\npropose the Universal Transformer (UT), a parallel-in-time self-attentive\\nrecurrent sequence model which can be cast as a generalization of the\\nTransformer model and which addresses these issues. UTs combine the\\nparallelizability and global receptive field of feed-forward sequence models\\nlike the Transformer with the recurrent inductive bias of RNNs. We also add a\\ndynamic per-position halting mechanism and find that it improves accuracy on\\nseveral tasks. In contrast to the standard Transformer, under certain\\nassumptions, UTs can be shown to be Turing-complete. Our experiments show that\\nUTs outperform standard Transformers on a wide range of algorithmic and\\nlanguage understanding tasks, including the challenging LAMBADA language\\nmodeling task where UTs achieve a new state of the art, and machine translation\\nwhere UTs achieve a 0.9 BLEU improvement over Transformers on the WMT14 En-De\\ndataset.\\n', 'publish_date': 'Tue, 10 Jul 2018 18:39:15 GMT'}\n",
      "ID: 2103.13076, Distance: 0.38173413276672363, Content: {'title': 'Finetuning Pretrained Transformers into RNNs', 'doi': None, 'categories': 'cs.CL', 'abstract': \"  Transformers have outperformed recurrent neural networks (RNNs) in natural\\nlanguage generation. But this comes with a significant computational cost, as\\nthe attention mechanism's complexity scales quadratically with sequence length.\\nEfficient transformer variants have received increasing interest in recent\\nworks. Among them, a linear-complexity recurrent variant has proven well suited\\nfor autoregressive generation. It approximates the softmax attention with\\nrandomized or heuristic feature maps, but can be difficult to train and may\\nyield suboptimal accuracy. This work aims to convert a pretrained transformer\\ninto its efficient recurrent counterpart, improving efficiency while\\nmaintaining accuracy. Specifically, we propose a swap-then-finetune procedure:\\nin an off-the-shelf pretrained transformer, we replace the softmax attention\\nwith its linear-complexity recurrent alternative and then finetune. With a\\nlearned feature map, our approach provides an improved tradeoff between\\nefficiency and accuracy over the standard transformer and other recurrent\\nvariants. We also show that the finetuning process has lower training cost\\nrelative to training these recurrent variants from scratch. As many models for\\nnatural language tasks are increasingly dependent on large-scale pretrained\\ntransformers, this work presents a viable approach to improving inference\\nefficiency without repeating the expensive pretraining process.\\n\", 'publish_date': 'Wed, 24 Mar 2021 10:50:43 GMT'}\n",
      "ID: 1904.09408, Distance: 0.38406863808631897, Content: {'title': 'Language Models with Transformers', 'doi': None, 'categories': 'cs.CL cs.AI cs.LG', 'abstract': '  The Transformer architecture is superior to RNN-based models in computational\\nefficiency. Recently, GPT and BERT demonstrate the efficacy of Transformer\\nmodels on various NLP tasks using pre-trained language models on large-scale\\ncorpora. Surprisingly, these Transformer architectures are suboptimal for\\nlanguage model itself. Neither self-attention nor the positional encoding in\\nthe Transformer is able to efficiently incorporate the word-level sequential\\ncontext crucial to language modeling.\\n  In this paper, we explore effective Transformer architectures for language\\nmodel, including adding additional LSTM layers to better capture the sequential\\ncontext while still keeping the computation efficient. We propose Coordinate\\nArchitecture Search (CAS) to find an effective architecture through iterative\\nrefinement of the model. Experimental results on the PTB, WikiText-2, and\\nWikiText-103 show that CAS achieves perplexities between 20.42 and 34.11 on all\\nproblems, i.e. on average an improvement of 12.0 perplexity units compared to\\nstate-of-the-art LSTMs. The source code is publicly available.\\n', 'publish_date': 'Sat, 20 Apr 2019 06:43:14 GMT'}\n",
      "\n",
      "Answer:\n",
      "\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "生成嵌入中: 100%|██████████| 1/1 [00:00<00:00, 10.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "cost 0.1726 seconds\n",
      "\n",
      "\n",
      "You must reply in English. You are a professional academic assistant specializing in the cs.CL domain. Strictly based on the provided literature information, answer the user's query. You may slightly expand and refine the response using existing knowledge, but avoid fabricating information. The response language must be English. Ensure the response is concise and limited to 500 words. Include a citation with the corresponding literature link (like [text](f\"https://arxiv.org/abs/{{id}}\")) and adjust the response style based on the temperature parameter (from 0.0 to 1.0). If the temperature is low (e.g., 0.2), provide a strict and precise answer. If the temperature is high (e.g., 0.8), provide a more creative and exploratory answer.\n",
      "\n",
      "Temperature: 0.2\n",
      "Question: 大语言模型的 “幻觉” 问题本质是语义理解缺陷还是推理过程偏差？如何构建可靠性评估框架？\n",
      "Relevant Literature: \n",
      "ID: 2403.20009, Distance: 0.29913201928138733, Content: {'title': \"On Large Language Models' Hallucination with Regard to Known Facts\", 'doi': None, 'categories': 'cs.CL cs.LG', 'abstract': \"  Large language models are successful in answering factoid questions but are\\nalso prone to hallucination. We investigate the phenomenon of LLMs possessing\\ncorrect answer knowledge yet still hallucinating from the perspective of\\ninference dynamics, an area not previously covered in studies on\\nhallucinations. We are able to conduct this analysis via two key ideas. First,\\nwe identify the factual questions that query the same triplet knowledge but\\nresult in different answers. The difference between the model behaviors on the\\ncorrect and incorrect outputs hence suggests the patterns when hallucinations\\nhappen. Second, to measure the pattern, we utilize mappings from the residual\\nstreams to vocabulary space. We reveal the different dynamics of the output\\ntoken probabilities along the depths of layers between the correct and\\nhallucinated cases. In hallucinated cases, the output token's information\\nrarely demonstrates abrupt increases and consistent superiority in the later\\nstages of the model. Leveraging the dynamic curve as a feature, we build a\\nclassifier capable of accurately detecting hallucinatory predictions with an\\n88\\\\% success rate. Our study shed light on understanding the reasons for LLMs'\\nhallucinations on their known facts, and more importantly, on accurately\\npredicting when they are hallucinating.\\n\", 'publish_date': 'Fri, 29 Mar 2024 06:48:30 GMT'}\n",
      "ID: 2305.14552, Distance: 0.3027987480163574, Content: {'title': 'Sources of Hallucination by Large Language Models on Inference Tasks', 'doi': None, 'categories': 'cs.CL cs.AI', 'abstract': \"  Large Language Models (LLMs) are claimed to be capable of Natural Language\\nInference (NLI), necessary for applied tasks like question answering and\\nsummarization. We present a series of behavioral studies on several LLM\\nfamilies (LLaMA, GPT-3.5, and PaLM) which probe their behavior using controlled\\nexperiments. We establish two biases originating from pretraining which predict\\nmuch of their behavior, and show that these are major sources of hallucination\\nin generative LLMs. First, memorization at the level of sentences: we show\\nthat, regardless of the premise, models falsely label NLI test samples as\\nentailing when the hypothesis is attested in training data, and that entities\\nare used as ``indices'' to access the memorized data. Second, statistical\\npatterns of usage learned at the level of corpora: we further show a similar\\neffect when the premise predicate is less frequent than that of the hypothesis\\nin the training data, a bias following from previous studies. We demonstrate\\nthat LLMs perform significantly worse on NLI test samples which do not conform\\nto these biases than those which do, and we offer these as valuable controls\\nfor future LLM evaluation.\\n\", 'publish_date': 'Tue, 23 May 2023 22:24:44 GMT'}\n",
      "ID: 2402.10543, Distance: 0.30965283513069153, Content: {'title': 'Strong hallucinations from negation and how to fix them', 'doi': None, 'categories': 'cs.CL cs.AI', 'abstract': \"  Despite great performance on many tasks, language models (LMs) still struggle\\nwith reasoning, sometimes providing responses that cannot possibly be true\\nbecause they stem from logical incoherence. We call such responses\\n\\\\textit{strong hallucinations} and prove that they follow from an LM's\\ncomputation of its internal representations for logical operators and outputs\\nfrom those representations. Focusing on negation, we provide a novel solution\\nin which negation is treated not as another element of a latent representation,\\nbut as \\\\textit{an operation over an LM's latent representations that constrains\\nhow they may evolve}. We show that our approach improves model performance in\\ncloze prompting and natural language inference tasks with negation without\\nrequiring training on sparse negative data.\\n\", 'publish_date': 'Fri, 16 Feb 2024 10:11:20 GMT'}\n",
      "ID: 2311.14648, Distance: 0.31446754932403564, Content: {'title': 'Calibrated Language Models Must Hallucinate', 'doi': None, 'categories': 'cs.CL cs.AI', 'abstract': '  Recent language models generate false but plausible-sounding text with\\nsurprising frequency. Such \"hallucinations\" are an obstacle to the usability of\\nlanguage-based AI systems and can harm people who rely upon their outputs. This\\nwork shows that there is an inherent statistical lower-bound on the rate that\\npretrained language models hallucinate certain types of facts, having nothing\\nto do with the transformer LM architecture or data quality. For \"arbitrary\"\\nfacts whose veracity cannot be determined from the training data, we show that\\nhallucinations must occur at a certain rate for language models that satisfy a\\nstatistical calibration condition appropriate for generative language models.\\nSpecifically, if the maximum probability of any fact is bounded, we show that\\nthe probability of generating a hallucination is close to the fraction of facts\\nthat occur exactly once in the training data (a \"Good-Turing\" estimate), even\\nassuming ideal training data without errors.\\n  One conclusion is that models pretrained to be sufficiently good predictors\\n(i.e., calibrated) may require post-training to mitigate hallucinations on the\\ntype of arbitrary facts that tend to appear once in the training set. However,\\nour analysis also suggests that there is no statistical reason that pretraining\\nwill lead to hallucination on facts that tend to appear more than once in the\\ntraining data (like references to publications such as articles and books,\\nwhose hallucinations have been particularly notable and problematic) or on\\nsystematic facts (like arithmetic calculations). Therefore, different\\narchitectures and learning algorithms may mitigate these latter types of\\nhallucinations.\\n', 'publish_date': 'Fri, 24 Nov 2023 18:29:50 GMT'}\n",
      "ID: 2401.11817, Distance: 0.3182944655418396, Content: {'title': 'Hallucination is Inevitable: An Innate Limitation of Large Language\\n  Models', 'doi': None, 'categories': 'cs.CL cs.AI cs.LG', 'abstract': '  Hallucination has been widely recognized to be a significant drawback for\\nlarge language models (LLMs). There have been many works that attempt to reduce\\nthe extent of hallucination. These efforts have mostly been empirical so far,\\nwhich cannot answer the fundamental question whether it can be completely\\neliminated. In this paper, we formalize the problem and show that it is\\nimpossible to eliminate hallucination in LLMs. Specifically, we define a formal\\nworld where hallucination is defined as inconsistencies between a computable\\nLLM and a computable ground truth function. By employing results from learning\\ntheory, we show that LLMs cannot learn all the computable functions and will\\ntherefore inevitably hallucinate if used as general problem solvers. Since the\\nformal world is a part of the real world which is much more complicated,\\nhallucinations are also inevitable for real world LLMs. Furthermore, for real\\nworld LLMs constrained by provable time complexity, we describe the\\nhallucination-prone tasks and empirically validate our claims. Finally, using\\nthe formal world framework, we discuss the possible mechanisms and efficacies\\nof existing hallucination mitigators as well as the practical implications on\\nthe safe deployment of LLMs.\\n', 'publish_date': 'Mon, 22 Jan 2024 10:26:14 GMT'}\n",
      "\n",
      "Answer:\n",
      "\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "生成嵌入中: 100%|██████████| 1/1 [00:00<00:00, 15.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "cost 0.1273 seconds\n",
      "\n",
      "\n",
      "You must reply in English. You are a professional academic assistant specializing in the cs.CL domain. Strictly based on the provided literature information, answer the user's query. You may slightly expand and refine the response using existing knowledge, but avoid fabricating information. The response language must be English. Ensure the response is concise and limited to 500 words. Include a citation with the corresponding literature link (like [text](f\"https://arxiv.org/abs/{{id}}\")) and adjust the response style based on the temperature parameter (from 0.0 to 1.0). If the temperature is low (e.g., 0.2), provide a strict and precise answer. If the temperature is high (e.g., 0.8), provide a more creative and exploratory answer.\n",
      "\n",
      "Temperature: 0.2\n",
      "Question: 零样本学习中，提示词工程能否真正激活模型的 “潜在知识”？其理论边界在哪？\n",
      "Relevant Literature: \n",
      "ID: 2209.15206, Distance: 0.38994061946868896, Content: {'title': 'What Makes Pre-trained Language Models Better Zero-shot Learners?', 'doi': None, 'categories': 'cs.CL cs.AI', 'abstract': '  Current methods for prompt learning in zeroshot scenarios widely rely on a\\ndevelopment set with sufficient human-annotated data to select the\\nbest-performing prompt template a posteriori. This is not ideal because in a\\nrealworld zero-shot scenario of practical relevance, no labelled data is\\navailable. Thus, we propose a simple yet effective method for screening\\nreasonable prompt templates in zero-shot text classification: Perplexity\\nSelection (Perplection). We hypothesize that language discrepancy can be used\\nto measure the efficacy of prompt templates, and thereby develop a\\nsubstantiated perplexity-based scheme allowing for forecasting the performance\\nof prompt templates in advance. Experiments show that our method leads to\\nimproved prediction performance in a realistic zero-shot setting, eliminating\\nthe need for any labelled examples.\\n', 'publish_date': 'Fri, 30 Sep 2022 03:28:19 GMT'}\n",
      "ID: 2109.01247, Distance: 0.3977474570274353, Content: {'title': 'Do Prompt-Based Models Really Understand the Meaning of their Prompts?', 'doi': None, 'categories': 'cs.CL', 'abstract': '  Recently, a boom of papers has shown extraordinary progress in zero-shot and\\nfew-shot learning with various prompt-based models. It is commonly argued that\\nprompts help models to learn faster in the same way that humans learn faster\\nwhen provided with task instructions expressed in natural language. In this\\nstudy, we experiment with over 30 prompt templates manually written for natural\\nlanguage inference (NLI). We find that models learn just as fast with many\\nprompts that are intentionally irrelevant or even pathologically misleading as\\nthey do with instructively \"good\" prompts. Further, such patterns hold even for\\nmodels as large as 175 billion parameters (Brown et al., 2020) as well as the\\nrecently proposed instruction-tuned models which are trained on hundreds of\\nprompts (Sanh et al., 2022). That is, instruction-tuned models often produce\\ngood predictions with irrelevant and misleading prompts even at zero shots. In\\nsum, notwithstanding prompt-based models\\' impressive improvement, we find\\nevidence of serious limitations that question the degree to which such\\nimprovement is derived from models understanding task instructions in ways\\nanalogous to humans\\' use of task instructions.\\n', 'publish_date': 'Thu, 2 Sep 2021 23:46:36 GMT'}\n",
      "ID: 2305.14310, Distance: 0.4021965265274048, Content: {'title': 'Navigating Prompt Complexity for Zero-Shot Classification: A Study of\\n  Large Language Models in Computational Social Science', 'doi': None, 'categories': 'cs.CL', 'abstract': '  Instruction-tuned Large Language Models (LLMs) have exhibited impressive\\nlanguage understanding and the capacity to generate responses that follow\\nspecific prompts. However, due to the computational demands associated with\\ntraining these models, their applications often adopt a zero-shot setting. In\\nthis paper, we evaluate the zero-shot performance of two publicly accessible\\nLLMs, ChatGPT and OpenAssistant, in the context of six Computational Social\\nScience classification tasks, while also investigating the effects of various\\nprompting strategies. Our experiments investigate the impact of prompt\\ncomplexity, including the effect of incorporating label definitions into the\\nprompt; use of synonyms for label names; and the influence of integrating past\\nmemories during foundation model training. The findings indicate that in a\\nzero-shot setting, current LLMs are unable to match the performance of smaller,\\nfine-tuned baseline transformer models (such as BERT-large). Additionally, we\\nfind that different prompting strategies can significantly affect\\nclassification accuracy, with variations in accuracy and F1 scores exceeding\\n10\\\\%.\\n', 'publish_date': 'Tue, 23 May 2023 17:48:21 GMT'}\n",
      "ID: 2309.13205, Distance: 0.4152686297893524, Content: {'title': 'A Practical Survey on Zero-shot Prompt Design for In-context Learning', 'doi': '10.26615/978-954-452-092-2_069', 'categories': 'cs.CL cs.AI cs.ET cs.LG', 'abstract': '  The remarkable advancements in large language models (LLMs) have brought\\nabout significant improvements in Natural Language Processing(NLP) tasks. This\\npaper presents a comprehensive review of in-context learning techniques,\\nfocusing on different types of prompts, including discrete, continuous,\\nfew-shot, and zero-shot, and their impact on LLM performance. We explore\\nvarious approaches to prompt design, such as manual design, optimization\\nalgorithms, and evaluation methods, to optimize LLM performance across diverse\\ntasks. Our review covers key research studies in prompt engineering, discussing\\ntheir methodologies and contributions to the field. We also delve into the\\nchallenges faced in evaluating prompt performance, given the absence of a\\nsingle \"best\" prompt and the importance of considering multiple metrics. In\\nconclusion, the paper highlights the critical role of prompt design in\\nharnessing the full potential of LLMs and provides insights into the\\ncombination of manual design, optimization techniques, and rigorous evaluation\\nfor more effective and efficient use of LLMs in various NLP tasks.\\n', 'publish_date': 'Fri, 22 Sep 2023 23:00:34 GMT'}\n",
      "ID: 2210.14803, Distance: 0.41742053627967834, Content: {'title': \"Don't Prompt, Search! Mining-based Zero-Shot Learning with Language\\n  Models\", 'doi': None, 'categories': 'cs.CL cs.AI cs.LG', 'abstract': '  Masked language models like BERT can perform text classification in a\\nzero-shot fashion by reformulating downstream tasks as text infilling. However,\\nthis approach is highly sensitive to the template used to prompt the model, yet\\npractitioners are blind when designing them in strict zero-shot settings. In\\nthis paper, we propose an alternative mining-based approach for zero-shot\\nlearning. Instead of prompting language models, we use regular expressions to\\nmine labeled examples from unlabeled corpora, which can optionally be filtered\\nthrough prompting, and used to finetune a pretrained model. Our method is more\\nflexible and interpretable than prompting, and outperforms it on a wide range\\nof tasks when using comparable templates. Our results suggest that the success\\nof prompting can partly be explained by the model being exposed to similar\\nexamples during pretraining, which can be directly retrieved through regular\\nexpressions.\\n', 'publish_date': 'Wed, 26 Oct 2022 15:52:30 GMT'}\n",
      "\n",
      "Answer:\n",
      "\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "生成嵌入中: 100%|██████████| 1/1 [00:00<00:00, 12.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "cost 0.1398 seconds\n",
      "\n",
      "\n",
      "You must reply in English. You are a professional academic assistant specializing in the cs.CL domain. Strictly based on the provided literature information, answer the user's query. You may slightly expand and refine the response using existing knowledge, but avoid fabricating information. The response language must be English. Ensure the response is concise and limited to 500 words. Include a citation with the corresponding literature link (like [text](f\"https://arxiv.org/abs/{{id}}\")) and adjust the response style based on the temperature parameter (from 0.0 to 1.0). If the temperature is low (e.g., 0.2), provide a strict and precise answer. If the temperature is high (e.g., 0.8), provide a more creative and exploratory answer.\n",
      "\n",
      "Temperature: 0.2\n",
      "Question: 小样本场景下，元学习与预训练模型的适应性调整策略如何结合？\n",
      "Relevant Literature: \n",
      "ID: 2305.14521, Distance: 0.41722217202186584, Content: {'title': 'Few-shot Adaptation to Distribution Shifts By Mixing Source and Target\\n  Embeddings', 'doi': None, 'categories': 'cs.LG cs.CL cs.CV', 'abstract': '  Pretrained machine learning models need to be adapted to distribution shifts\\nwhen deployed in new target environments. When obtaining labeled data from the\\ntarget distribution is expensive, few-shot adaptation with only a few examples\\nfrom the target distribution becomes essential. In this work, we propose\\nMixPro, a lightweight and highly data-efficient approach for few-shot\\nadaptation. MixPro first generates a relatively large dataset by mixing\\n(linearly combining) pre-trained embeddings of large source data with those of\\nthe few target examples. This process preserves important features of both\\nsource and target distributions, while mitigating the specific noise in the\\nsmall target data. Then, it trains a linear classifier on the mixed embeddings\\nto effectively adapts the model to the target distribution without overfitting\\nthe small target data. Theoretically, we demonstrate the advantages of MixPro\\nover previous methods. Our experiments, conducted across various model\\narchitectures on 8 datasets featuring different types of distribution shifts,\\nreveal that MixPro can outperform baselines by up to 7\\\\%, with only 2-4 target\\nexamples.\\n', 'publish_date': 'Tue, 23 May 2023 20:49:45 GMT'}\n",
      "ID: 2204.03044, Distance: 0.42667075991630554, Content: {'title': 'Fusing finetuned models for better pretraining', 'doi': None, 'categories': 'cs.CL cs.CV cs.LG', 'abstract': '  Pretrained models are the standard starting point for training. This approach\\nconsistently outperforms the use of a random initialization. However,\\npretraining is a costly endeavour that few can undertake.\\n  In this paper, we create better base models at hardly any cost, by fusing\\nmultiple existing fine tuned models into one. Specifically, we fuse by\\naveraging the weights of these models. We show that the fused model results\\nsurpass the pretrained model ones. We also show that fusing is often better\\nthan intertraining.\\n  We find that fusing is less dependent on the target task. Furthermore, weight\\ndecay nullifies intertraining effects but not those of fusing.\\n', 'publish_date': 'Wed, 6 Apr 2022 18:54:48 GMT'}\n",
      "ID: 1908.03265, Distance: 0.4296816289424896, Content: {'title': 'On the Variance of the Adaptive Learning Rate and Beyond', 'doi': None, 'categories': 'cs.LG cs.CL stat.ML', 'abstract': '  The learning rate warmup heuristic achieves remarkable success in stabilizing\\ntraining, accelerating convergence and improving generalization for adaptive\\nstochastic optimization algorithms like RMSprop and Adam. Here, we study its\\nmechanism in details. Pursuing the theory behind warmup, we identify a problem\\nof the adaptive learning rate (i.e., it has problematically large variance in\\nthe early stage), suggest warmup works as a variance reduction technique, and\\nprovide both empirical and theoretical evidence to verify our hypothesis. We\\nfurther propose RAdam, a new variant of Adam, by introducing a term to rectify\\nthe variance of the adaptive learning rate. Extensive experimental results on\\nimage classification, language modeling, and neural machine translation verify\\nour intuition and demonstrate the effectiveness and robustness of our proposed\\nmethod. All implementations are available at:\\nhttps://github.com/LiyuanLucasLiu/RAdam.\\n', 'publish_date': 'Thu, 8 Aug 2019 20:51:17 GMT'}\n",
      "ID: 2311.11973, Distance: 0.45514047145843506, Content: {'title': 'Adaptive Training Distributions with Scalable Online Bilevel\\n  Optimization', 'doi': None, 'categories': 'cs.LG cs.CL', 'abstract': '  Large neural networks pretrained on web-scale corpora are central to modern\\nmachine learning. In this paradigm, the distribution of the large,\\nheterogeneous pretraining data rarely matches that of the application domain.\\nThis work considers modifying the pretraining distribution in the case where\\none has a small sample of data reflecting the targeted test conditions. We\\npropose an algorithm motivated by a recent formulation of this setting as an\\nonline, bilevel optimization problem. With scalability in mind, our algorithm\\nprioritizes computing gradients at training points which are likely to most\\nimprove the loss on the targeted distribution. Empirically, we show that in\\nsome cases this approach is beneficial over existing strategies from the domain\\nadaptation literature but may not succeed in other cases. We propose a simple\\ntest to evaluate when our approach can be expected to work well and point\\ntowards further research to address current limitations.\\n', 'publish_date': 'Mon, 20 Nov 2023 18:01:29 GMT'}\n",
      "ID: cmp-lg/9806003, Distance: 0.4585559070110321, Content: {'title': 'Lazy Transformation-Based Learning', 'doi': None, 'categories': 'cmp-lg cs.CL', 'abstract': '  We introduce a significant improvement for a relatively new machine learning\\nmethod called Transformation-Based Learning. By applying a Monte Carlo strategy\\nto randomly sample from the space of rules, rather than exhaustively analyzing\\nall possible rules, we drastically reduce the memory and time costs of the\\nalgorithm, without compromising accuracy on unseen data. This enables\\nTransformation- Based Learning to apply to a wider range of domains, as it can\\neffectively consider a larger number of different features and feature\\ninteractions in the data. In addition, the Monte Carlo improvement decreases\\nthe labor demands on the human developer, who no longer needs to develop a\\nminimal set of rule templates to maintain tractability.\\n', 'publish_date': 'Wed, 3 Jun 1998 16:47:37 GMT'}\n",
      "\n",
      "Answer:\n",
      "\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "生成嵌入中: 100%|██████████| 1/1 [00:00<00:00, 14.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "cost 0.1321 seconds\n",
      "\n",
      "\n",
      "You must reply in English. You are a professional academic assistant specializing in the cs.CL domain. Strictly based on the provided literature information, answer the user's query. You may slightly expand and refine the response using existing knowledge, but avoid fabricating information. The response language must be English. Ensure the response is concise and limited to 500 words. Include a citation with the corresponding literature link (like [text](f\"https://arxiv.org/abs/{{id}}\")) and adjust the response style based on the temperature parameter (from 0.0 to 1.0). If the temperature is low (e.g., 0.2), provide a strict and precise answer. If the temperature is high (e.g., 0.8), provide a more creative and exploratory answer.\n",
      "\n",
      "Temperature: 0.2\n",
      "Question: 预训练模型的 “涌现能力” 是否具有可预测性？如何从理论上解释其突现机制？\n",
      "Relevant Literature: \n",
      "ID: 1512.01926, Distance: 0.49014726281166077, Content: {'title': 'Thinking Required', 'doi': None, 'categories': 'cs.LG cs.AI cs.CL', 'abstract': '  There exists a theory of a single general-purpose learning algorithm which\\ncould explain the principles its operation. It assumes the initial rough\\narchitecture, a small library of simple innate circuits which are prewired at\\nbirth. and proposes that all significant mental algorithms are learned. Given\\ncurrent understanding and observations, this paper reviews and lists the\\ningredients of such an algorithm from architectural and functional\\nperspectives.\\n', 'publish_date': 'Mon, 7 Dec 2015 06:37:49 GMT'}\n",
      "ID: 2505.09855, Distance: 0.49173110723495483, Content: {'title': 'Predictability Shapes Adaptation: An Evolutionary Perspective on Modes\\n  of Learning in Transformers', 'doi': None, 'categories': 'cs.LG cs.AI cs.CL', 'abstract': \"  Transformer models learn in two distinct modes: in-weights learning (IWL),\\nencoding knowledge into model weights, and in-context learning (ICL), adapting\\nflexibly to context without weight modification. To better understand the\\ninterplay between these learning modes, we draw inspiration from evolutionary\\nbiology's analogous adaptive strategies: genetic encoding (akin to IWL,\\nadapting over generations and fixed within an individual's lifetime) and\\nphenotypic plasticity (akin to ICL, enabling flexible behavioral responses to\\nenvironmental cues). In evolutionary biology, environmental predictability\\ndictates the balance between these strategies: stability favors genetic\\nencoding, while reliable predictive cues promote phenotypic plasticity. We\\nexperimentally operationalize these dimensions of predictability and\\nsystematically investigate their influence on the ICL/IWL balance in\\nTransformers. Using regression and classification tasks, we show that high\\nenvironmental stability decisively favors IWL, as predicted, with a sharp\\ntransition at maximal stability. Conversely, high cue reliability enhances ICL\\nefficacy, particularly when stability is low. Furthermore, learning dynamics\\nreveal task-contingent temporal evolution: while a canonical ICL-to-IWL shift\\noccurs in some settings (e.g., classification with many classes), we\\ndemonstrate that scenarios with easier IWL (e.g., fewer classes) or slower ICL\\nacquisition (e.g., regression) can exhibit an initial IWL phase later yielding\\nto ICL dominance. These findings support a relative-cost hypothesis for\\nexplaining these learning mode transitions, establishing predictability as a\\ncritical factor governing adaptive strategies in Transformers, and offering\\nnovel insights for understanding ICL and guiding training methodologies.\\n\", 'publish_date': 'Wed, 14 May 2025 23:31:17 GMT'}\n",
      "ID: 1511.06379, Distance: 0.4974609911441803, Content: {'title': 'Dynamic Adaptive Network Intelligence', 'doi': None, 'categories': 'cs.CL cs.LG', 'abstract': '  Accurate representational learning of both the explicit and implicit\\nrelationships within data is critical to the ability of machines to perform\\nmore complex and abstract reasoning tasks. We describe the efficient weakly\\nsupervised learning of such inferences by our Dynamic Adaptive Network\\nIntelligence (DANI) model. We report state-of-the-art results for DANI over\\nquestion answering tasks in the bAbI dataset that have proved difficult for\\ncontemporary approaches to learning representation (Weston et al., 2015).\\n', 'publish_date': 'Thu, 19 Nov 2015 21:07:27 GMT'}\n",
      "ID: 2305.18390, Distance: 0.5104956030845642, Content: {'title': 'Emergent Modularity in Pre-trained Transformers', 'doi': None, 'categories': 'cs.CL cs.LG', 'abstract': '  This work examines the presence of modularity in pre-trained Transformers, a\\nfeature commonly found in human brains and thought to be vital for general\\nintelligence. In analogy to human brains, we consider two main characteristics\\nof modularity: (1) functional specialization of neurons: we evaluate whether\\neach neuron is mainly specialized in a certain function, and find that the\\nanswer is yes. (2) function-based neuron grouping: we explore finding a\\nstructure that groups neurons into modules by function, and each module works\\nfor its corresponding function. Given the enormous amount of possible\\nstructures, we focus on Mixture-of-Experts as a promising candidate, which\\npartitions neurons into experts and usually activates different experts for\\ndifferent inputs. Experimental results show that there are functional experts,\\nwhere clustered are the neurons specialized in a certain function. Moreover,\\nperturbing the activations of functional experts significantly affects the\\ncorresponding function. Finally, we study how modularity emerges during\\npre-training, and find that the modular structure is stabilized at the early\\nstage, which is faster than neuron stabilization. It suggests that Transformers\\nfirst construct the modular structure and then learn fine-grained neuron\\nfunctions. Our code and data are available at\\nhttps://github.com/THUNLP/modularity-analysis.\\n', 'publish_date': 'Sun, 28 May 2023 11:02:32 GMT'}\n",
      "ID: 2201.10222, Distance: 0.5105546712875366, Content: {'title': 'Explanatory Learning: Beyond Empiricism in Neural Networks', 'doi': None, 'categories': 'cs.LG cs.AI cs.CL physics.hist-ph', 'abstract': '  We introduce Explanatory Learning (EL), a framework to let machines use\\nexisting knowledge buried in symbolic sequences -- e.g. explanations written in\\nhieroglyphic -- by autonomously learning to interpret them. In EL, the burden\\nof interpreting symbols is not left to humans or rigid human-coded compilers,\\nas done in Program Synthesis. Rather, EL calls for a learned interpreter, built\\nupon a limited collection of symbolic sequences paired with observations of\\nseveral phenomena. This interpreter can be used to make predictions on a novel\\nphenomenon given its explanation, and even to find that explanation using only\\na handful of observations, like human scientists do. We formulate the EL\\nproblem as a simple binary classification task, so that common end-to-end\\napproaches aligned with the dominant empiricist view of machine learning could,\\nin principle, solve it. To these models, we oppose Critical Rationalist\\nNetworks (CRNs), which instead embrace a rationalist view on the acquisition of\\nknowledge. CRNs express several desired properties by construction, they are\\ntruly explainable, can adjust their processing at test-time for harder\\ninferences, and can offer strong confidence guarantees on their predictions. As\\na final contribution, we introduce Odeen, a basic EL environment that simulates\\na small flatland-style universe full of phenomena to explain. Using Odeen as a\\ntestbed, we show how CRNs outperform empiricist end-to-end approaches of\\nsimilar size and architecture (Transformers) in discovering explanations for\\nnovel phenomena.\\n', 'publish_date': 'Tue, 25 Jan 2022 10:21:53 GMT'}\n",
      "\n",
      "Answer:\n",
      "\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "生成嵌入中: 100%|██████████| 1/1 [00:00<00:00, 12.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "cost 0.1385 seconds\n",
      "\n",
      "\n",
      "You must reply in English. You are a professional academic assistant specializing in the cs.CL domain. Strictly based on the provided literature information, answer the user's query. You may slightly expand and refine the response using existing knowledge, but avoid fabricating information. The response language must be English. Ensure the response is concise and limited to 500 words. Include a citation with the corresponding literature link (like [text](f\"https://arxiv.org/abs/{{id}}\")) and adjust the response style based on the temperature parameter (from 0.0 to 1.0). If the temperature is low (e.g., 0.2), provide a strict and precise answer. If the temperature is high (e.g., 0.8), provide a more creative and exploratory answer.\n",
      "\n",
      "Temperature: 0.2\n",
      "Question: 大模型的上下文窗口扩展是否会引入 “长距离语义稀释”？如何设计高效的长期依赖建模机制？\n",
      "Relevant Literature: \n",
      "ID: 2405.17915, Distance: 0.3706572949886322, Content: {'title': 'Long Context is Not Long at All: A Prospector of Long-Dependency Data\\n  for Large Language Models', 'doi': None, 'categories': 'cs.CL', 'abstract': '  Long-context modeling capabilities are important for large language models\\n(LLMs) in various applications. However, directly training LLMs with long\\ncontext windows is insufficient to enhance this capability since some training\\nsamples do not exhibit strong semantic dependencies across long contexts. In\\nthis study, we propose a data mining framework \\\\textbf{ProLong} that can assign\\neach training sample with a long dependency score, which can be used to rank\\nand filter samples that are more advantageous for enhancing long-context\\nmodeling abilities in LLM training. Specifically, we first use delta perplexity\\nscores to measure the \\\\textit{Dependency Strength} between text segments in a\\ngiven document. Then we refine this metric based on the \\\\textit{Dependency\\nDistance} of these segments to incorporate spatial relationships across\\nlong-contexts. Final results are calibrated with a \\\\textit{Dependency\\nSpecificity} metric to prevent trivial dependencies introduced by repetitive\\npatterns. Moreover, a random sampling approach is proposed to optimize the\\ncomputational efficiency of ProLong. Comprehensive experiments on multiple\\nbenchmarks indicate that ProLong effectively identifies documents that carry\\nlong dependencies and LLMs trained on these documents exhibit significantly\\nenhanced long-context modeling capabilities.\\n', 'publish_date': 'Tue, 28 May 2024 07:36:56 GMT'}\n",
      "ID: 2312.09571, Distance: 0.3711542785167694, Content: {'title': 'Extending Context Window of Large Language Models via Semantic\\n  Compression', 'doi': None, 'categories': 'cs.CL cs.IT math.IT', 'abstract': '  Transformer-based Large Language Models (LLMs) often impose limitations on\\nthe length of the text input to ensure the generation of fluent and relevant\\nresponses. This constraint restricts their applicability in scenarios involving\\nlong texts. We propose a novel semantic compression method that enables\\ngeneralization to texts that are 6-8 times longer, without incurring\\nsignificant computational costs or requiring fine-tuning. Our proposed\\nframework draws inspiration from source coding in information theory and\\nemploys a pre-trained model to reduce the semantic redundancy of long inputs\\nbefore passing them to the LLMs for downstream tasks. Experimental results\\ndemonstrate that our method effectively extends the context window of LLMs\\nacross a range of tasks including question answering, summarization, few-shot\\nlearning, and information retrieval. Furthermore, the proposed semantic\\ncompression method exhibits consistent fluency in text generation while\\nreducing the associated computational overhead.\\n', 'publish_date': 'Fri, 15 Dec 2023 07:04:33 GMT'}\n",
      "ID: 2404.02060, Distance: 0.38668161630630493, Content: {'title': 'Long-context LLMs Struggle with Long In-context Learning', 'doi': None, 'categories': 'cs.CL cs.AI', 'abstract': '  Large Language Models (LLMs) have made significant strides in handling long\\nsequences. Some models like Gemini could even to be capable of dealing with\\nmillions of tokens. However, their performance evaluation has largely been\\nconfined to metrics like perplexity and synthetic tasks, which may not fully\\ncapture their true abilities in more challenging, real-world scenarios. We\\nintroduce a benchmark (LongICLBench) for long in-context learning in\\nextreme-label classification using six datasets with 28 to 174 classes and\\ninput lengths from 2K to 50K tokens. Our benchmark requires LLMs to comprehend\\nthe entire input to recognize the massive label spaces to make correct\\npredictions. We evaluate on 15 long-context LLMs and find that they perform\\nwell on less challenging classification tasks with smaller label space and\\nshorter demonstrations. However, they struggle with more challenging task like\\nDiscovery with 174 labels, suggesting a gap in their ability to process long,\\ncontext-rich sequences. Further analysis reveals a bias towards labels\\npresented later in the sequence and a need for improved reasoning over multiple\\npieces of information. Our study reveals that long context understanding and\\nreasoning is still a challenging task for the existing LLMs. We believe\\nLongICLBench could serve as a more realistic evaluation for the future\\nlong-context LLMs.\\n', 'publish_date': 'Tue, 2 Apr 2024 15:59:11 GMT'}\n",
      "ID: 2311.04939, Distance: 0.389009952545166, Content: {'title': 'LooGLE: Can Long-Context Language Models Understand Long Contexts?', 'doi': None, 'categories': 'cs.CL cs.AI', 'abstract': '  Large language models (LLMs), despite their impressive performance in various\\nlanguage tasks, are typically limited to processing texts within context-window\\nsize. This limitation has spurred significant research efforts to enhance LLMs\\'\\nlong-context understanding with high-quality long-sequence benchmarks. However,\\nprior datasets in this regard suffer from shortcomings, such as short context\\nlength compared to the context window of modern LLMs; outdated documents that\\nhave data leakage problems; and an emphasis on short dependency tasks rather\\nthan long dependency tasks. In this paper, we present LooGLE, a Long Context\\nGeneric Language Evaluation benchmark for LLMs\\' long context understanding.\\nLooGLE features relatively new documents post-2022, with over 24,000 tokens per\\ndocument and 6,000 newly generated questions spanning diverse domains. Human\\nannotators meticulously crafted more than 1,100 high-quality question-answer\\npairs to meet the long dependency requirements. These pairs underwent thorough\\ncross-validation, yielding the most precise assessment of LLMs\\' long dependency\\ncapabilities. The evaluation of eight state-of-the-art LLMs on LooGLE revealed\\nkey findings: (i) commercial models outperformed open-sourced models; (ii) LLMs\\nexcelled in short dependency tasks like short question-answering and cloze\\ntasks but struggled with more intricate long dependency tasks; (iii) in-context\\nlearning and chaining thoughts offered only marginal improvements; (iv)\\nretrieval-based techniques demonstrated substantial benefits for short\\nquestion-answering, while strategies for extending context window length had\\nlimited impact on long context understanding. As such, LooGLE not only provides\\na systematic and comprehensive evaluation schema on long-context LLMs, but also\\nsheds light on future development of enhanced models towards \"true long-context\\nunderstanding\".\\n', 'publish_date': 'Wed, 8 Nov 2023 01:45:37 GMT'}\n",
      "ID: 2212.10947, Distance: 0.3892439305782318, Content: {'title': 'Parallel Context Windows for Large Language Models', 'doi': None, 'categories': 'cs.CL', 'abstract': \"  When applied to processing long text, Large Language Models (LLMs) are\\nlimited by their context window. Existing efforts to address this limitation\\ninvolve training specialized architectures, and cannot be easily applied to\\noff-the-shelf LLMs. We present Parallel Context Windows (PCW), a method that\\nalleviates the context window restriction for any off-the-shelf LLM without\\nfurther training. The key to the approach is to carve a long context into\\nchunks (``windows''), restrict the attention mechanism to apply only within\\neach window, and re-use the positional embeddings across the windows. Our main\\nresults test the PCW approach on in-context learning with models that range in\\nsize between 750 million and 178 billion parameters, and show substantial\\nimprovements for tasks with diverse input and output spaces. We show additional\\nbenefits in other settings where long context windows may be beneficial:\\nmulti-hop questions and retrieval-augmented question answering with multiple\\nretrieved documents. Our results highlight Parallel Context Windows as a\\npromising method for applying off-the-shelf LLMs in a range of settings that\\nrequire long text sequences. We make our code publicly available at\\nhttps://github.com/ai21labs/parallel-context-windows.\\n\", 'publish_date': 'Wed, 21 Dec 2022 11:38:51 GMT'}\n",
      "\n",
      "Answer:\n",
      "\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "生成嵌入中: 100%|██████████| 1/1 [00:00<00:00, 10.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "cost 0.1557 seconds\n",
      "\n",
      "\n",
      "You must reply in English. You are a professional academic assistant specializing in the cs.CL domain. Strictly based on the provided literature information, answer the user's query. You may slightly expand and refine the response using existing knowledge, but avoid fabricating information. The response language must be English. Ensure the response is concise and limited to 500 words. Include a citation with the corresponding literature link (like [text](f\"https://arxiv.org/abs/{{id}}\")) and adjust the response style based on the temperature parameter (from 0.0 to 1.0). If the temperature is low (e.g., 0.2), provide a strict and precise answer. If the temperature is high (e.g., 0.8), provide a more creative and exploratory answer.\n",
      "\n",
      "Temperature: 0.2\n",
      "Question: 句法语义一体化建模中，成分结构与依存结构能否统一表示？\n",
      "Relevant Literature: \n",
      "ID: cmp-lg/9604021, Distance: 0.31888869404792786, Content: {'title': 'Extended Dependency Structures and their Formal Interpretation', 'doi': None, 'categories': 'cmp-lg cs.CL', 'abstract': \"  We describe two ``semantically-oriented'' dependency-structure formalisms,\\nU-forms and S-forms. U-forms have been previously used in machine translation\\nas interlingual representations, but without being provided with a formal\\ninterpretation. S-forms, which we introduce in this paper, are a scoped version\\nof U-forms, and we define a compositional semantics mechanism for them. Two\\ntypes of semantic composition are basic: complement incorporation and modifier\\nincorporation. Binding of variables is done at the time of incorporation,\\npermitting much flexibility in composition order and a simple account of the\\nsemantic effects of permuting several incorporations.\\n\", 'publish_date': 'Mon, 29 Apr 1996 16:00:01 GMT'}\n",
      "ID: cmp-lg/9405004, Distance: 0.34163472056388855, Content: {'title': 'Syntactic-Head-Driven Generation', 'doi': None, 'categories': 'cmp-lg cs.CL', 'abstract': \"  The previously proposed semantic-head-driven generation methods run into\\nproblems if none of the daughter constituents in the syntacto-semantic rule\\nschemata of a grammar fits the definition of a semantic head given in Shieber\\net al. 1990. This is the case for the semantic analysis rules of certain\\nconstraint-based semantic representations, e.g. Underspecified Discourse\\nRepresentation Structures (UDRSs) (Frank/Reyle 1992). Since head-driven\\ngeneration in general has its merits, we simply return to a syntactic\\ndefinition of `head' and demonstrate the feasibility of syntactic-head-driven\\ngeneration. In addition to its generality, a syntactic-head-driven algorithm\\nprovides a basis for a logically well-defined treatment of the movement of\\n(syntactic) heads, for which only ad-hoc solutions existed, so far.\\n\", 'publish_date': 'Tue, 3 May 1994 13:14:43 GMT'}\n",
      "ID: 1011.4155, Distance: 0.3422280251979828, Content: {'title': \"Motifs de graphe pour le calcul de d\\\\'ependances syntaxiques compl\\\\`etes\", 'doi': None, 'categories': 'cs.CL', 'abstract': '  This article describes a method to build syntactical dependencies starting\\nfrom the phrase structure parsing process. The goal is to obtain all the\\ninformation needed for a detailled semantical analysis. Interaction Grammars\\nare used for parsing; the saturation of polarities which is the core of this\\nformalism can be mapped to dependency relation. Formally, graph patterns are\\nused to express the set of constraints which control dependency creations.\\n', 'publish_date': 'Thu, 18 Nov 2010 08:59:55 GMT'}\n",
      "ID: cmp-lg/9808012, Distance: 0.36063557863235474, Content: {'title': 'Separating Surface Order and Syntactic Relations in a Dependency Grammar', 'doi': None, 'categories': 'cmp-lg cs.CL', 'abstract': '  This paper proposes decoupling the dependency tree from word order, such that\\nsurface ordering is not determined by traversing the dependency tree. We\\ndevelop the notion of a \\\\emph{word order domain structure}, which is linked but\\nstructurally dissimilar to the syntactic dependency tree. The proposal results\\nin a lexicalized, declarative, and formally precise description of word order;\\nfeatures which lack previous proposals for dependency grammars. Contrary to\\nother lexicalized approaches to word order, our proposal does not require\\nlexical ambiguities for ordering alternatives.\\n', 'publish_date': 'Tue, 25 Aug 1998 08:06:45 GMT'}\n",
      "ID: cmp-lg/9808009, Distance: 0.3881697356700897, Content: {'title': 'How to define a context-free backbone for DGs: Implementing a DG in the\\n  LFG formalism', 'doi': None, 'categories': 'cmp-lg cs.CL', 'abstract': \"  This paper presents a multidimensional Dependency Grammar (DG), which\\ndecouples the dependency tree from word order, such that surface ordering is\\nnot determined by traversing the dependency tree. We develop the notion of a\\n\\\\emph{word order domain structure}, which is linked but structurally dissimilar\\nto the syntactic dependency tree. We then discuss the implementation of such a\\nDG using constructs from a unification-based phrase-structure approach, namely\\nLexical-Functional Grammar (LFG). Particular attention is given to the analysis\\nof discontinuities in DG in terms of LFG's functional uncertainty.\\n\", 'publish_date': 'Fri, 21 Aug 1998 14:04:30 GMT'}\n",
      "\n",
      "Answer:\n",
      "\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "生成嵌入中: 100%|██████████| 1/1 [00:00<00:00,  9.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "cost 0.1641 seconds\n",
      "\n",
      "\n",
      "You must reply in English. You are a professional academic assistant specializing in the cs.CL domain. Strictly based on the provided literature information, answer the user's query. You may slightly expand and refine the response using existing knowledge, but avoid fabricating information. The response language must be English. Ensure the response is concise and limited to 500 words. Include a citation with the corresponding literature link (like [text](f\"https://arxiv.org/abs/{{id}}\")) and adjust the response style based on the temperature parameter (from 0.0 to 1.0). If the temperature is low (e.g., 0.2), provide a strict and precise answer. If the temperature is high (e.g., 0.8), provide a more creative and exploratory answer.\n",
      "\n",
      "Temperature: 0.2\n",
      "Question: Transformer模型的自注意力机制是否存在 “信息泄露” 风险？如何设计防护措施？\n",
      "Relevant Literature: \n",
      "ID: 2310.12462, Distance: 0.3729002773761749, Content: {'title': 'Unmasking Transformers: A Theoretical Approach to Data Recovery via\\n  Attention Weights', 'doi': None, 'categories': 'cs.LG cs.CL stat.ML', 'abstract': \"  In the realm of deep learning, transformers have emerged as a dominant\\narchitecture, particularly in natural language processing tasks. However, with\\ntheir widespread adoption, concerns regarding the security and privacy of the\\ndata processed by these models have arisen. In this paper, we address a pivotal\\nquestion: Can the data fed into transformers be recovered using their attention\\nweights and outputs? We introduce a theoretical framework to tackle this\\nproblem. Specifically, we present an algorithm that aims to recover the input\\ndata $X \\\\in \\\\mathbb{R}^{d \\\\times n}$ from given attention weights $W = QK^\\\\top\\n\\\\in \\\\mathbb{R}^{d \\\\times d}$ and output $B \\\\in \\\\mathbb{R}^{n \\\\times n}$ by\\nminimizing the loss function $L(X)$. This loss function captures the\\ndiscrepancy between the expected output and the actual output of the\\ntransformer. Our findings have significant implications for the Localized\\nLayer-wise Mechanism (LLM), suggesting potential vulnerabilities in the model's\\ndesign from a security and privacy perspective. This work underscores the\\nimportance of understanding and safeguarding the internal workings of\\ntransformers to ensure the confidentiality of processed data.\\n\", 'publish_date': 'Thu, 19 Oct 2023 04:41:01 GMT'}\n",
      "ID: 2302.07730, Distance: 0.4099262058734894, Content: {'title': 'Transformer models: an introduction and catalog', 'doi': None, 'categories': 'cs.CL', 'abstract': '  In the past few years we have seen the meteoric appearance of dozens of\\nfoundation models of the Transformer family, all of which have memorable and\\nsometimes funny, but not self-explanatory, names. The goal of this paper is to\\noffer a somewhat comprehensive but simple catalog and classification of the\\nmost popular Transformer models. The paper also includes an introduction to the\\nmost important aspects and innovations in Transformer models. Our catalog will\\ninclude models that are trained using self-supervised learning (e.g., BERT or\\nGPT3) as well as those that are further trained using a human-in-the-loop (e.g.\\nthe InstructGPT model used by ChatGPT).\\n', 'publish_date': 'Sun, 12 Feb 2023 01:26:49 GMT'}\n",
      "ID: 2009.06732, Distance: 0.41492292284965515, Content: {'title': 'Efficient Transformers: A Survey', 'doi': None, 'categories': 'cs.LG cs.AI cs.CL cs.CV cs.IR', 'abstract': '  Transformer model architectures have garnered immense interest lately due to\\ntheir effectiveness across a range of domains like language, vision and\\nreinforcement learning. In the field of natural language processing for\\nexample, Transformers have become an indispensable staple in the modern deep\\nlearning stack. Recently, a dizzying number of \"X-former\" models have been\\nproposed - Reformer, Linformer, Performer, Longformer, to name a few - which\\nimprove upon the original Transformer architecture, many of which make\\nimprovements around computational and memory efficiency. With the aim of\\nhelping the avid researcher navigate this flurry, this paper characterizes a\\nlarge and thoughtful selection of recent efficiency-flavored \"X-former\" models,\\nproviding an organized and comprehensive overview of existing work and models\\nacross multiple domains.\\n', 'publish_date': 'Mon, 14 Sep 2020 20:38:14 GMT'}\n",
      "ID: 2005.00743, Distance: 0.41934147477149963, Content: {'title': 'Synthesizer: Rethinking Self-Attention in Transformer Models', 'doi': None, 'categories': 'cs.CL cs.IR cs.LG', 'abstract': '  The dot product self-attention is known to be central and indispensable to\\nstate-of-the-art Transformer models. But is it really required? This paper\\ninvestigates the true importance and contribution of the dot product-based\\nself-attention mechanism on the performance of Transformer models. Via\\nextensive experiments, we find that (1) random alignment matrices surprisingly\\nperform quite competitively and (2) learning attention weights from token-token\\n(query-key) interactions is useful but not that important after all. To this\\nend, we propose \\\\textsc{Synthesizer}, a model that learns synthetic attention\\nweights without token-token interactions. In our experiments, we first show\\nthat simple Synthesizers achieve highly competitive performance when compared\\nagainst vanilla Transformer models across a range of tasks, including machine\\ntranslation, language modeling, text generation and GLUE/SuperGLUE benchmarks.\\nWhen composed with dot product attention, we find that Synthesizers\\nconsistently outperform Transformers. Moreover, we conduct additional\\ncomparisons of Synthesizers against Dynamic Convolutions, showing that simple\\nRandom Synthesizer is not only $60\\\\%$ faster but also improves perplexity by a\\nrelative $3.5\\\\%$. Finally, we show that simple factorized Synthesizers can\\noutperform Linformers on encoding only tasks.\\n', 'publish_date': 'Sat, 2 May 2020 08:16:19 GMT'}\n",
      "ID: 2210.05794, Distance: 0.4230707585811615, Content: {'title': 'Designing Robust Transformers using Robust Kernel Density Estimation', 'doi': None, 'categories': 'cs.LG cs.CL cs.CV', 'abstract': '  Recent advances in Transformer architectures have empowered their empirical\\nsuccess in a variety of tasks across different domains. However, existing works\\nmainly focus on predictive accuracy and computational cost, without considering\\nother practical issues, such as robustness to contaminated samples. Recent work\\nby Nguyen et al., (2022) has shown that the self-attention mechanism, which is\\nthe center of the Transformer architecture, can be viewed as a non-parametric\\nestimator based on kernel density estimation (KDE). This motivates us to\\nleverage a set of robust kernel density estimation methods for alleviating the\\nissue of data contamination. Specifically, we introduce a series of\\nself-attention mechanisms that can be incorporated into different Transformer\\narchitectures and discuss the special properties of each method. We then\\nperform extensive empirical studies on language modeling and image\\nclassification tasks. Our methods demonstrate robust performance in multiple\\nscenarios while maintaining competitive results on clean datasets.\\n', 'publish_date': 'Tue, 11 Oct 2022 21:39:52 GMT'}\n",
      "\n",
      "Answer:\n",
      "\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "生成嵌入中: 100%|██████████| 1/1 [00:00<00:00, 10.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "cost 0.1498 seconds\n",
      "\n",
      "\n",
      "You must reply in English. You are a professional academic assistant specializing in the cs.CL domain. Strictly based on the provided literature information, answer the user's query. You may slightly expand and refine the response using existing knowledge, but avoid fabricating information. The response language must be English. Ensure the response is concise and limited to 500 words. Include a citation with the corresponding literature link (like [text](f\"https://arxiv.org/abs/{{id}}\")) and adjust the response style based on the temperature parameter (from 0.0 to 1.0). If the temperature is low (e.g., 0.2), provide a strict and precise answer. If the temperature is high (e.g., 0.8), provide a more creative and exploratory answer.\n",
      "\n",
      "Temperature: 0.2\n",
      "Question: Transformer上的文本分类和基于CNN的图片分类遇到的问题及解决方案的异同？\n",
      "Relevant Literature: \n",
      "ID: 1804.00968, Distance: 0.33558589220046997, Content: {'title': 'In-depth Question classification using Convolutional Neural Networks', 'doi': None, 'categories': 'cs.CL', 'abstract': '  Convolutional neural networks for computer vision are fairly intuitive. In a\\ntypical CNN used in image classification, the first layers learn edges, and the\\nfollowing layers learn some filters that can identify an object. But CNNs for\\nNatural Language Processing are not used often and are not completely\\nintuitive. We have a good idea about what the convolution filters learn for the\\ntask of text classification, and to that, we propose a neural network structure\\nthat will be able to give good results in less time. We will be using\\nconvolutional neural networks to predict the primary or broader topic of a\\nquestion, and then use separate networks for each of these predicted topics to\\naccurately classify their sub-topics.\\n', 'publish_date': 'Sat, 31 Mar 2018 19:52:26 GMT'}\n",
      "ID: 1911.04115, Distance: 0.37048929929733276, Content: {'title': 'Text classification with pixel embedding', 'doi': None, 'categories': 'cs.CL', 'abstract': \"  We propose a novel framework to understand the text by converting sentences\\nor articles into video-like 3-dimensional tensors. Each frame, corresponding to\\na slice of the tensor, is a word image that is rendered by the word's shape.\\nThe length of the tensor equals to the number of words in the sentence or\\narticle. The proposed transformation from the text to a 3-dimensional tensor\\nmakes it very convenient to implement an $n$-gram model with convolutional\\nneural networks for text analysis. Concretely, we impose a 3-dimensional\\nconvolutional kernel on the 3-dimensional text tensor. The first two dimensions\\nof the convolutional kernel size equal the size of the word image and the last\\ndimension of the kernel size is $n$. That is, every time when we slide the\\n3-dimensional kernel over a word sequence, the convolution covers $n$ word\\nimages and outputs a scalar. By iterating this process continuously for each\\n$n$-gram along with the sentence or article with multiple kernels, we obtain a\\n2-dimensional feature map. A subsequent 1-dimensional max-over-time pooling is\\napplied to this feature map, and three fully-connected layers are used for\\nconducting text classification finally. Experiments of several text\\nclassification datasets demonstrate surprisingly superior performances using\\nthe proposed model in comparison with existing methods.\\n\", 'publish_date': 'Mon, 11 Nov 2019 07:28:25 GMT'}\n",
      "ID: 1606.01781, Distance: 0.3714621067047119, Content: {'title': 'Very Deep Convolutional Networks for Text Classification', 'doi': None, 'categories': 'cs.CL cs.LG cs.NE', 'abstract': '  The dominant approach for many NLP tasks are recurrent neural networks, in\\nparticular LSTMs, and convolutional neural networks. However, these\\narchitectures are rather shallow in comparison to the deep convolutional\\nnetworks which have pushed the state-of-the-art in computer vision. We present\\na new architecture (VDCNN) for text processing which operates directly at the\\ncharacter level and uses only small convolutions and pooling operations. We are\\nable to show that the performance of this model increases with depth: using up\\nto 29 convolutional layers, we report improvements over the state-of-the-art on\\nseveral public text classification tasks. To the best of our knowledge, this is\\nthe first time that very deep convolutional nets have been applied to text\\nprocessing.\\n', 'publish_date': 'Mon, 6 Jun 2016 15:14:50 GMT'}\n",
      "ID: 1809.08037, Distance: 0.37699344754219055, Content: {'title': 'Understanding Convolutional Neural Networks for Text Classification', 'doi': None, 'categories': 'cs.CL', 'abstract': '  We present an analysis into the inner workings of Convolutional Neural\\nNetworks (CNNs) for processing text. CNNs used for computer vision can be\\ninterpreted by projecting filters into image space, but for discrete sequence\\ninputs CNNs remain a mystery. We aim to understand the method by which the\\nnetworks process and classify text. We examine common hypotheses to this\\nproblem: that filters, accompanied by global max-pooling, serve as ngram\\ndetectors. We show that filters may capture several different semantic classes\\nof ngrams by using different activation patterns, and that global max-pooling\\ninduces behavior which separates important ngrams from the rest. Finally, we\\nshow practical use cases derived from our findings in the form of model\\ninterpretability (explaining a trained model by deriving a concrete identity\\nfor each filter, bridging the gap between visualization tools in vision tasks\\nand NLP) and prediction interpretability (explaining predictions). Code\\nimplementation is available online at\\ngithub.com/sayaendo/interpreting-cnn-for-text.\\n', 'publish_date': 'Fri, 21 Sep 2018 11:03:48 GMT'}\n",
      "ID: 1509.01626, Distance: 0.3779098391532898, Content: {'title': 'Character-level Convolutional Networks for Text Classification', 'doi': None, 'categories': 'cs.LG cs.CL', 'abstract': '  This article offers an empirical exploration on the use of character-level\\nconvolutional networks (ConvNets) for text classification. We constructed\\nseveral large-scale datasets to show that character-level convolutional\\nnetworks could achieve state-of-the-art or competitive results. Comparisons are\\noffered against traditional models such as bag of words, n-grams and their\\nTFIDF variants, and deep learning models such as word-based ConvNets and\\nrecurrent neural networks.\\n', 'publish_date': 'Fri, 4 Sep 2015 22:31:53 GMT'}\n",
      "\n",
      "Answer:\n",
      "\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "生成嵌入中: 100%|██████████| 1/1 [00:00<00:00,  9.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "cost 0.1743 seconds\n",
      "\n",
      "\n",
      "You must reply in English. You are a professional academic assistant specializing in the cs.CL domain. Strictly based on the provided literature information, answer the user's query. You may slightly expand and refine the response using existing knowledge, but avoid fabricating information. The response language must be English. Ensure the response is concise and limited to 500 words. Include a citation with the corresponding literature link (like [text](f\"https://arxiv.org/abs/{{id}}\")) and adjust the response style based on the temperature parameter (from 0.0 to 1.0). If the temperature is low (e.g., 0.2), provide a strict and precise answer. If the temperature is high (e.g., 0.8), provide a more creative and exploratory answer.\n",
      "\n",
      "Temperature: 0.2\n",
      "Question: 什么是对抗攻击，常用黑盒文本对抗攻击手段有哪些？\n",
      "Relevant Literature: \n",
      "ID: 2201.08555, Distance: 0.2739735245704651, Content: {'title': 'Identifying Adversarial Attacks on Text Classifiers', 'doi': None, 'categories': 'cs.CL cs.CR cs.LG', 'abstract': '  The landscape of adversarial attacks against text classifiers continues to\\ngrow, with new attacks developed every year and many of them available in\\nstandard toolkits, such as TextAttack and OpenAttack. In response, there is a\\ngrowing body of work on robust learning, which reduces vulnerability to these\\nattacks, though sometimes at a high cost in compute time or accuracy. In this\\npaper, we take an alternate approach -- we attempt to understand the attacker\\nby analyzing adversarial text to determine which methods were used to create\\nit. Our first contribution is an extensive dataset for attack detection and\\nlabeling: 1.5~million attack instances, generated by twelve adversarial attacks\\ntargeting three classifiers trained on six source datasets for sentiment\\nanalysis and abuse detection in English. As our second contribution, we use\\nthis dataset to develop and benchmark a number of classifiers for attack\\nidentification -- determining if a given text has been adversarially\\nmanipulated and by which attack. As a third contribution, we demonstrate the\\neffectiveness of three classes of features for these tasks: text properties,\\ncapturing content and presentation of text; language model properties,\\ndetermining which tokens are more or less probable throughout the input; and\\ntarget model properties, representing how the text classifier is influenced by\\nthe attack, including internal node activations. Overall, this represents a\\nfirst step towards forensics for adversarial attacks against text classifiers.\\n', 'publish_date': 'Fri, 21 Jan 2022 06:16:04 GMT'}\n",
      "ID: 2104.13484, Distance: 0.28071328997612, Content: {'title': 'Improved and Efficient Text Adversarial Attacks using Target Information', 'doi': None, 'categories': 'cs.LG cs.AI cs.CL cs.CR', 'abstract': '  There has been recently a growing interest in studying adversarial examples\\non natural language models in the black-box setting. These methods attack\\nnatural language classifiers by perturbing certain important words until the\\nclassifier label is changed. In order to find these important words, these\\nmethods rank all words by importance by querying the target model word by word\\nfor each input sentence, resulting in high query inefficiency. A new\\ninteresting approach was introduced that addresses this problem through\\ninterpretable learning to learn the word ranking instead of previous expensive\\nsearch. The main advantage of using this approach is that it achieves\\ncomparable attack rates to the state-of-the-art methods, yet faster and with\\nfewer queries, where fewer queries are desirable to avoid suspicion towards the\\nattacking agent. Nonetheless, this approach sacrificed the useful information\\nthat could be leveraged from the target classifier for that sake of query\\nefficiency. In this paper we study the effect of leveraging the target model\\noutputs and data on both attack rates and average number of queries, and we\\nshow that both can be improved, with a limited overhead of additional queries.\\n', 'publish_date': 'Tue, 27 Apr 2021 21:25:55 GMT'}\n",
      "ID: 2206.05015, Distance: 0.2830166816711426, Content: {'title': 'A Simple Yet Efficient Method for Adversarial Word-Substitute Attack', 'doi': None, 'categories': 'cs.CL cs.AI', 'abstract': '  NLP researchers propose different word-substitute black-box attacks that can\\nfool text classification models. In such attack, an adversary keeps sending\\ncrafted adversarial queries to the target model until it can successfully\\nachieve the intended outcome. State-of-the-art attack methods usually require\\nhundreds or thousands of queries to find one adversarial example. In this\\npaper, we study whether a sophisticated adversary can attack the system with\\nmuch less queries. We propose a simple yet efficient method that can reduce the\\naverage number of adversarial queries by 3-30 times and maintain the attack\\neffectiveness. This research highlights that an adversary can fool a deep NLP\\nmodel with much less cost.\\n', 'publish_date': 'Sat, 7 May 2022 14:20:57 GMT'}\n",
      "ID: 1909.07873, Distance: 0.28530609607696533, Content: {'title': 'Generating Black-Box Adversarial Examples for Text Classifiers Using a\\n  Deep Reinforced Model', 'doi': '10.1007/978-3-030-46147-8_43', 'categories': 'cs.LG cs.CL cs.IR stat.ML', 'abstract': \"  Recently, generating adversarial examples has become an important means of\\nmeasuring robustness of a deep learning model. Adversarial examples help us\\nidentify the susceptibilities of the model and further counter those\\nvulnerabilities by applying adversarial training techniques. In natural\\nlanguage domain, small perturbations in the form of misspellings or paraphrases\\ncan drastically change the semantics of the text. We propose a reinforcement\\nlearning based approach towards generating adversarial examples in black-box\\nsettings. We demonstrate that our method is able to fool well-trained models\\nfor (a) IMDB sentiment classification task and (b) AG's news corpus news\\ncategorization task with significantly high success rates. We find that the\\nadversarial examples generated are semantics-preserving perturbations to the\\noriginal text.\\n\", 'publish_date': 'Tue, 17 Sep 2019 15:05:31 GMT'}\n",
      "ID: 2405.03789, Distance: 0.2896369695663452, Content: {'title': 'On Adversarial Examples for Text Classification by Perturbing Latent\\n  Representations', 'doi': None, 'categories': 'cs.LG cs.AI cs.CL cs.CR', 'abstract': '  Recently, with the advancement of deep learning, several applications in text\\nclassification have advanced significantly. However, this improvement comes\\nwith a cost because deep learning is vulnerable to adversarial examples. This\\nweakness indicates that deep learning is not very robust. Fortunately, the\\ninput of a text classifier is discrete. Hence, it can prevent the classifier\\nfrom state-of-the-art attacks. Nonetheless, previous works have generated\\nblack-box attacks that successfully manipulate the discrete values of the input\\nto find adversarial examples. Therefore, instead of changing the discrete\\nvalues, we transform the input into its embedding vector containing real values\\nto perform the state-of-the-art white-box attacks. Then, we convert the\\nperturbed embedding vector back into a text and name it an adversarial example.\\nIn summary, we create a framework that measures the robustness of a text\\nclassifier by using the gradients of the classifier.\\n', 'publish_date': 'Mon, 6 May 2024 18:45:18 GMT'}\n",
      "\n",
      "Answer:\n",
      "\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "生成嵌入中: 100%|██████████| 1/1 [00:00<00:00, 13.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "cost 0.1359 seconds\n",
      "\n",
      "\n",
      "You must reply in English. You are a professional academic assistant specializing in the cs.CL domain. Strictly based on the provided literature information, answer the user's query. You may slightly expand and refine the response using existing knowledge, but avoid fabricating information. The response language must be English. Ensure the response is concise and limited to 500 words. Include a citation with the corresponding literature link (like [text](f\"https://arxiv.org/abs/{{id}}\")) and adjust the response style based on the temperature parameter (from 0.0 to 1.0). If the temperature is low (e.g., 0.2), provide a strict and precise answer. If the temperature is high (e.g., 0.8), provide a more creative and exploratory answer.\n",
      "\n",
      "Temperature: 0.2\n",
      "Question: 什么是对抗攻击，常用白盒文本对抗攻击手段有哪些？\n",
      "Relevant Literature: \n",
      "ID: 2008.05536, Distance: 0.3000302314758301, Content: {'title': 'Model Robustness with Text Classification: Semantic-preserving\\n  adversarial attacks', 'doi': None, 'categories': 'cs.CL cs.LG', 'abstract': '  We propose algorithms to create adversarial attacks to assess model\\nrobustness in text classification problems. They can be used to create white\\nbox attacks and black box attacks while at the same time preserving the\\nsemantics and syntax of the original text. The attacks cause significant number\\nof flips in white-box setting and same rule based can be used in black-box\\nsetting. In a black-box setting, the attacks created are able to reverse\\ndecisions of transformer based architectures.\\n', 'publish_date': 'Wed, 12 Aug 2020 19:17:46 GMT'}\n",
      "ID: 2201.08555, Distance: 0.30486223101615906, Content: {'title': 'Identifying Adversarial Attacks on Text Classifiers', 'doi': None, 'categories': 'cs.CL cs.CR cs.LG', 'abstract': '  The landscape of adversarial attacks against text classifiers continues to\\ngrow, with new attacks developed every year and many of them available in\\nstandard toolkits, such as TextAttack and OpenAttack. In response, there is a\\ngrowing body of work on robust learning, which reduces vulnerability to these\\nattacks, though sometimes at a high cost in compute time or accuracy. In this\\npaper, we take an alternate approach -- we attempt to understand the attacker\\nby analyzing adversarial text to determine which methods were used to create\\nit. Our first contribution is an extensive dataset for attack detection and\\nlabeling: 1.5~million attack instances, generated by twelve adversarial attacks\\ntargeting three classifiers trained on six source datasets for sentiment\\nanalysis and abuse detection in English. As our second contribution, we use\\nthis dataset to develop and benchmark a number of classifiers for attack\\nidentification -- determining if a given text has been adversarially\\nmanipulated and by which attack. As a third contribution, we demonstrate the\\neffectiveness of three classes of features for these tasks: text properties,\\ncapturing content and presentation of text; language model properties,\\ndetermining which tokens are more or less probable throughout the input; and\\ntarget model properties, representing how the text classifier is influenced by\\nthe attack, including internal node activations. Overall, this represents a\\nfirst step towards forensics for adversarial attacks against text classifiers.\\n', 'publish_date': 'Fri, 21 Jan 2022 06:16:04 GMT'}\n",
      "ID: 2405.03789, Distance: 0.30765044689178467, Content: {'title': 'On Adversarial Examples for Text Classification by Perturbing Latent\\n  Representations', 'doi': None, 'categories': 'cs.LG cs.AI cs.CL cs.CR', 'abstract': '  Recently, with the advancement of deep learning, several applications in text\\nclassification have advanced significantly. However, this improvement comes\\nwith a cost because deep learning is vulnerable to adversarial examples. This\\nweakness indicates that deep learning is not very robust. Fortunately, the\\ninput of a text classifier is discrete. Hence, it can prevent the classifier\\nfrom state-of-the-art attacks. Nonetheless, previous works have generated\\nblack-box attacks that successfully manipulate the discrete values of the input\\nto find adversarial examples. Therefore, instead of changing the discrete\\nvalues, we transform the input into its embedding vector containing real values\\nto perform the state-of-the-art white-box attacks. Then, we convert the\\nperturbed embedding vector back into a text and name it an adversarial example.\\nIn summary, we create a framework that measures the robustness of a text\\nclassifier by using the gradients of the classifier.\\n', 'publish_date': 'Mon, 6 May 2024 18:45:18 GMT'}\n",
      "ID: 2104.13484, Distance: 0.31862643361091614, Content: {'title': 'Improved and Efficient Text Adversarial Attacks using Target Information', 'doi': None, 'categories': 'cs.LG cs.AI cs.CL cs.CR', 'abstract': '  There has been recently a growing interest in studying adversarial examples\\non natural language models in the black-box setting. These methods attack\\nnatural language classifiers by perturbing certain important words until the\\nclassifier label is changed. In order to find these important words, these\\nmethods rank all words by importance by querying the target model word by word\\nfor each input sentence, resulting in high query inefficiency. A new\\ninteresting approach was introduced that addresses this problem through\\ninterpretable learning to learn the word ranking instead of previous expensive\\nsearch. The main advantage of using this approach is that it achieves\\ncomparable attack rates to the state-of-the-art methods, yet faster and with\\nfewer queries, where fewer queries are desirable to avoid suspicion towards the\\nattacking agent. Nonetheless, this approach sacrificed the useful information\\nthat could be leveraged from the target classifier for that sake of query\\nefficiency. In this paper we study the effect of leveraging the target model\\noutputs and data on both attack rates and average number of queries, and we\\nshow that both can be improved, with a limited overhead of additional queries.\\n', 'publish_date': 'Tue, 27 Apr 2021 21:25:55 GMT'}\n",
      "ID: 2211.06571, Distance: 0.3199475109577179, Content: {'title': 'Generating Textual Adversaries with Minimal Perturbation', 'doi': None, 'categories': 'cs.CL', 'abstract': '  Many word-level adversarial attack approaches for textual data have been\\nproposed in recent studies. However, due to the massive search space consisting\\nof combinations of candidate words, the existing approaches face the problem of\\npreserving the semantics of texts when crafting adversarial counterparts. In\\nthis paper, we develop a novel attack strategy to find adversarial texts with\\nhigh similarity to the original texts while introducing minimal perturbation.\\nThe rationale is that we expect the adversarial texts with small perturbation\\ncan better preserve the semantic meaning of original texts. Experiments show\\nthat, compared with state-of-the-art attack approaches, our approach achieves\\nhigher success rates and lower perturbation rates in four benchmark datasets.\\n', 'publish_date': 'Sat, 12 Nov 2022 04:46:07 GMT'}\n",
      "\n",
      "Answer:\n",
      "\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "生成嵌入中: 100%|██████████| 1/1 [00:00<00:00, 11.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "cost 0.1478 seconds\n",
      "\n",
      "\n",
      "You must reply in English. You are a professional academic assistant specializing in the cs.CL domain. Strictly based on the provided literature information, answer the user's query. You may slightly expand and refine the response using existing knowledge, but avoid fabricating information. The response language must be English. Ensure the response is concise and limited to 500 words. Include a citation with the corresponding literature link (like [text](f\"https://arxiv.org/abs/{{id}}\")) and adjust the response style based on the temperature parameter (from 0.0 to 1.0). If the temperature is low (e.g., 0.2), provide a strict and precise answer. If the temperature is high (e.g., 0.8), provide a more creative and exploratory answer.\n",
      "\n",
      "Temperature: 0.2\n",
      "Question: FGSM对抗攻击方法是否能以及如何迁移到文本对抗攻击领域？\n",
      "Relevant Literature: \n",
      "ID: 2008.03709, Distance: 0.343606173992157, Content: {'title': 'Adversarial Training with Fast Gradient Projection Method against\\n  Synonym Substitution based Text Attacks', 'doi': None, 'categories': 'cs.CL cs.LG', 'abstract': '  Adversarial training is the most empirically successful approach in improving\\nthe robustness of deep neural networks for image classification.For text\\nclassification, however, existing synonym substitution based adversarial\\nattacks are effective but not efficient to be incorporated into practical text\\nadversarial training. Gradient-based attacks, which are very efficient for\\nimages, are hard to be implemented for synonym substitution based text attacks\\ndue to the lexical, grammatical and semantic constraints and the discrete text\\ninput space. Thereby, we propose a fast text adversarial attack method called\\nFast Gradient Projection Method (FGPM) based on synonym substitution, which is\\nabout 20 times faster than existing text attack methods and could achieve\\nsimilar attack performance. We then incorporate FGPM with adversarial training\\nand propose a text defense method called Adversarial Training with FGPM\\nenhanced by Logit pairing (ATFL). Experiments show that ATFL could\\nsignificantly improve the model robustness and block the transferability of\\nadversarial examples.\\n', 'publish_date': 'Sun, 9 Aug 2020 11:02:06 GMT'}\n",
      "ID: 2201.08555, Distance: 0.3629932403564453, Content: {'title': 'Identifying Adversarial Attacks on Text Classifiers', 'doi': None, 'categories': 'cs.CL cs.CR cs.LG', 'abstract': '  The landscape of adversarial attacks against text classifiers continues to\\ngrow, with new attacks developed every year and many of them available in\\nstandard toolkits, such as TextAttack and OpenAttack. In response, there is a\\ngrowing body of work on robust learning, which reduces vulnerability to these\\nattacks, though sometimes at a high cost in compute time or accuracy. In this\\npaper, we take an alternate approach -- we attempt to understand the attacker\\nby analyzing adversarial text to determine which methods were used to create\\nit. Our first contribution is an extensive dataset for attack detection and\\nlabeling: 1.5~million attack instances, generated by twelve adversarial attacks\\ntargeting three classifiers trained on six source datasets for sentiment\\nanalysis and abuse detection in English. As our second contribution, we use\\nthis dataset to develop and benchmark a number of classifiers for attack\\nidentification -- determining if a given text has been adversarially\\nmanipulated and by which attack. As a third contribution, we demonstrate the\\neffectiveness of three classes of features for these tasks: text properties,\\ncapturing content and presentation of text; language model properties,\\ndetermining which tokens are more or less probable throughout the input; and\\ntarget model properties, representing how the text classifier is influenced by\\nthe attack, including internal node activations. Overall, this represents a\\nfirst step towards forensics for adversarial attacks against text classifiers.\\n', 'publish_date': 'Fri, 21 Jan 2022 06:16:04 GMT'}\n",
      "ID: 2011.08558, Distance: 0.3702811300754547, Content: {'title': 'On the Transferability of Adversarial Attacksagainst Neural Text\\n  Classifier', 'doi': None, 'categories': 'cs.LG cs.CL', 'abstract': '  Deep neural networks are vulnerable to adversarial attacks, where a small\\nperturbation to an input alters the model prediction. In many cases, malicious\\ninputs intentionally crafted for one model can fool another model. In this\\npaper, we present the first study to systematically investigate the\\ntransferability of adversarial examples for text classification models and\\nexplore how various factors, including network architecture, tokenization\\nscheme, word embedding, and model capacity, affect the transferability of\\nadversarial examples. Based on these studies, we propose a genetic algorithm to\\nfind an ensemble of models that can be used to induce adversarial examples to\\nfool almost all existing models. Such adversarial examples reflect the defects\\nof the learning process and the data bias in the training set. Finally, we\\nderive word replacement rules that can be used for model diagnostics from these\\nadversarial examples.\\n', 'publish_date': 'Tue, 17 Nov 2020 10:45:05 GMT'}\n",
      "ID: 1801.07175, Distance: 0.37107425928115845, Content: {'title': 'Adversarial Texts with Gradient Methods', 'doi': None, 'categories': 'cs.CL cs.CR cs.LG', 'abstract': \"  Adversarial samples for images have been extensively studied in the\\nliterature. Among many of the attacking methods, gradient-based methods are\\nboth effective and easy to compute. In this work, we propose a framework to\\nadapt the gradient attacking methods on images to text domain. The main\\ndifficulties for generating adversarial texts with gradient methods are i) the\\ninput space is discrete, which makes it difficult to accumulate small noise\\ndirectly in the inputs, and ii) the measurement of the quality of the\\nadversarial texts is difficult. We tackle the first problem by searching for\\nadversarials in the embedding space and then reconstruct the adversarial texts\\nvia nearest neighbor search. For the latter problem, we employ the Word Mover's\\nDistance (WMD) to quantify the quality of adversarial texts. Through extensive\\nexperiments on three datasets, IMDB movie reviews, Reuters-2 and Reuters-5\\nnewswires, we show that our framework can leverage gradient attacking methods\\nto generate very high-quality adversarial texts that are only a few words\\ndifferent from the original texts. There are many cases where we can change one\\nword to alter the label of the whole piece of text. We successfully incorporate\\nFGM and DeepFool into our framework. In addition, we empirically show that WMD\\nis closely related to the quality of adversarial texts.\\n\", 'publish_date': 'Mon, 22 Jan 2018 16:19:52 GMT'}\n",
      "ID: 2211.06571, Distance: 0.3799896836280823, Content: {'title': 'Generating Textual Adversaries with Minimal Perturbation', 'doi': None, 'categories': 'cs.CL', 'abstract': '  Many word-level adversarial attack approaches for textual data have been\\nproposed in recent studies. However, due to the massive search space consisting\\nof combinations of candidate words, the existing approaches face the problem of\\npreserving the semantics of texts when crafting adversarial counterparts. In\\nthis paper, we develop a novel attack strategy to find adversarial texts with\\nhigh similarity to the original texts while introducing minimal perturbation.\\nThe rationale is that we expect the adversarial texts with small perturbation\\ncan better preserve the semantic meaning of original texts. Experiments show\\nthat, compared with state-of-the-art attack approaches, our approach achieves\\nhigher success rates and lower perturbation rates in four benchmark datasets.\\n', 'publish_date': 'Sat, 12 Nov 2022 04:46:07 GMT'}\n",
      "\n",
      "Answer:\n",
      "\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "生成嵌入中: 100%|██████████| 1/1 [00:00<00:00, 13.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "cost 0.1420 seconds\n",
      "\n",
      "\n",
      "You must reply in English. You are a professional academic assistant specializing in the cs.CL domain. Strictly based on the provided literature information, answer the user's query. You may slightly expand and refine the response using existing knowledge, but avoid fabricating information. The response language must be English. Ensure the response is concise and limited to 500 words. Include a citation with the corresponding literature link (like [text](f\"https://arxiv.org/abs/{{id}}\")) and adjust the response style based on the temperature parameter (from 0.0 to 1.0). If the temperature is low (e.g., 0.2), provide a strict and precise answer. If the temperature is high (e.g., 0.8), provide a more creative and exploratory answer.\n",
      "\n",
      "Temperature: 0.2\n",
      "Question: 解释一下如何使用BERT生成黑盒对抗攻击样本\n",
      "Relevant Literature: \n",
      "ID: 2004.09984, Distance: 0.3013704717159271, Content: {'title': 'BERT-ATTACK: Adversarial Attack Against BERT Using BERT', 'doi': None, 'categories': 'cs.CL', 'abstract': '  Adversarial attacks for discrete data (such as texts) have been proved\\nsignificantly more challenging than continuous data (such as images) since it\\nis difficult to generate adversarial samples with gradient-based methods.\\nCurrent successful attack methods for texts usually adopt heuristic replacement\\nstrategies on the character or word level, which remains challenging to find\\nthe optimal solution in the massive space of possible combinations of\\nreplacements while preserving semantic consistency and language fluency. In\\nthis paper, we propose \\\\textbf{BERT-Attack}, a high-quality and effective\\nmethod to generate adversarial samples using pre-trained masked language models\\nexemplified by BERT. We turn BERT against its fine-tuned models and other deep\\nneural models in downstream tasks so that we can successfully mislead the\\ntarget models to predict incorrectly. Our method outperforms state-of-the-art\\nattack strategies in both success rate and perturb percentage, while the\\ngenerated adversarial samples are fluent and semantically preserved. Also, the\\ncost of calculation is low, thus possible for large-scale generations. The code\\nis available at https://github.com/LinyangLee/BERT-Attack.\\n', 'publish_date': 'Tue, 21 Apr 2020 13:30:02 GMT'}\n",
      "ID: 2003.04985, Distance: 0.338335245847702, Content: {'title': 'Adv-BERT: BERT is not robust on misspellings! Generating nature\\n  adversarial samples on BERT', 'doi': None, 'categories': 'cs.CL', 'abstract': '  There is an increasing amount of literature that claims the brittleness of\\ndeep neural networks in dealing with adversarial examples that are created\\nmaliciously. It is unclear, however, how the models will perform in realistic\\nscenarios where \\\\textit{natural rather than malicious} adversarial instances\\noften exist. This work systematically explores the robustness of BERT, the\\nstate-of-the-art Transformer-style model in NLP, in dealing with noisy data,\\nparticularly mistakes in typing the keyboard, that occur inadvertently.\\nIntensive experiments on sentiment analysis and question answering benchmarks\\nindicate that: (i) Typos in various words of a sentence do not influence\\nequally. The typos in informative words make severer damages; (ii) Mistype is\\nthe most damaging factor, compared with inserting, deleting, etc.; (iii) Humans\\nand machines have different focuses on recognizing adversarial attacks.\\n', 'publish_date': 'Thu, 27 Feb 2020 22:07:11 GMT'}\n",
      "ID: 2008.04203, Distance: 0.33911171555519104, Content: {'title': 'FireBERT: Hardening BERT-based classifiers against adversarial attack', 'doi': None, 'categories': 'cs.CL cs.LG', 'abstract': '  We present FireBERT, a set of three proof-of-concept NLP classifiers hardened\\nagainst TextFooler-style word-perturbation by producing diverse alternatives to\\noriginal samples. In one approach, we co-tune BERT against the training data\\nand synthetic adversarial samples. In a second approach, we generate the\\nsynthetic samples at evaluation time through substitution of words and\\nperturbation of embedding vectors. The diversified evaluation results are then\\ncombined by voting. A third approach replaces evaluation-time word substitution\\nwith perturbation of embedding vectors. We evaluate FireBERT for MNLI and IMDB\\nMovie Review datasets, in the original and on adversarial examples generated by\\nTextFooler. We also test whether TextFooler is less successful in creating new\\nadversarial samples when manipulating FireBERT, compared to working on\\nunhardened classifiers. We show that it is possible to improve the accuracy of\\nBERT-based models in the face of adversarial attacks without significantly\\nreducing the accuracy for regular benchmark samples. We present co-tuning with\\na synthetic data generator as a highly effective method to protect against 95%\\nof pre-manufactured adversarial samples while maintaining 98% of original\\nbenchmark performance. We also demonstrate evaluation-time perturbation as a\\npromising direction for further research, restoring accuracy up to 75% of\\nbenchmark performance for pre-made adversarials, and up to 65% (from a baseline\\nof 75% orig. / 12% attack) under active attack by TextFooler.\\n', 'publish_date': 'Mon, 10 Aug 2020 15:43:28 GMT'}\n",
      "ID: 2109.07403, Distance: 0.3467260003089905, Content: {'title': 'BERT is Robust! A Case Against Synonym-Based Adversarial Examples in\\n  Text Classification', 'doi': None, 'categories': 'cs.CL', 'abstract': '  Deep Neural Networks have taken Natural Language Processing by storm. While\\nthis led to incredible improvements across many tasks, it also initiated a new\\nresearch field, questioning the robustness of these neural networks by\\nattacking them. In this paper, we investigate four word substitution-based\\nattacks on BERT. We combine a human evaluation of individual word substitutions\\nand a probabilistic analysis to show that between 96% and 99% of the analyzed\\nattacks do not preserve semantics, indicating that their success is mainly\\nbased on feeding poor data to the model. To further confirm that, we introduce\\nan efficient data augmentation procedure and show that many adversarial\\nexamples can be prevented by including data similar to the attacks during\\ntraining. An additional post-processing step reduces the success rates of\\nstate-of-the-art attacks below 5%. Finally, by looking at more reasonable\\nthresholds on constraints for word substitutions, we conclude that BERT is a\\nlot more robust than research on attacks suggests.\\n', 'publish_date': 'Wed, 15 Sep 2021 16:15:16 GMT'}\n",
      "ID: 2103.10013, Distance: 0.35214006900787354, Content: {'title': 'Model Extraction and Adversarial Transferability, Your BERT is\\n  Vulnerable!', 'doi': None, 'categories': 'cs.CL', 'abstract': '  Natural language processing (NLP) tasks, ranging from text classification to\\ntext generation, have been revolutionised by the pre-trained language models,\\nsuch as BERT. This allows corporations to easily build powerful APIs by\\nencapsulating fine-tuned BERT models for downstream tasks. However, when a\\nfine-tuned BERT model is deployed as a service, it may suffer from different\\nattacks launched by malicious users. In this work, we first present how an\\nadversary can steal a BERT-based API service (the victim/target model) on\\nmultiple benchmark datasets with limited prior knowledge and queries. We\\nfurther show that the extracted model can lead to highly transferable\\nadversarial attacks against the victim model. Our studies indicate that the\\npotential vulnerabilities of BERT-based API services still hold, even when\\nthere is an architectural mismatch between the victim model and the attack\\nmodel. Finally, we investigate two defence strategies to protect the victim\\nmodel and find that unless the performance of the victim model is sacrificed,\\nboth model ex-traction and adversarial transferability can effectively\\ncompromise the target models\\n', 'publish_date': 'Thu, 18 Mar 2021 04:23:21 GMT'}\n",
      "\n",
      "Answer:\n",
      "\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "生成嵌入中: 100%|██████████| 1/1 [00:00<00:00, 12.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "cost 0.1418 seconds\n",
      "\n",
      "\n",
      "You must reply in English. You are a professional academic assistant specializing in the cs.CL domain. Strictly based on the provided literature information, answer the user's query. You may slightly expand and refine the response using existing knowledge, but avoid fabricating information. The response language must be English. Ensure the response is concise and limited to 500 words. Include a citation with the corresponding literature link (like [text](f\"https://arxiv.org/abs/{{id}}\")) and adjust the response style based on the temperature parameter (from 0.0 to 1.0). If the temperature is low (e.g., 0.2), provide a strict and precise answer. If the temperature is high (e.g., 0.8), provide a more creative and exploratory answer.\n",
      "\n",
      "Temperature: 0.2\n",
      "Question: 低资源语言的形态丰富性（如黏着语）如何影响少样本词义消歧？基于类型学的迁移学习是否有效？\n",
      "Relevant Literature: \n",
      "ID: 1909.12375, Distance: 0.28734734654426575, Content: {'title': 'On the Importance of Subword Information for Morphological Tasks in\\n  Truly Low-Resource Languages', 'doi': None, 'categories': 'cs.CL', 'abstract': '  Recent work has validated the importance of subword information for word\\nrepresentation learning. Since subwords increase parameter sharing ability in\\nneural models, their value should be even more pronounced in low-data regimes.\\nIn this work, we therefore provide a comprehensive analysis focused on the\\nusefulness of subwords for word representation learning in truly low-resource\\nscenarios and for three representative morphological tasks: fine-grained entity\\ntyping, morphological tagging, and named entity recognition. We conduct a\\nsystematic study that spans several dimensions of comparison: 1) type of data\\nscarcity which can stem from the lack of task-specific training data, or even\\nfrom the lack of unannotated data required to train word embeddings, or both;\\n2) language type by working with a sample of 16 typologically diverse languages\\nincluding some truly low-resource ones (e.g. Rusyn, Buryat, and Zulu); 3) the\\nchoice of the subword-informed word representation method. Our main results\\nshow that subword-informed models are universally useful across all language\\ntypes, with large gains over subword-agnostic embeddings. They also suggest\\nthat the effective use of subwords largely depends on the language (type) and\\nthe task at hand, as well as on the amount of available data for training the\\nembeddings and task-based models, where having sufficient in-task data is a\\nmore critical requirement.\\n', 'publish_date': 'Thu, 26 Sep 2019 20:26:51 GMT'}\n",
      "ID: 2004.13304, Distance: 0.29905104637145996, Content: {'title': 'Learning to Learn Morphological Inflection for Resource-Poor Languages', 'doi': None, 'categories': 'cs.CL', 'abstract': '  We propose to cast the task of morphological inflection - mapping a lemma to\\nan indicated inflected form - for resource-poor languages as a meta-learning\\nproblem. Treating each language as a separate task, we use data from\\nhigh-resource source languages to learn a set of model parameters that can\\nserve as a strong initialization point for fine-tuning on a resource-poor\\ntarget language. Experiments with two model architectures on 29 target\\nlanguages from 3 families show that our suggested approach outperforms all\\nbaselines. In particular, it obtains a 31.7% higher absolute accuracy than a\\npreviously proposed cross-lingual transfer model and outperforms the previous\\nstate of the art by 1.7% absolute accuracy on average over languages.\\n', 'publish_date': 'Tue, 28 Apr 2020 05:13:17 GMT'}\n",
      "ID: 1705.02314, Distance: 0.3131658732891083, Content: {'title': 'Building Morphological Chains for Agglutinative Languages', 'doi': None, 'categories': 'cs.CL', 'abstract': '  In this paper, we build morphological chains for agglutinative languages by\\nusing a log-linear model for the morphological segmentation task. The model is\\nbased on the unsupervised morphological segmentation system called\\nMorphoChains. We extend MorphoChains log linear model by expanding the\\ncandidate space recursively to cover more split points for agglutinative\\nlanguages such as Turkish, whereas in the original model candidates are\\ngenerated by considering only binary segmentation of each word. The results\\nshow that we improve the state-of-art Turkish scores by 12% having a F-measure\\nof 72% and we improve the English scores by 3% having a F-measure of 74%.\\nEventually, the system outperforms both MorphoChains and other well-known\\nunsupervised morphological segmentation systems. The results indicate that\\ncandidate generation plays an important role in such an unsupervised log-linear\\nmodel that is learned using contrastive estimation with negative samples.\\n', 'publish_date': 'Fri, 5 May 2017 17:30:50 GMT'}\n",
      "ID: 2411.14198, Distance: 0.31762588024139404, Content: {'title': 'Why do language models perform worse for morphologically complex\\n  languages?', 'doi': None, 'categories': 'cs.CL', 'abstract': '  Language models perform differently across languages. It has been previously\\nsuggested that morphological typology may explain some of this variability\\n(Cotterell et al., 2018). We replicate previous analyses and find additional\\nnew evidence for a performance gap between agglutinative and fusional\\nlanguages, where fusional languages, such as English, tend to have better\\nlanguage modeling performance than morphologically more complex languages like\\nTurkish. We then propose and test three possible causes for this performance\\ngap: morphological alignment of tokenizers, tokenization quality, and\\ndisparities in dataset sizes and measurement. To test the morphological\\nalignment hypothesis, we present MorphScore, a tokenizer evaluation metric, and\\nsupporting datasets for 22 languages. We find some evidence that tokenization\\nquality explains the performance gap, but none for the role of morphological\\nalignment. Instead we find that the performance gap is most reduced when\\ntraining datasets are of equivalent size across language types, but only when\\nscaled according to the so-called \"byte-premium\" -- the different encoding\\nefficiencies of different languages and orthographies. These results suggest\\nthat no language is harder or easier for a language model to learn on the basis\\nof its morphological typology. Differences in performance can be attributed to\\ndisparities in dataset size. These results bear on ongoing efforts to improve\\nperformance for low-performing and under-resourced languages.\\n', 'publish_date': 'Thu, 21 Nov 2024 15:06:51 GMT'}\n",
      "ID: 1707.09569, Distance: 0.3180468678474426, Content: {'title': 'Learning Language Representations for Typology Prediction', 'doi': None, 'categories': 'cs.CL', 'abstract': '  One central mystery of neural NLP is what neural models \"know\" about their\\nsubject matter. When a neural machine translation system learns to translate\\nfrom one language to another, does it learn the syntax or semantics of the\\nlanguages? Can this knowledge be extracted from the system to fill holes in\\nhuman scientific knowledge? Existing typological databases contain relatively\\nfull feature specifications for only a few hundred languages. Exploiting the\\nexistence of parallel texts in more than a thousand languages, we build a\\nmassive many-to-one neural machine translation (NMT) system from 1017 languages\\ninto English, and use this to predict information missing from typological\\ndatabases. Experiments show that the proposed method is able to infer not only\\nsyntactic, but also phonological and phonetic inventory features, and improves\\nover a baseline that has access to information about the languages\\' geographic\\nand phylogenetic neighbors.\\n', 'publish_date': 'Sat, 29 Jul 2017 23:38:25 GMT'}\n",
      "\n",
      "Answer:\n",
      "\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "生成嵌入中: 100%|██████████| 1/1 [00:00<00:00, 12.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "cost 0.1412 seconds\n",
      "\n",
      "\n",
      "You must reply in English. You are a professional academic assistant specializing in the cs.CL domain. Strictly based on the provided literature information, answer the user's query. You may slightly expand and refine the response using existing knowledge, but avoid fabricating information. The response language must be English. Ensure the response is concise and limited to 500 words. Include a citation with the corresponding literature link (like [text](f\"https://arxiv.org/abs/{{id}}\")) and adjust the response style based on the temperature parameter (from 0.0 to 1.0). If the temperature is low (e.g., 0.2), provide a strict and precise answer. If the temperature is high (e.g., 0.8), provide a more creative and exploratory answer.\n",
      "\n",
      "Temperature: 0.2\n",
      "Question: BGE模型如何将文本向量化\n",
      "Relevant Literature: \n",
      "ID: 2303.07203, Distance: 0.4288061559200287, Content: {'title': 'On the Robustness of Text Vectorizers', 'doi': None, 'categories': 'cs.CL cs.LG stat.ML', 'abstract': '  A fundamental issue in machine learning is the robustness of the model with\\nrespect to changes in the input. In natural language processing, models\\ntypically contain a first embedding layer, transforming a sequence of tokens\\ninto vector representations. While the robustness with respect to changes of\\ncontinuous inputs is well-understood, the situation is less clear when\\nconsidering discrete changes, for instance replacing a word by another in an\\ninput sentence. Our work formally proves that popular embedding schemes, such\\nas concatenation, TF-IDF, and Paragraph Vector (a.k.a. doc2vec), exhibit\\nrobustness in the H\\\\\"older or Lipschitz sense with respect to the Hamming\\ndistance. We provide quantitative bounds for these schemes and demonstrate how\\nthe constants involved are affected by the length of the document. These\\nfindings are exemplified through a series of numerical examples.\\n', 'publish_date': 'Thu, 9 Mar 2023 16:37:37 GMT'}\n",
      "ID: 1607.00534, Distance: 0.4422970712184906, Content: {'title': 'Text comparison using word vector representations and dimensionality\\n  reduction', 'doi': None, 'categories': 'cs.CL', 'abstract': '  This paper describes a technique to compare large text sources using word\\nvector representations (word2vec) and dimensionality reduction (t-SNE) and how\\nit can be implemented using Python. The technique provides a bird\\'s-eye view of\\ntext sources, e.g. text summaries and their source material, and enables users\\nto explore text sources like a geographical map. Word vector representations\\ncapture many linguistic properties such as gender, tense, plurality and even\\nsemantic concepts like \"capital city of\". Using dimensionality reduction, a 2D\\nmap can be computed where semantically similar words are close to each other.\\nThe technique uses the word2vec model from the gensim Python library and t-SNE\\nfrom scikit-learn.\\n', 'publish_date': 'Sat, 2 Jul 2016 17:17:22 GMT'}\n",
      "ID: 1709.05778, Distance: 0.4977453351020813, Content: {'title': 'Word Vector Enrichment of Low Frequency Words in the Bag-of-Words Model\\n  for Short Text Multi-class Classification Problems', 'doi': None, 'categories': 'cs.CL cs.LG', 'abstract': '  The bag-of-words model is a standard representation of text for many linear\\nclassifier learners. In many problem domains, linear classifiers are preferred\\nover more complex models due to their efficiency, robustness and\\ninterpretability, and the bag-of-words text representation can capture\\nsufficient information for linear classifiers to make highly accurate\\npredictions. However in settings where there is a large vocabulary, large\\nvariance in the frequency of terms in the training corpus, many classes and\\nvery short text (e.g., single sentences or document titles) the bag-of-words\\nrepresentation becomes extremely sparse, and this can reduce the accuracy of\\nclassifiers. A particular issue in such settings is that short texts tend to\\ncontain infrequently occurring or rare terms which lack class-conditional\\nevidence. In this work we introduce a method for enriching the bag-of-words\\nmodel by complementing such rare term information with related terms from both\\ngeneral and domain-specific Word Vector models. By reducing sparseness in the\\nbag-of-words models, our enrichment approach achieves improved classification\\nover several baseline classifiers in a variety of text classification problems.\\nOur approach is also efficient because it requires no change to the linear\\nclassifier before or during training, since bag-of-words enrichment applies\\nonly to text being classified.\\n', 'publish_date': 'Mon, 18 Sep 2017 05:00:34 GMT'}\n",
      "ID: cs/0412114, Distance: 0.5134426951408386, Content: {'title': 'State of the Art, Evaluation and Recommendations regarding \"Document\\n  Processing and Visualization Techniques\"', 'doi': None, 'categories': 'cs.CL', 'abstract': '  Several Networks of Excellence have been set up in the framework of the\\nEuropean FP5 research program. Among these Networks of Excellence, the NEMIS\\nproject focuses on the field of Text Mining.\\n  Within this field, document processing and visualization was identified as\\none of the key topics and the WG1 working group was created in the NEMIS\\nproject, to carry out a detailed survey of techniques associated with the text\\nmining process and to identify the relevant research topics in related research\\nareas.\\n  In this document we present the results of this comprehensive survey. The\\nreport includes a description of the current state-of-the-art and practice, a\\nroadmap for follow-up research in the identified areas, and recommendations for\\nanticipated technological development in the domain of text mining.\\n', 'publish_date': 'Wed, 29 Dec 2004 15:19:03 GMT'}\n",
      "ID: cmp-lg/9706016, Distance: 0.5153446197509766, Content: {'title': 'Text Segmentation Using Exponential Models', 'doi': None, 'categories': 'cmp-lg cs.CL', 'abstract': '  This paper introduces a new statistical approach to partitioning text\\nautomatically into coherent segments. Our approach enlists both short-range and\\nlong-range language models to help it sniff out likely sites of topic changes\\nin text. To aid its search, the system consults a set of simple lexical hints\\nit has learned to associate with the presence of boundaries through inspection\\nof a large corpus of annotated data. We also propose a new probabilistically\\nmotivated error metric for use by the natural language processing and\\ninformation retrieval communities, intended to supersede precision and recall\\nfor appraising segmentation algorithms. Qualitative assessment of our algorithm\\nas well as evaluation using this new metric demonstrate the effectiveness of\\nour approach in two very different domains, Wall Street Journal articles and\\nthe TDT Corpus, a collection of newswire articles and broadcast news\\ntranscripts.\\n', 'publish_date': 'Wed, 11 Jun 1997 20:16:04 GMT'}\n",
      "\n",
      "Answer:\n",
      "\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "生成嵌入中: 100%|██████████| 1/1 [00:00<00:00, 10.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "cost 0.1507 seconds\n",
      "\n",
      "\n",
      "You must reply in English. You are a professional academic assistant specializing in the cs.CL domain. Strictly based on the provided literature information, answer the user's query. You may slightly expand and refine the response using existing knowledge, but avoid fabricating information. The response language must be English. Ensure the response is concise and limited to 500 words. Include a citation with the corresponding literature link (like [text](f\"https://arxiv.org/abs/{{id}}\")) and adjust the response style based on the temperature parameter (from 0.0 to 1.0). If the temperature is low (e.g., 0.2), provide a strict and precise answer. If the temperature is high (e.g., 0.8), provide a more creative and exploratory answer.\n",
      "\n",
      "Temperature: 0.2\n",
      "Question: 在机器翻译中，如何处理专有名词和术语的翻译问题？\n",
      "Relevant Literature: \n",
      "ID: cmp-lg/9601008, Distance: 0.40932410955429077, Content: {'title': 'Noun Phrase Reference in Japanese-to-English Machine Translation', 'doi': None, 'categories': 'cmp-lg cs.CL', 'abstract': '  This paper shows the necessity of distinguishing different referential uses\\nof noun phrases in machine translation. We argue that differentiating between\\nthe generic, referential and ascriptive uses of noun phrases is the minimum\\nnecessary to generate articles and number correctly when translating from\\nJapanese to English. Heuristics for determining these differences are proposed\\nfor a Japanese-to-English machine translation system. Finally the results of\\nusing the proposed heuristics are shown to have raised the percentage of noun\\nphrases generated with correct use of articles and number in the\\nJapanese-to-English machine translation system ALT-J/E from 65% to 77%.\\n', 'publish_date': 'Tue, 23 Jan 1996 13:52:50 GMT'}\n",
      "ID: cmp-lg/9608014, Distance: 0.4405078887939453, Content: {'title': 'Classifiers in Japanese-to-English Machine Translation', 'doi': None, 'categories': 'cmp-lg cs.CL', 'abstract': '  This paper proposes an analysis of classifiers into four major types: UNIT,\\nMETRIC, GROUP and SPECIES, based on properties of both Japanese and English.\\nThe analysis makes possible a uniform and straightforward treatment of noun\\nphrases headed by classifiers in Japanese-to-English machine translation, and\\nhas been implemented in the MT system ALT-J/E. Although the analysis is based\\non the characteristics of, and differences between, Japanese and English, it is\\nshown to be also applicable to the unrelated language Thai.\\n', 'publish_date': 'Wed, 21 Aug 1996 06:05:54 GMT'}\n",
      "ID: 2101.10035, Distance: 0.45035213232040405, Content: {'title': 'Facilitating Terminology Translation with Target Lemma Annotations', 'doi': None, 'categories': 'cs.CL', 'abstract': '  Most of the recent work on terminology integration in machine translation has\\nassumed that terminology translations are given already inflected in forms that\\nare suitable for the target language sentence. In day-to-day work of\\nprofessional translators, however, it is seldom the case as translators work\\nwith bilingual glossaries where terms are given in their dictionary forms;\\nfinding the right target language form is part of the translation process. We\\nargue that the requirement for apriori specified target language forms is\\nunrealistic and impedes the practical applicability of previous work. In this\\nwork, we propose to train machine translation systems using a source-side data\\naugmentation method that annotates randomly selected source language words with\\ntheir target language lemmas. We show that systems trained on such augmented\\ndata are readily usable for terminology integration in real-life translation\\nscenarios. Our experiments on terminology translation into the morphologically\\ncomplex Baltic and Uralic languages show an improvement of up to 7 BLEU points\\nover baseline systems with no means for terminology integration and an average\\nimprovement of 4 BLEU points over the previous work. Results of the human\\nevaluation indicate a 47.7% absolute improvement over the previous work in term\\ntranslation accuracy when translating into Latvian.\\n', 'publish_date': 'Mon, 25 Jan 2021 12:07:20 GMT'}\n",
      "ID: 1105.1072, Distance: 0.4505748748779297, Content: {'title': 'English-Lithuanian-English Machine Translation lexicon and engine:\\n  current state and future work', 'doi': None, 'categories': 'cs.CL', 'abstract': '  This article overviews the current state of the English-Lithuanian-English\\nmachine translation system. The first part of the article describes the\\nproblems that system poses today and what actions will be taken to solve them\\nin the future. The second part of the article tackles the main issue of the\\ntranslation process. Article briefly overviews the word sense disambiguation\\nfor MT technique using Google.\\n', 'publish_date': 'Thu, 5 May 2011 13:51:46 GMT'}\n",
      "ID: 1407.1605, Distance: 0.45342761278152466, Content: {'title': \"Les noms propres se traduisent-ils ? \\\\'Etude d'un corpus multilingue\", 'doi': None, 'categories': 'cs.CL', 'abstract': '  In this paper, we tackle the problem of the translation of proper names. We\\nintroduce our hypothesis according to which proper names can be translated more\\noften than most people seem to think. Then, we describe the construction of a\\nparallel multilingual corpus used to illustrate our point. We eventually\\nevaluate both the advantages and limits of this corpus in our study.\\n', 'publish_date': 'Mon, 7 Jul 2014 07:08:07 GMT'}\n",
      "\n",
      "Answer:\n",
      "\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "生成嵌入中: 100%|██████████| 1/1 [00:00<00:00, 11.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "cost 0.1547 seconds\n",
      "\n",
      "\n",
      "You must reply in English. You are a professional academic assistant specializing in the cs.CL domain. Strictly based on the provided literature information, answer the user's query. You may slightly expand and refine the response using existing knowledge, but avoid fabricating information. The response language must be English. Ensure the response is concise and limited to 500 words. Include a citation with the corresponding literature link (like [text](f\"https://arxiv.org/abs/{{id}}\")) and adjust the response style based on the temperature parameter (from 0.0 to 1.0). If the temperature is low (e.g., 0.2), provide a strict and precise answer. If the temperature is high (e.g., 0.8), provide a more creative and exploratory answer.\n",
      "\n",
      "Temperature: 0.2\n",
      "Question: 在机器翻译中，如何提高对长句和复杂句的翻译质量？\n",
      "Relevant Literature: \n",
      "ID: 2111.00554, Distance: 0.3593173921108246, Content: {'title': 'Quality Estimation Using Round-trip Translation with Sentence Embeddings', 'doi': None, 'categories': 'cs.CL cs.LG', 'abstract': '  Estimating the quality of machine translation systems has been an ongoing\\nchallenge for researchers in this field. Many previous attempts at using\\nround-trip translation as a measure of quality have failed, and there is much\\ndisagreement as to whether it can be a viable method of quality estimation. In\\nthis paper, we revisit round-trip translation, proposing a system which aims to\\nsolve the previous pitfalls found with the approach. Our method makes use of\\nrecent advances in language representation learning to more accurately gauge\\nthe similarity between the original and round-trip translated sentences.\\nExperiments show that while our approach does not reach the performance of\\ncurrent state of the art methods, it may still be an effective approach for\\nsome language pairs.\\n', 'publish_date': 'Sun, 31 Oct 2021 17:51:12 GMT'}\n",
      "ID: 2005.03519, Distance: 0.3666062653064728, Content: {'title': 'Practical Perspectives on Quality Estimation for Machine Translation', 'doi': None, 'categories': 'cs.CL', 'abstract': '  Sentence level quality estimation (QE) for machine translation (MT) attempts\\nto predict the translation edit rate (TER) cost of post-editing work required\\nto correct MT output. We describe our view on sentence-level QE as dictated by\\nseveral practical setups encountered in the industry. We find consumers of MT\\noutput---whether human or algorithmic ones---to be primarily interested in a\\nbinary quality metric: is the translated sentence adequate as-is or does it\\nneed post-editing? Motivated by this we propose a quality classification (QC)\\nview on sentence-level QE whereby we focus on maximizing recall at precision\\nabove a given threshold. We demonstrate that, while classical QE regression\\nmodels fare poorly on this task, they can be re-purposed by replacing the\\noutput regression layer with a binary classification one, achieving 50-60\\\\%\\nrecall at 90\\\\% precision. For a high-quality MT system producing 75-80\\\\%\\ncorrect translations, this promises a significant reduction in post-editing\\nwork indeed.\\n', 'publish_date': 'Sat, 2 May 2020 01:50:10 GMT'}\n",
      "ID: 2306.15399, Distance: 0.36823415756225586, Content: {'title': 'Quality Estimation of Machine Translated Texts based on Direct Evidence\\n  from Training Data', 'doi': None, 'categories': 'cs.CL', 'abstract': '  Current Machine Translation systems achieve very good results on a growing\\nvariety of language pairs and data sets. However, it is now well known that\\nthey produce fluent translation outputs that often can contain important\\nmeaning errors. Quality Estimation task deals with the estimation of quality of\\ntranslations produced by a Machine Translation system without depending on\\nReference Translations. A number of approaches have been suggested over the\\nyears. In this paper we show that the parallel corpus used as training data for\\ntraining the MT system holds direct clues for estimating the quality of\\ntranslations produced by the MT system. Our experiments show that this simple\\nand direct method holds promise for quality estimation of translations produced\\nby any purely data driven machine translation system.\\n', 'publish_date': 'Tue, 27 Jun 2023 11:52:28 GMT'}\n",
      "ID: 2109.05016, Distance: 0.3713919520378113, Content: {'title': 'Neural Machine Translation Quality and Post-Editing Performance', 'doi': None, 'categories': 'cs.CL cs.HC', 'abstract': '  We test the natural expectation that using MT in professional translation\\nsaves human processing time. The last such study was carried out by\\nSanchez-Torron and Koehn (2016) with phrase-based MT, artificially reducing the\\ntranslation quality. In contrast, we focus on neural MT (NMT) of high quality,\\nwhich has become the state-of-the-art approach since then and also got adopted\\nby most translation companies.\\n  Through an experimental study involving over 30 professional translators for\\nEnglish -> Czech translation, we examine the relationship between NMT\\nperformance and post-editing time and quality. Across all models, we found that\\nbetter MT systems indeed lead to fewer changes in the sentences in this\\nindustry setting. The relation between system quality and post-editing time is\\nhowever not straightforward and, contrary to the results on phrase-based MT,\\nBLEU is definitely not a stable predictor of the time or final output quality.\\n', 'publish_date': 'Fri, 10 Sep 2021 17:56:02 GMT'}\n",
      "ID: 1802.01451, Distance: 0.3731197714805603, Content: {'title': 'Quantitative Fine-Grained Human Evaluation of Machine Translation\\n  Systems: a Case Study on English to Croatian', 'doi': '10.1007/s10590-018-9214-x', 'categories': 'cs.CL cs.AI', 'abstract': '  This paper presents a quantitative fine-grained manual evaluation approach to\\ncomparing the performance of different machine translation (MT) systems. We\\nbuild upon the well-established Multidimensional Quality Metrics (MQM) error\\ntaxonomy and implement a novel method that assesses whether the differences in\\nperformance for MQM error types between different MT systems are statistically\\nsignificant. We conduct a case study for English-to-Croatian, a language\\ndirection that involves translating into a morphologically rich language, for\\nwhich we compare three MT systems belonging to different paradigms: pure\\nphrase-based, factored phrase-based and neural. First, we design an\\nMQM-compliant error taxonomy tailored to the relevant linguistic phenomena of\\nSlavic languages, which made the annotation process feasible and accurate.\\nErrors in MT outputs were then annotated by two annotators following this\\ntaxonomy. Subsequently, we carried out a statistical analysis which showed that\\nthe best-performing system (neural) reduces the errors produced by the worst\\nsystem (pure phrase-based) by more than half (54\\\\%). Moreover, we conducted an\\nadditional analysis of agreement errors in which we distinguished between short\\n(phrase-level) and long distance (sentence-level) errors. We discovered that\\nphrase-based MT approaches are of limited use for long distance agreement\\nphenomena, for which neural MT was found to be especially effective.\\n', 'publish_date': 'Fri, 2 Feb 2018 14:41:08 GMT'}\n",
      "\n",
      "Answer:\n",
      "\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "生成嵌入中: 100%|██████████| 1/1 [00:00<00:00, 12.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "cost 0.1442 seconds\n",
      "\n",
      "\n",
      "You must reply in English. You are a professional academic assistant specializing in the cs.CL domain. Strictly based on the provided literature information, answer the user's query. You may slightly expand and refine the response using existing knowledge, but avoid fabricating information. The response language must be English. Ensure the response is concise and limited to 500 words. Include a citation with the corresponding literature link (like [text](f\"https://arxiv.org/abs/{{id}}\")) and adjust the response style based on the temperature parameter (from 0.0 to 1.0). If the temperature is low (e.g., 0.2), provide a strict and precise answer. If the temperature is high (e.g., 0.8), provide a more creative and exploratory answer.\n",
      "\n",
      "Temperature: 0.2\n",
      "Question: 如何构建一个高效的文本语义相似度计算模型？\n",
      "Relevant Literature: \n",
      "ID: 2004.13820, Distance: 0.4167209267616272, Content: {'title': 'Evolution of Semantic Similarity -- A Survey', 'doi': '10.1145/3440755', 'categories': 'cs.CL cs.IR', 'abstract': '  Estimating the semantic similarity between text data is one of the\\nchallenging and open research problems in the field of Natural Language\\nProcessing (NLP). The versatility of natural language makes it difficult to\\ndefine rule-based methods for determining semantic similarity measures. In\\norder to address this issue, various semantic similarity methods have been\\nproposed over the years. This survey article traces the evolution of such\\nmethods, categorizing them based on their underlying principles as\\nknowledge-based, corpus-based, deep neural network-based methods, and hybrid\\nmethods. Discussing the strengths and weaknesses of each method, this survey\\nprovides a comprehensive view of existing systems in place, for new researchers\\nto experiment and develop innovative ideas to address the issue of semantic\\nsimilarity.\\n', 'publish_date': 'Sun, 19 Apr 2020 22:07:39 GMT'}\n",
      "ID: 1310.8059, Distance: 0.4580899477005005, Content: {'title': 'Description and Evaluation of Semantic Similarity Measures Approaches', 'doi': '10.5120/13897-1851', 'categories': 'cs.CL', 'abstract': '  In recent years, semantic similarity measure has a great interest in Semantic\\nWeb and Natural Language Processing (NLP). Several similarity measures have\\nbeen developed, being given the existence of a structured knowledge\\nrepresentation offered by ontologies and corpus which enable semantic\\ninterpretation of terms. Semantic similarity measures compute the similarity\\nbetween concepts/terms included in knowledge sources in order to perform\\nestimations. This paper discusses the existing semantic similarity methods\\nbased on structure, information content and feature approaches. Additionally,\\nwe present a critical evaluation of several categories of semantic similarity\\napproaches based on two standard benchmarks. The aim of this paper is to give\\nan efficient evaluation of all these measures which help researcher and\\npractitioners to select the measure that best fit for their requirements.\\n', 'publish_date': 'Wed, 30 Oct 2013 08:08:43 GMT'}\n",
      "ID: 1610.04533, Distance: 0.46748241782188416, Content: {'title': 'A Comprehensive Comparative Study of Word and Sentence Similarity\\n  Measures', 'doi': '10.5120/ijca2016908259', 'categories': 'cs.IR cs.CL', 'abstract': '  Sentence similarity is considered the basis of many natural language tasks\\nsuch as information retrieval, question answering and text summarization. The\\nsemantic meaning between compared text fragments is based on the words semantic\\nfeatures and their relationships. This article reviews a set of word and\\nsentence similarity measures and compares them on benchmark datasets. On the\\nstudied datasets, results showed that hybrid semantic measures perform better\\nthan both knowledge and corpus based measures.\\n', 'publish_date': 'Wed, 17 Feb 2016 19:33:47 GMT'}\n",
      "ID: 1401.5699, Distance: 0.4688728451728821, Content: {'title': 'Text Relatedness Based on a Word Thesaurus', 'doi': '10.1613/jair.2880', 'categories': 'cs.CL', 'abstract': '  The computation of relatedness between two fragments of text in an automated\\nmanner requires taking into account a wide range of factors pertaining to the\\nmeaning the two fragments convey, and the pairwise relations between their\\nwords. Without doubt, a measure of relatedness between text segments must take\\ninto account both the lexical and the semantic relatedness between words. Such\\na measure that captures well both aspects of text relatedness may help in many\\ntasks, such as text retrieval, classification and clustering. In this paper we\\npresent a new approach for measuring the semantic relatedness between words\\nbased on their implicit semantic links. The approach exploits only a word\\nthesaurus in order to devise implicit semantic links between words. Based on\\nthis approach, we introduce Omiotis, a new measure of semantic relatedness\\nbetween texts which capitalizes on the word-to-word semantic relatedness\\nmeasure (SR) and extends it to measure the relatedness between texts. We\\ngradually validate our method: we first evaluate the performance of the\\nsemantic relatedness measure between individual words, covering word-to-word\\nsimilarity and relatedness, synonym identification and word analogy; then, we\\nproceed with evaluating the performance of our method in measuring text-to-text\\nsemantic relatedness in two tasks, namely sentence-to-sentence similarity and\\nparaphrase recognition. Experimental evaluation shows that the proposed method\\noutperforms every lexicon-based method of semantic relatedness in the selected\\ntasks and the used data sets, and competes well against corpus-based and hybrid\\napproaches.\\n', 'publish_date': 'Wed, 15 Jan 2014 05:41:08 GMT'}\n",
      "ID: cmp-lg/9709008, Distance: 0.47116953134536743, Content: {'title': 'Semantic Similarity Based on Corpus Statistics and Lexical Taxonomy', 'doi': None, 'categories': 'cmp-lg cs.CL', 'abstract': '  This paper presents a new approach for measuring semantic similarity/distance\\nbetween words and concepts. It combines a lexical taxonomy structure with\\ncorpus statistical information so that the semantic distance between nodes in\\nthe semantic space constructed by the taxonomy can be better quantified with\\nthe computational evidence derived from a distributional analysis of corpus\\ndata. Specifically, the proposed measure is a combined approach that inherits\\nthe edge-based approach of the edge counting scheme, which is then enhanced by\\nthe node-based approach of the information content calculation. When tested on\\na common data set of word pair similarity ratings, the proposed approach\\noutperforms other computational models. It gives the highest correlation value\\n(r = 0.828) with a benchmark based on human similarity judgements, whereas an\\nupper bound (r = 0.885) is observed when human subjects replicate the same\\ntask.\\n', 'publish_date': 'Sat, 20 Sep 1997 15:16:26 GMT'}\n",
      "\n",
      "Answer:\n",
      "\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "生成嵌入中: 100%|██████████| 1/1 [00:00<00:00,  8.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "cost 0.1877 seconds\n",
      "\n",
      "\n",
      "You must reply in English. You are a professional academic assistant specializing in the cs.CL domain. Strictly based on the provided literature information, answer the user's query. You may slightly expand and refine the response using existing knowledge, but avoid fabricating information. The response language must be English. Ensure the response is concise and limited to 500 words. Include a citation with the corresponding literature link (like [text](f\"https://arxiv.org/abs/{{id}}\")) and adjust the response style based on the temperature parameter (from 0.0 to 1.0). If the temperature is low (e.g., 0.2), provide a strict and precise answer. If the temperature is high (e.g., 0.8), provide a more creative and exploratory answer.\n",
      "\n",
      "Temperature: 0.2\n",
      "Question: 在多语言对话系统中，如何实现语言的无缝切换和交流？\n",
      "Relevant Literature: \n",
      "ID: 2502.12813, Distance: 0.41870278120040894, Content: {'title': 'Simulating User Diversity in Task-Oriented Dialogue Systems using Large\\n  Language Models', 'doi': None, 'categories': 'cs.CL', 'abstract': '  In this study, we explore the application of Large Language Models (LLMs) for\\ngenerating synthetic users and simulating user conversations with a\\ntask-oriented dialogue system and present detailed results and their analysis.\\nWe propose a comprehensive novel approach to user simulation technique that\\nuses LLMs to create diverse user profiles, set goals, engage in multi-turn\\ndialogues, and evaluate the conversation success. We employ two proprietary\\nLLMs, namely GPT-4o and GPT-o1 (Achiam et al., 2023), to generate a\\nheterogeneous base of user profiles, characterized by varied demographics,\\nmultiple user goals, different conversational styles, initial knowledge levels,\\ninterests, and conversational objectives. We perform a detailed analysis of the\\nuser profiles generated by LLMs to assess the diversity, consistency, and\\npotential biases inherent in these LLM-generated user simulations. We find that\\nGPT-o1 generates more heterogeneous user distribution across most user\\nattributes, while GPT-4o generates more skewed user attributes. The generated\\nset of user profiles are then utilized to simulate dialogue sessions by\\ninteracting with a task-oriented dialogue system.\\n', 'publish_date': 'Tue, 18 Feb 2025 12:20:16 GMT'}\n",
      "ID: 2003.07568, Distance: 0.4248824417591095, Content: {'title': 'XPersona: Evaluating Multilingual Personalized Chatbot', 'doi': None, 'categories': 'cs.CL', 'abstract': '  Personalized dialogue systems are an essential step toward better\\nhuman-machine interaction. Existing personalized dialogue agents rely on\\nproperly designed conversational datasets, which are mostly monolingual (e.g.,\\nEnglish), which greatly limits the usage of conversational agents in other\\nlanguages. In this paper, we propose a multi-lingual extension of Persona-Chat,\\nnamely XPersona. Our dataset includes persona conversations in six different\\nlanguages other than English for building and evaluating multilingual\\npersonalized agents. We experiment with both multilingual and cross-lingual\\ntrained baselines, and evaluate them against monolingual and\\ntranslation-pipeline models using both automatic and human evaluation.\\nExperimental results show that the multilingual trained models outperform the\\ntranslation-pipeline and that they are on par with the monolingual models, with\\nthe advantage of having a single model across multiple languages. On the other\\nhand, the state-of-the-art cross-lingual trained models achieve inferior\\nperformance to the other models, showing that cross-lingual conversation\\nmodeling is a challenging task. We hope that our dataset and baselines will\\naccelerate research in multilingual dialogue systems.\\n', 'publish_date': 'Tue, 17 Mar 2020 07:52:08 GMT'}\n",
      "ID: 2305.16324, Distance: 0.42533189058303833, Content: {'title': 'Talking with Machines: A Comprehensive Survey of Emergent Dialogue\\n  Systems', 'doi': None, 'categories': 'cs.CL', 'abstract': '  From the earliest experiments in the 20th century to the utilization of large\\nlanguage models and transformers, dialogue systems research has continued to\\nevolve, playing crucial roles in numerous fields. This paper offers a\\ncomprehensive review of these systems, tracing their historical development and\\nexamining their fundamental operations. We analyze popular and emerging\\ndatasets for training and survey key contributions in dialogue systems\\nresearch, including traditional systems and advanced machine learning methods.\\nFinally, we consider conventional and transformer-based evaluation metrics,\\nfollowed by a short discussion of prevailing challenges and future prospects in\\nthe field.\\n', 'publish_date': 'Wed, 10 May 2023 12:24:03 GMT'}\n",
      "ID: cmp-lg/9612004, Distance: 0.42772236466407776, Content: {'title': 'Dialogos: a Robust System for Human-Machine Spoken Dialogue on the\\n  Telephone', 'doi': None, 'categories': 'cmp-lg cs.CL', 'abstract': '  This paper presents Dialogos, a real-time system for human-machine spoken\\ndialogue on the telephone in task-oriented domains. The system has been tested\\nin a large trial with inexperienced users and it has proved robust enough to\\nallow spontaneous interactions both to users which get good recognition\\nperformance and to the ones which get lower scores. The robust behavior of the\\nsystem has been achieved by combining the use of specific language models\\nduring the recognition phase of analysis, the tolerance toward spontaneous\\nspeech phenomena, the activity of a robust parser, and the use of\\npragmatic-based dialogue knowledge. This integration of the different modules\\nallows to deal with partial or total breakdowns of the different levels of\\nanalysis. We report the field trial data of the system and the evaluation\\nresults of the overall system and of the submodules.\\n', 'publish_date': 'Fri, 20 Dec 1996 09:50:56 GMT'}\n",
      "ID: cs/9903008, Distance: 0.43078622221946716, Content: {'title': 'Empirically Evaluating an Adaptable Spoken Dialogue System', 'doi': None, 'categories': 'cs.CL', 'abstract': \"  Recent technological advances have made it possible to build real-time,\\ninteractive spoken dialogue systems for a wide variety of applications.\\nHowever, when users do not respect the limitations of such systems, performance\\ntypically degrades. Although users differ with respect to their knowledge of\\nsystem limitations, and although different dialogue strategies make system\\nlimitations more apparent to users, most current systems do not try to improve\\nperformance by adapting dialogue behavior to individual users. This paper\\npresents an empirical evaluation of TOOT, an adaptable spoken dialogue system\\nfor retrieving train schedules on the web. We conduct an experiment in which 20\\nusers carry out 4 tasks with both adaptable and non-adaptable versions of TOOT,\\nresulting in a corpus of 80 dialogues. The values for a wide range of\\nevaluation measures are then extracted from this corpus. Our results show that\\nadaptable TOOT generally outperforms non-adaptable TOOT, and that the utility\\nof adaptation depends on TOOT's initial dialogue strategies.\\n\", 'publish_date': 'Fri, 5 Mar 1999 22:03:13 GMT'}\n",
      "\n",
      "Answer:\n",
      "\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "生成嵌入中: 100%|██████████| 1/1 [00:00<00:00, 11.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "cost 0.1744 seconds\n",
      "\n",
      "\n",
      "You must reply in English. You are a professional academic assistant specializing in the cs.CL domain. Strictly based on the provided literature information, answer the user's query. You may slightly expand and refine the response using existing knowledge, but avoid fabricating information. The response language must be English. Ensure the response is concise and limited to 500 words. Include a citation with the corresponding literature link (like [text](f\"https://arxiv.org/abs/{{id}}\")) and adjust the response style based on the temperature parameter (from 0.0 to 1.0). If the temperature is low (e.g., 0.2), provide a strict and precise answer. If the temperature is high (e.g., 0.8), provide a more creative and exploratory answer.\n",
      "\n",
      "Temperature: 0.2\n",
      "Question: 在诗歌生成任务中，如何让机器生成符合诗歌格律和意境的作品？\n",
      "Relevant Literature: \n",
      "ID: 2205.01821, Distance: 0.3823384940624237, Content: {'title': 'Zero-shot Sonnet Generation with Discourse-level Planning and Aesthetics\\n  Features', 'doi': None, 'categories': 'cs.CL cs.AI cs.LG', 'abstract': '  Poetry generation, and creative language generation in general, usually\\nsuffers from the lack of large training data. In this paper, we present a novel\\nframework to generate sonnets that does not require training on poems. We\\ndesign a hierarchical framework which plans the poem sketch before decoding.\\nSpecifically, a content planning module is trained on non-poetic texts to\\nobtain discourse-level coherence; then a rhyme module generates rhyme words and\\na polishing module introduces imagery and similes for aesthetics purposes.\\nFinally, we design a constrained decoding algorithm to impose the\\nmeter-and-rhyme constraint of the generated sonnets. Automatic and human\\nevaluation show that our multi-stage approach without training on poem corpora\\ngenerates more coherent, poetic, and creative sonnets than several strong\\nbaselines.\\n', 'publish_date': 'Tue, 3 May 2022 23:44:28 GMT'}\n",
      "ID: 2412.15263, Distance: 0.3843775987625122, Content: {'title': \"PROPOE 2: Avan\\\\c{c}os na S\\\\'intese Computacional de Poemas Baseados em\\n  Prosa Liter\\\\'aria Brasileira\", 'doi': None, 'categories': 'cs.CL', 'abstract': \"  The computational generation of poems is a complex task, which involves\\nseveral sound, prosodic and rhythmic resources. In this work we present PROPOE\\n2, with the extension of structural and rhythmic possibilities compared to the\\noriginal system, generating poems from metered sentences extracted from the\\nprose of Brazilian literature, with multiple rhythmic assembly criteria. These\\nadvances allow for a more coherent exploration of rhythms and sound effects for\\nthe poem. Results of poems generated by the system are demonstrated, with\\nvariations in parameters to exemplify generation and evaluation using various\\ncriteria.\\n  A gera\\\\c{c}\\\\~ao computacional de poemas \\\\'e uma tarefa complexa, que envolve\\ndiversos recursos sonoros, pros\\\\'odicos e r\\\\'itmicos. Neste trabalho\\napresentamos PROPOE 2, com a amplia\\\\c{c}\\\\~ao de possibilidades estruturais e\\nr\\\\'itmicas em rela\\\\c{c}\\\\~ao ao sistema original, gerando poemas a partir de\\nsenten\\\\c{c}as metrificadas extra\\\\'idas da prosa da literatura brasileira, com\\nm\\\\'ultiplos crit\\\\'erios r\\\\'itmicos de montagem. Esses avan\\\\c{c}os permitem uma\\nexplora\\\\c{c}\\\\~ao mais coerente de ritmos e efeitos sonoros para o poema.\\nResultados de poemas gerados pelo sistema s\\\\~ao demonstrados, com\\nvaria\\\\c{c}\\\\~oes de par\\\\^ametros para exemplificar a gera\\\\c{c}\\\\~ao e a\\navalia\\\\c{c}\\\\~ao pelos variados crit\\\\'erios.\\n\", 'publish_date': 'Mon, 16 Dec 2024 20:53:24 GMT'}\n",
      "ID: 1910.13946, Distance: 0.4063025414943695, Content: {'title': \"Let's FACE it. Finnish Poetry Generation with Aesthetics and Framing\", 'doi': None, 'categories': 'cs.CL', 'abstract': '  We present a creative poem generator for the morphologically rich Finnish\\nlanguage. Our method falls into the master-apprentice paradigm, where a\\ncomputationally creative genetic algorithm teaches a BRNN model to generate\\npoetry. We model several parts of poetic aesthetics in the fitness function of\\nthe genetic algorithm, such as sonic features, semantic coherence, imagery and\\nmetaphor. Furthermore, we justify the creativity of our method based on the\\nFACE theory on computational creativity and take additional care in evaluating\\nour system by automatic metrics for concepts together with human evaluation for\\naesthetics, framing and expressions.\\n', 'publish_date': 'Wed, 30 Oct 2019 16:00:54 GMT'}\n",
      "ID: 2002.02511, Distance: 0.41297638416290283, Content: {'title': 'Introducing Aspects of Creativity in Automatic Poetry Generation', 'doi': None, 'categories': 'cs.CL cs.AI cs.LG', 'abstract': '  Poetry Generation involves teaching systems to automatically generate text\\nthat resembles poetic work. A deep learning system can learn to generate poetry\\non its own by training on a corpus of poems and modeling the particular style\\nof language. In this paper, we propose taking an approach that fine-tunes\\nGPT-2, a pre-trained language model, to our downstream task of poetry\\ngeneration. We extend prior work on poetry generation by introducing creative\\nelements. Specifically, we generate poems that express emotion and elicit the\\nsame in readers, and poems that use the language of dreams---called dream\\npoetry. We are able to produce poems that correctly elicit the emotions of\\nsadness and joy 87.5 and 85 percent, respectively, of the time. We produce\\ndreamlike poetry by training on a corpus of texts that describe dreams. Poems\\nfrom this model are shown to capture elements of dream poetry with scores of no\\nless than 3.2 on the Likert scale. We perform crowdsourced human-evaluation for\\nall our poems. We also make use of the Coh-Metrix tool, outlining metrics we\\nuse to gauge the quality of text generated.\\n', 'publish_date': 'Thu, 6 Feb 2020 20:44:12 GMT'}\n",
      "ID: 2406.15267, Distance: 0.4135574698448181, Content: {'title': 'Evaluating Diversity in Automatic Poetry Generation', 'doi': None, 'categories': 'cs.CL', 'abstract': '  Natural Language Generation (NLG), and more generally generative AI, are\\namong the currently most impactful research fields. Creative NLG, such as\\nautomatic poetry generation, is a fascinating niche in this area. While most\\nprevious research has focused on forms of the Turing test when evaluating\\nautomatic poetry generation -- can humans distinguish between automatic and\\nhuman generated poetry -- we evaluate the diversity of automatically generated\\npoetry (with a focus on quatrains), by comparing distributions of generated\\npoetry to distributions of human poetry along structural, lexical, semantic and\\nstylistic dimensions, assessing different model types (word vs.\\ncharacter-level, general purpose LLMs vs. poetry-specific models), including\\nthe very recent LLaMA3-8B, and types of fine-tuning (conditioned vs.\\nunconditioned). We find that current automatic poetry systems are considerably\\nunderdiverse along multiple dimensions -- they often do not rhyme sufficiently,\\nare semantically too uniform and even do not match the length distribution of\\nhuman poetry. Our experiments reveal, however, that style-conditioning and\\ncharacter-level modeling clearly increases diversity across virtually all\\ndimensions we explore. Our identified limitations may serve as the basis for\\nmore genuinely diverse future poetry generation models.\\n', 'publish_date': 'Fri, 21 Jun 2024 16:03:21 GMT'}\n",
      "\n",
      "Answer:\n",
      "\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "生成嵌入中: 100%|██████████| 1/1 [00:00<00:00, 14.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "cost 0.1339 seconds\n",
      "\n",
      "\n",
      "You must reply in English. You are a professional academic assistant specializing in the cs.CL domain. Strictly based on the provided literature information, answer the user's query. You may slightly expand and refine the response using existing knowledge, but avoid fabricating information. The response language must be English. Ensure the response is concise and limited to 500 words. Include a citation with the corresponding literature link (like [text](f\"https://arxiv.org/abs/{{id}}\")) and adjust the response style based on the temperature parameter (from 0.0 to 1.0). If the temperature is low (e.g., 0.2), provide a strict and precise answer. If the temperature is high (e.g., 0.8), provide a more creative and exploratory answer.\n",
      "\n",
      "Temperature: 0.2\n",
      "Question: 在文本纠错任务中，如何提高对拼写错误、语法错误的检测和纠正能力？\n",
      "Relevant Literature: \n",
      "ID: cmp-lg/9502031, Distance: 0.3797731101512909, Content: {'title': 'Cooperative Error Handling and Shallow Processing', 'doi': None, 'categories': 'cmp-lg cs.CL', 'abstract': '  This paper is concerned with the detection and correction of sub-sentential\\nEnglish text errors. Previous spelling programs, unless restricted to a very\\nsmall set of words, have operated as post-processors. And to date, grammar\\ncheckers and other programs which deal with ill-formed input usually step\\ndirectly from spelling considerations to a full-scale parse, assuming a\\ncomplete sentence. Work described below is aimed at evaluating the\\neffectiveness of shallow (sub-sentential) processing and the feasibility of\\ncooperative error checking, through building and testing appropriately an\\nerror-processing system. A system under construction is outlined which\\nincorporates morphological checks (using new two-level error rules) over a\\ndirected letter graph, tag positional trigrams and partial parsing. Intended\\ntesting is discussed.\\n', 'publish_date': 'Thu, 23 Feb 1995 11:19:11 GMT'}\n",
      "ID: 2302.06407, Distance: 0.42555558681488037, Content: {'title': 'Correcting Real-Word Spelling Errors: A New Hybrid Approach', 'doi': '10.1093/llc/fqx054', 'categories': 'cs.CL cs.AI', 'abstract': \"  Spelling correction is one of the main tasks in the field of Natural Language\\nProcessing. Contrary to common spelling errors, real-word errors cannot be\\ndetected by conventional spelling correction methods. The real-word correction\\nmodel proposed by Mays, Damerau and Mercer showed a great performance in\\ndifferent evaluations. In this research, however, a new hybrid approach is\\nproposed which relies on statistical and syntactic knowledge to detect and\\ncorrect real-word errors. In this model, Constraint Grammar (CG) is used to\\ndiscriminate among sets of correction candidates in the search space. Mays,\\nDamerau and Mercer's trigram approach is manipulated to estimate the\\nprobability of syntactically well-formed correction candidates. The approach\\nproposed here is tested on the Wall Street Journal corpus. The model can prove\\nto be more practical than some other models, such as WordNet-based method of\\nHirst and Budanitsky and fixed windows size method of Wilcox-O'Hearn and Hirst.\\n\", 'publish_date': 'Thu, 9 Feb 2023 06:03:11 GMT'}\n",
      "ID: cmp-lg/9806010, Distance: 0.42740118503570557, Content: {'title': 'Towards a single proposal is spelling correction', 'doi': None, 'categories': 'cmp-lg cs.CL', 'abstract': '  The study presented here relies on the integrated use of different kinds of\\nknowledge in order to improve first-guess accuracy in non-word\\ncontext-sensitive correction for general unrestricted texts. State of the art\\nspelling correction systems, e.g. ispell, apart from detecting spelling errors,\\nalso assist the user by offering a set of candidate corrections that are close\\nto the misspelled word. Based on the correction proposals of ispell, we built\\nseveral guessers, which were combined in different ways. Firstly, we evaluated\\nall possibilities and selected the best ones in a corpus with artificially\\ngenerated typing errors. Secondly, the best combinations were tested on texts\\nwith genuine spelling errors. The results for the latter suggest that we can\\nexpect automatic non-word correction for all the errors in a free running text\\nwith 80% precision and a single proposal 98% of the times (1.02 proposals on\\naverage).\\n', 'publish_date': 'Mon, 15 Jun 1998 19:38:11 GMT'}\n",
      "ID: 2112.01846, Distance: 0.43136054277420044, Content: {'title': 'A Proposal of Automatic Error Correction in Text', 'doi': None, 'categories': 'cs.CL cs.AI', 'abstract': '  The great amount of information that can be stored in electronic media is\\ngrowing up daily. Many of them is got mainly by typing, such as the huge of\\ninformation obtained from web 2.0 sites; or scaned and processing by an Optical\\nCharacter Recognition software, like the texts of libraries and goverment\\noffices. Both processes introduce error in texts, so it is difficult to use the\\ndata for other purposes than just to read it, i.e. the processing of those\\ntexts by other applications like e-learning, learning of languages, electronic\\ntutorials, data minning, information retrieval and even more specialized\\nsystems such as tiflologic software, specifically blinded people-oriented\\napplications like automatic reading, where the text would be error free as\\npossible in order to make easier the text to speech task, and so on. In this\\npaper it is showed an application of automatic recognition and correction of\\nortographic errors in electronic texts. This task is composed of three stages:\\na) error detection; b) candidate corrections generation; and c) correction\\n-selection of the best candidate. The proposal is based in part of speech text\\ncategorization, word similarity, word diccionaries, statistical measures,\\nmorphologic analisys and n-grams based language model of Spanish.\\n', 'publish_date': 'Fri, 24 Sep 2021 17:17:56 GMT'}\n",
      "ID: 2205.05730, Distance: 0.43305298686027527, Content: {'title': 'Some Grammatical Errors are Frequent, Others are Important', 'doi': None, 'categories': 'cs.CL cs.AI cs.CY', 'abstract': '  In Grammatical Error Correction, systems are evaluated by the number of\\nerrors they correct. However, no one has assessed whether all error types are\\nequally important. We provide and apply a method to quantify the importance of\\ndifferent grammatical error types to humans. We show that some rare errors are\\nconsidered disturbing while other common ones are not. This affects possible\\ndirections to improve both systems and their evaluation.\\n', 'publish_date': 'Wed, 11 May 2022 18:59:20 GMT'}\n",
      "\n",
      "Answer:\n",
      "\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "生成嵌入中: 100%|██████████| 1/1 [00:00<00:00, 14.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "cost 0.1396 seconds\n",
      "\n",
      "\n",
      "You must reply in English. You are a professional academic assistant specializing in the cs.CL domain. Strictly based on the provided literature information, answer the user's query. You may slightly expand and refine the response using existing knowledge, but avoid fabricating information. The response language must be English. Ensure the response is concise and limited to 500 words. Include a citation with the corresponding literature link (like [text](f\"https://arxiv.org/abs/{{id}}\")) and adjust the response style based on the temperature parameter (from 0.0 to 1.0). If the temperature is low (e.g., 0.2), provide a strict and precise answer. If the temperature is high (e.g., 0.8), provide a more creative and exploratory answer.\n",
      "\n",
      "Temperature: 0.2\n",
      "Question: 在机器翻译中，如何更好地处理源语言与目标语言之间的语义差异和文化差异？\n",
      "Relevant Literature: \n",
      "ID: 1805.06522, Distance: 0.3782072961330414, Content: {'title': 'Semantic Relatedness for All (Languages): A Comparative Analysis of\\n  Multilingual Semantic Relatedness Using Machine Translation', 'doi': '10.1007/978-3-319-49004-5_14', 'categories': 'cs.CL', 'abstract': '  This paper provides a comparative analysis of the performance of four\\nstate-of-the-art distributional semantic models (DSMs) over 11 languages,\\ncontrasting the native language-specific models with the use of machine\\ntranslation over English-based DSMs. The experimental results show that there\\nis a significant improvement (average of 16.7% for the Spearman correlation) by\\nusing state-of-the-art machine translation approaches. The results also show\\nthat the benefit of using the most informative corpus outweighs the possible\\nerrors introduced by the machine translation. For all languages, the\\ncombination of machine translation over the Word2Vec English distributional\\nmodel provided the best results consistently (average Spearman correlation of\\n0.68).\\n', 'publish_date': 'Wed, 16 May 2018 20:43:45 GMT'}\n",
      "ID: 1609.03204, Distance: 0.3907979726791382, Content: {'title': 'On the Similarities Between Native, Non-native and Translated Texts', 'doi': None, 'categories': 'cs.CL', 'abstract': '  We present a computational analysis of three language varieties: native,\\nadvanced non-native, and translation. Our goal is to investigate the\\nsimilarities and differences between non-native language productions and\\ntranslations, contrasting both with native language. Using a collection of\\ncomputational methods we establish three main results: (1) the three types of\\ntexts are easily distinguishable; (2) non-native language and translations are\\ncloser to each other than each of them is to native language; and (3) some of\\nthese characteristics depend on the source or native language, while others do\\nnot, reflecting, perhaps, unified principles that similarly affect translations\\nand non-native language.\\n', 'publish_date': 'Sun, 11 Sep 2016 19:51:46 GMT'}\n",
      "ID: 1711.09476, Distance: 0.3975697159767151, Content: {'title': 'Machine Translation using Semantic Web Technologies: A Survey', 'doi': '10.1016/j.websem.2018.07.001', 'categories': 'cs.CL', 'abstract': '  A large number of machine translation approaches have recently been developed\\nto facilitate the fluid migration of content across languages. However, the\\nliterature suggests that many obstacles must still be dealt with to achieve\\nbetter automatic translations. One of these obstacles is lexical and syntactic\\nambiguity. A promising way of overcoming this problem is using Semantic Web\\ntechnologies. This article presents the results of a systematic review of\\nmachine translation approaches that rely on Semantic Web technologies for\\ntranslating texts. Overall, our survey suggests that while Semantic Web\\ntechnologies can enhance the quality of machine translation outputs for various\\nproblems, the combination of both is still in its infancy.\\n', 'publish_date': 'Sun, 26 Nov 2017 22:30:31 GMT'}\n",
      "ID: 1907.10676, Distance: 0.4017772078514099, Content: {'title': 'Semantic Web for Machine Translation: Challenges and Directions', 'doi': None, 'categories': 'cs.CL', 'abstract': '  A large number of machine translation approaches have recently been developed\\nto facilitate the fluid migration of content across languages. However, the\\nliterature suggests that many obstacles must still be dealt with to achieve\\nbetter automatic translations. One of these obstacles is lexical and syntactic\\nambiguity. A promising way of overcoming this problem is using Semantic Web\\ntechnologies. This article is an extended abstract of our systematic review on\\nmachine translation approaches that rely on Semantic Web technologies for\\nimproving the translation of texts. Overall, we present the challenges and\\nopportunities in the use of Semantic Web technologies in Machine Translation.\\nMoreover, our research suggests that while Semantic Web technologies can\\nenhance the quality of machine translation outputs for various problems, the\\ncombination of both is still in its infancy.\\n', 'publish_date': 'Tue, 23 Jul 2019 15:49:20 GMT'}\n",
      "ID: 2401.01419, Distance: 0.41347238421440125, Content: {'title': 'To Diverge or Not to Diverge: A Morphosyntactic Perspective on Machine\\n  Translation vs Human Translation', 'doi': None, 'categories': 'cs.CL', 'abstract': '  We conduct a large-scale fine-grained comparative analysis of machine\\ntranslations (MT) against human translations (HT) through the lens of\\nmorphosyntactic divergence. Across three language pairs and two types of\\ndivergence defined as the structural difference between the source and the\\ntarget, MT is consistently more conservative than HT, with less morphosyntactic\\ndiversity, more convergent patterns, and more one-to-one alignments. Through\\nanalysis on different decoding algorithms, we attribute this discrepancy to the\\nuse of beam search that biases MT towards more convergent patterns. This bias\\nis most amplified when the convergent pattern appears around 50% of the time in\\ntraining data. Lastly, we show that for a majority of morphosyntactic\\ndivergences, their presence in HT is correlated with decreased MT performance,\\npresenting a greater challenge for MT systems.\\n', 'publish_date': 'Tue, 2 Jan 2024 20:05:56 GMT'}\n",
      "\n",
      "Answer:\n",
      "\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "生成嵌入中: 100%|██████████| 1/1 [00:00<00:00, 12.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "cost 0.1481 seconds\n",
      "\n",
      "\n",
      "You must reply in English. You are a professional academic assistant specializing in the cs.CL domain. Strictly based on the provided literature information, answer the user's query. You may slightly expand and refine the response using existing knowledge, but avoid fabricating information. The response language must be English. Ensure the response is concise and limited to 500 words. Include a citation with the corresponding literature link (like [text](f\"https://arxiv.org/abs/{{id}}\")) and adjust the response style based on the temperature parameter (from 0.0 to 1.0). If the temperature is low (e.g., 0.2), provide a strict and precise answer. If the temperature is high (e.g., 0.8), provide a more creative and exploratory answer.\n",
      "\n",
      "Temperature: 0.2\n",
      "Question: 在文本生成任务中，如何平衡生成文本的多样性、正确性和连贯性？\n",
      "Relevant Literature: \n",
      "ID: 2004.02990, Distance: 0.3690778613090515, Content: {'title': 'Evaluating the Evaluation of Diversity in Natural Language Generation', 'doi': None, 'categories': 'cs.CL', 'abstract': '  Despite growing interest in natural language generation (NLG) models that\\nproduce diverse outputs, there is currently no principled method for evaluating\\nthe diversity of an NLG system. In this work, we propose a framework for\\nevaluating diversity metrics. The framework measures the correlation between a\\nproposed diversity metric and a diversity parameter, a single parameter that\\ncontrols some aspect of diversity in generated text. For example, a diversity\\nparameter might be a binary variable used to instruct crowdsourcing workers to\\ngenerate text with either low or high content diversity. We demonstrate the\\nutility of our framework by: (a) establishing best practices for eliciting\\ndiversity judgments from humans, (b) showing that humans substantially\\noutperform automatic metrics in estimating content diversity, and (c)\\ndemonstrating that existing methods for controlling diversity by tuning a\\n\"decoding parameter\" mostly affect form but not meaning. Our framework can\\nadvance the understanding of different diversity metrics, an essential step on\\nthe road towards better NLG systems.\\n', 'publish_date': 'Mon, 6 Apr 2020 20:44:10 GMT'}\n",
      "ID: 1904.03971, Distance: 0.3912854790687561, Content: {'title': 'Jointly Measuring Diversity and Quality in Text Generation Models', 'doi': None, 'categories': 'cs.LG cs.CL stat.ML', 'abstract': \"  Text generation is an important Natural Language Processing task with various\\napplications. Although several metrics have already been introduced to evaluate\\nthe text generation methods, each of them has its own shortcomings. The most\\nwidely used metrics such as BLEU only consider the quality of generated\\nsentences and neglect their diversity. For example, repeatedly generation of\\nonly one high quality sentence would result in a high BLEU score. On the other\\nhand, the more recent metric introduced to evaluate the diversity of generated\\ntexts known as Self-BLEU ignores the quality of generated texts. In this paper,\\nwe propose metrics to evaluate both the quality and diversity simultaneously by\\napproximating the distance of the learned generative model and the real data\\ndistribution. For this purpose, we first introduce a metric that approximates\\nthis distance using n-gram based measures. Then, a feature-based measure which\\nis based on a recent highly deep model trained on a large text corpus called\\nBERT is introduced. Finally, for oracle training mode in which the generator's\\ndensity can also be calculated, we propose to use the distance measures between\\nthe corresponding explicit distributions. Eventually, the most popular and\\nrecent text generation models are evaluated using both the existing and the\\nproposed metrics and the preferences of the proposed metrics are determined.\\n\", 'publish_date': 'Mon, 8 Apr 2019 11:44:41 GMT'}\n",
      "ID: 2205.10938, Distance: 0.4029007852077484, Content: {'title': 'Diversity Enhanced Table-to-Text Generation via Type Control', 'doi': None, 'categories': 'cs.CL', 'abstract': '  Generating natural language statements to convey logical inferences from\\ntabular data (i.e., Logical NLG) is a process with one input and a variety of\\nvalid outputs. This characteristic underscores the need for a method to produce\\na diverse set of valid outputs, presenting different perspectives of the input\\ndata. We propose a simple yet effective diversity-enhancing scheme that builds\\nupon an inherent property of the statements, their logic-types, by using a\\ntype-controlled table-to-text generation model. We demonstrate, through\\nextensive automatic and human evaluations over the two publicly available\\nLogical NLG datasets, that our proposed method both facilitates the ability to\\neffectively control the generated statement type, and produces results superior\\nto the strongest baselines in terms of quality and factuality-diversity\\ntrade-off.\\n', 'publish_date': 'Sun, 22 May 2022 22:05:21 GMT'}\n",
      "ID: 2203.15108, Distance: 0.4039303660392761, Content: {'title': 'A Well-Composed Text is Half Done! Composition Sampling for Diverse\\n  Conditional Generation', 'doi': None, 'categories': 'cs.CL', 'abstract': '  We propose Composition Sampling, a simple but effective method to generate\\ndiverse outputs for conditional generation of higher quality compared to\\nprevious stochastic decoding strategies. It builds on recently proposed\\nplan-based neural generation models (Narayan et al, 2021) that are trained to\\nfirst create a composition of the output and then generate by conditioning on\\nit and the input. Our approach avoids text degeneration by first sampling a\\ncomposition in the form of an entity chain and then using beam search to\\ngenerate the best possible text grounded to this entity chain. Experiments on\\nsummarization (CNN/DailyMail and XSum) and question generation (SQuAD), using\\nexisting and newly proposed automatic metrics together with human-based\\nevaluation, demonstrate that Composition Sampling is currently the best\\navailable decoding strategy for generating diverse meaningful outputs.\\n', 'publish_date': 'Mon, 28 Mar 2022 21:24:03 GMT'}\n",
      "ID: 2403.00553, Distance: 0.40433967113494873, Content: {'title': 'Standardizing the Measurement of Text Diversity: A Tool and a\\n  Comparative Analysis of Scores', 'doi': None, 'categories': 'cs.CL', 'abstract': \"  The diversity across outputs generated by LLMs shapes perception of their\\nquality and utility. High lexical diversity is often desirable, but there is no\\nstandard method to measure this property. Templated answer structures and\\n``canned'' responses across different documents are readily noticeable, but\\ndifficult to visualize across large corpora. This work aims to standardize\\nmeasurement of text diversity. Specifically, we empirically investigate the\\nconvergent validity of existing scores across English texts, and we release\\ndiversity, an open-source Python package for measuring and extracting\\nrepetition in text. We also build a platform based on diversity for users to\\ninteractively explore repetition in text. We find that fast compression\\nalgorithms capture information similar to what is measured by slow-to-compute\\n$n$-gram overlap homogeneity scores. Further, a combination of measures --\\ncompression ratios, self-repetition of long $n$-grams, and Self-BLEU and\\nBERTScore -- are sufficient to report, as they have low mutual correlation with\\neach other.\\n\", 'publish_date': 'Fri, 1 Mar 2024 14:23:12 GMT'}\n",
      "\n",
      "Answer:\n",
      "\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "生成嵌入中: 100%|██████████| 1/1 [00:00<00:00, 10.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "cost 0.1530 seconds\n",
      "\n",
      "\n",
      "You must reply in English. You are a professional academic assistant specializing in the cs.CL domain. Strictly based on the provided literature information, answer the user's query. You may slightly expand and refine the response using existing knowledge, but avoid fabricating information. The response language must be English. Ensure the response is concise and limited to 500 words. Include a citation with the corresponding literature link (like [text](f\"https://arxiv.org/abs/{{id}}\")) and adjust the response style based on the temperature parameter (from 0.0 to 1.0). If the temperature is low (e.g., 0.2), provide a strict and precise answer. If the temperature is high (e.g., 0.8), provide a more creative and exploratory answer.\n",
      "\n",
      "Temperature: 0.2\n",
      "Question: 在对话系统中，如何提高对用户意图的理解准确率并生成更加自然流畅的回复？\n",
      "Relevant Literature: \n",
      "ID: 1302.1380, Distance: 0.39405548572540283, Content: {'title': 'Towards the Rapid Development of a Natural Language Understanding Module', 'doi': None, 'categories': 'cs.CL', 'abstract': '  When developing a conversational agent, there is often an urgent need to have\\na prototype available in order to test the application with real users. A\\nWizard of Oz is a possibility, but sometimes the agent should be simply\\ndeployed in the environment where it will be used. Here, the agent should be\\nable to capture as many interactions as possible and to understand how people\\nreact to failure. In this paper, we focus on the rapid development of a natural\\nlanguage understanding module by non experts. Our approach follows the learning\\nparadigm and sees the process of understanding natural language as a\\nclassification problem. We test our module with a conversational agent that\\nanswers questions in the art domain. Moreover, we show how our approach can be\\nused by a natural language interface to a cinema database.\\n', 'publish_date': 'Wed, 6 Feb 2013 14:17:55 GMT'}\n",
      "ID: cmp-lg/9711008, Distance: 0.402814120054245, Content: {'title': 'On the use of expectations for detecting and repairing human-machine\\n  miscommunication', 'doi': None, 'categories': 'cmp-lg cs.CL', 'abstract': \"  In this paper I describe how miscommunication problems are dealt with in the\\nspoken language system DIALOGOS. The dialogue module of the system exploits\\ndialogic expectations in a twofold way: to model what future user utterance\\nmight be about (predictions), and to account how the user's next utterance may\\nbe related to previous ones in the ongoing interaction (pragmatic-based\\nexpectations). The analysis starts from the hypothesis that the occurrence of\\nmiscommunication is concomitant with two pragmatic phenomena: the deviation of\\nthe user from the expected behaviour and the generation of a conversational\\nimplicature. A preliminary evaluation of a large amount of interactions between\\nsubjects and DIALOGOS shows that the system performance is enhanced by the uses\\nof both predictions and pragmatic-based expectations.\\n\", 'publish_date': 'Wed, 19 Nov 1997 16:14:41 GMT'}\n",
      "ID: cs/9809022, Distance: 0.41370248794555664, Content: {'title': 'Modelling Users, Intentions, and Structure in Spoken Dialog', 'doi': None, 'categories': 'cs.CL', 'abstract': '  We outline how utterances in dialogs can be interpreted using a partial first\\norder logic. We exploit the capability of this logic to talk about the truth\\nstatus of formulae to define a notion of coherence between utterances and\\nexplain how this coherence relation can serve for the construction of AND/OR\\ntrees that represent the segmentation of the dialog. In a BDI model we\\nformalize basic assumptions about dialog and cooperative behaviour of\\nparticipants. These assumptions provide a basis for inferring speech acts from\\ncoherence relations between utterances and attitudes of dialog participants.\\nSpeech acts prove to be useful for determining dialog segments defined on the\\nnotion of completing expectations of dialog participants. Finally, we sketch\\nhow explicit segmentation signalled by cue phrases and performatives is covered\\nby our dialog model.\\n', 'publish_date': 'Thu, 17 Sep 1998 11:10:14 GMT'}\n",
      "ID: 2009.13902, Distance: 0.4164215326309204, Content: {'title': 'Utterance-level Dialogue Understanding: An Empirical Study', 'doi': None, 'categories': 'cs.CL', 'abstract': '  The recent abundance of conversational data on the Web and elsewhere calls\\nfor effective NLP systems for dialog understanding. Complete utterance-level\\nunderstanding often requires context understanding, defined by nearby\\nutterances. In recent years, a number of approaches have been proposed for\\nvarious utterance-level dialogue understanding tasks. Most of these approaches\\naccount for the context for effective understanding. In this paper, we explore\\nand quantify the role of context for different aspects of a dialogue, namely\\nemotion, intent, and dialogue act identification, using state-of-the-art dialog\\nunderstanding methods as baselines. Specifically, we employ various\\nperturbations to distort the context of a given utterance and study its impact\\non the different tasks and baselines. This provides us with insights into the\\nfundamental contextual controlling factors of different aspects of a dialogue.\\nSuch insights can inspire more effective dialogue understanding models, and\\nprovide support for future text generation approaches. The implementation\\npertaining to this work is available at\\nhttps://github.com/declare-lab/dialogue-understanding.\\n', 'publish_date': 'Tue, 29 Sep 2020 09:50:21 GMT'}\n",
      "ID: 2311.05450, Distance: 0.4228866696357727, Content: {'title': 'Cognitively Inspired Components for Social Conversational Agents', 'doi': None, 'categories': 'cs.CL cs.AI', 'abstract': '  Current conversational agents (CA) have seen improvement in conversational\\nquality in recent years due to the influence of large language models (LLMs)\\nlike GPT3. However, two key categories of problem remain. Firstly there are the\\nunique technical problems resulting from the approach taken in creating the CA,\\nsuch as scope with retrieval agents and the often nonsensical answers of former\\ngenerative agents. Secondly, humans perceive CAs as social actors, and as a\\nresult expect the CA to adhere to social convention. Failure on the part of the\\nCA in this respect can lead to a poor interaction and even the perception of\\nthreat by the user. As such, this paper presents a survey highlighting a\\npotential solution to both categories of problem through the introduction of\\ncognitively inspired additions to the CA. Through computational facsimiles of\\nsemantic and episodic memory, emotion, working memory, and the ability to\\nlearn, it is possible to address both the technical and social problems\\nencountered by CAs.\\n', 'publish_date': 'Thu, 9 Nov 2023 15:38:58 GMT'}\n",
      "\n",
      "Answer:\n",
      "\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "生成嵌入中: 100%|██████████| 1/1 [00:00<00:00, 10.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "cost 0.1588 seconds\n",
      "\n",
      "\n",
      "You must reply in English. You are a professional academic assistant specializing in the cs.CL domain. Strictly based on the provided literature information, answer the user's query. You may slightly expand and refine the response using existing knowledge, but avoid fabricating information. The response language must be English. Ensure the response is concise and limited to 500 words. Include a citation with the corresponding literature link (like [text](f\"https://arxiv.org/abs/{{id}}\")) and adjust the response style based on the temperature parameter (from 0.0 to 1.0). If the temperature is low (e.g., 0.2), provide a strict and precise answer. If the temperature is high (e.g., 0.8), provide a more creative and exploratory answer.\n",
      "\n",
      "Temperature: 0.2\n",
      "Question: 如何构建一个鲁棒的文本分类模型，以应对文本数据的噪声和不平衡问题？\n",
      "Relevant Literature: \n",
      "ID: 1904.08067, Distance: 0.4491845965385437, Content: {'title': 'Text Classification Algorithms: A Survey', 'doi': '10.3390/info10040150', 'categories': 'cs.LG cs.AI cs.CL cs.IR stat.ML', 'abstract': '  In recent years, there has been an exponential growth in the number of\\ncomplex documents and texts that require a deeper understanding of machine\\nlearning methods to be able to accurately classify texts in many applications.\\nMany machine learning approaches have achieved surpassing results in natural\\nlanguage processing. The success of these learning algorithms relies on their\\ncapacity to understand complex models and non-linear relationships within data.\\nHowever, finding suitable structures, architectures, and techniques for text\\nclassification is a challenge for researchers. In this paper, a brief overview\\nof text classification algorithms is discussed. This overview covers different\\ntext feature extractions, dimensionality reduction methods, existing algorithms\\nand techniques, and evaluations methods. Finally, the limitations of each\\ntechnique and their application in the real-world problem are discussed.\\n', 'publish_date': 'Wed, 17 Apr 2019 03:29:05 GMT'}\n",
      "ID: 1706.07912, Distance: 0.4678591787815094, Content: {'title': 'Cluster Based Symbolic Representation for Skewed Text Categorization', 'doi': '10.1007/978-981-10-4859-3_19', 'categories': 'cs.IR cs.CL', 'abstract': '  In this work, a problem associated with imbalanced text corpora is addressed.\\nA method of converting an imbalanced text corpus into a balanced one is\\npresented. The presented method employs a clustering algorithm for conversion.\\nInitially to avoid curse of dimensionality, an effective representation scheme\\nbased on term class relevancy measure is adapted, which drastically reduces the\\ndimension to the number of classes in the corpus. Subsequently, the samples of\\nlarger sized classes are grouped into a number of subclasses of smaller sizes\\nto make the entire corpus balanced. Each subclass is then given a single\\nsymbolic vector representation by the use of interval valued features. This\\nsymbolic representation in addition to being compact helps in reducing the\\nspace requirement and also the classification time. The proposed model has been\\nempirically demonstrated for its superiority on bench marking datasets viz.,\\nReuters 21578 and TDT2. Further, it has been compared against several other\\nexisting contemporary models including model based on support vector machine.\\nThe comparative analysis indicates that the proposed model outperforms the\\nother existing models.\\n', 'publish_date': 'Sat, 24 Jun 2017 06:04:21 GMT'}\n",
      "ID: 2010.02458, Distance: 0.4698503315448761, Content: {'title': 'Identifying Spurious Correlations for Robust Text Classification', 'doi': None, 'categories': 'cs.LG cs.CL cs.IR', 'abstract': '  The predictions of text classifiers are often driven by spurious correlations\\n-- e.g., the term `Spielberg\\' correlates with positively reviewed movies, even\\nthough the term itself does not semantically convey a positive sentiment. In\\nthis paper, we propose a method to distinguish spurious and genuine\\ncorrelations in text classification. We treat this as a supervised\\nclassification problem, using features derived from treatment effect estimators\\nto distinguish spurious correlations from \"genuine\" ones. Due to the generic\\nnature of these features and their small dimensionality, we find that the\\napproach works well even with limited training examples, and that it is\\npossible to transport the word classifier to new domains. Experiments on four\\ndatasets (sentiment classification and toxicity detection) suggest that using\\nthis approach to inform feature selection also leads to more robust\\nclassification, as measured by improved worst-case accuracy on the samples\\naffected by spurious correlations.\\n', 'publish_date': 'Tue, 6 Oct 2020 03:49:22 GMT'}\n",
      "ID: 1801.07875, Distance: 0.4810374975204468, Content: {'title': 'Support Vector Machine Active Learning Algorithms with\\n  Query-by-Committee versus Closest-to-Hyperplane Selection', 'doi': '10.1109/ICSC.2018.00029', 'categories': 'cs.LG cs.CL cs.IR stat.ML', 'abstract': '  This paper investigates and evaluates support vector machine active learning\\nalgorithms for use with imbalanced datasets, which commonly arise in many\\napplications such as information extraction applications. Algorithms based on\\nclosest-to-hyperplane selection and query-by-committee selection are combined\\nwith methods for addressing imbalance such as positive amplification based on\\nprevalence statistics from initial random samples. Three algorithms (ClosestPA,\\nQBagPA, and QBoostPA) are presented and carefully evaluated on datasets for\\ntext classification and relation extraction. The ClosestPA algorithm is shown\\nto consistently outperform the other two in a variety of ways and insights are\\nprovided as to why this is the case.\\n', 'publish_date': 'Wed, 24 Jan 2018 06:38:06 GMT'}\n",
      "ID: 2303.07203, Distance: 0.48456257581710815, Content: {'title': 'On the Robustness of Text Vectorizers', 'doi': None, 'categories': 'cs.CL cs.LG stat.ML', 'abstract': '  A fundamental issue in machine learning is the robustness of the model with\\nrespect to changes in the input. In natural language processing, models\\ntypically contain a first embedding layer, transforming a sequence of tokens\\ninto vector representations. While the robustness with respect to changes of\\ncontinuous inputs is well-understood, the situation is less clear when\\nconsidering discrete changes, for instance replacing a word by another in an\\ninput sentence. Our work formally proves that popular embedding schemes, such\\nas concatenation, TF-IDF, and Paragraph Vector (a.k.a. doc2vec), exhibit\\nrobustness in the H\\\\\"older or Lipschitz sense with respect to the Hamming\\ndistance. We provide quantitative bounds for these schemes and demonstrate how\\nthe constants involved are affected by the length of the document. These\\nfindings are exemplified through a series of numerical examples.\\n', 'publish_date': 'Thu, 9 Mar 2023 16:37:37 GMT'}\n",
      "\n",
      "Answer:\n",
      "\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "生成嵌入中: 100%|██████████| 1/1 [00:00<00:00, 10.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "cost 0.1778 seconds\n",
      "\n",
      "\n",
      "You must reply in English. You are a professional academic assistant specializing in the cs.CL domain. Strictly based on the provided literature information, answer the user's query. You may slightly expand and refine the response using existing knowledge, but avoid fabricating information. The response language must be English. Ensure the response is concise and limited to 500 words. Include a citation with the corresponding literature link (like [text](f\"https://arxiv.org/abs/{{id}}\")) and adjust the response style based on the temperature parameter (from 0.0 to 1.0). If the temperature is low (e.g., 0.2), provide a strict and precise answer. If the temperature is high (e.g., 0.8), provide a more creative and exploratory answer.\n",
      "\n",
      "Temperature: 0.2\n",
      "Question: 如何设计一种能够准确识别文本中隐喻表达的模型，并将其应用于情感分析或文本理解？\n",
      "Relevant Literature: \n",
      "ID: 2012.04540, Distance: 0.29042288661003113, Content: {'title': 'Improvements and Extensions on Metaphor Detection', 'doi': '10.18653/v1/2021.unimplicit-1.5', 'categories': 'cs.CL cs.LG', 'abstract': '  Metaphors are ubiquitous in human language. The metaphor detection task (MD)\\naims at detecting and interpreting metaphors from written language, which is\\ncrucial in natural language understanding (NLU) research. In this paper, we\\nintroduce a pre-trained Transformer-based model into MD. Our model outperforms\\nthe previous state-of-the-art models by large margins in our evaluations, with\\nrelative improvements on the F-1 score from 5.33% to 28.39%. Second, we extend\\nMD to a classification task about the metaphoricity of an entire piece of text\\nto make MD applicable in more general NLU scenes. Finally, we clean up the\\nimproper or outdated annotations in one of the MD benchmark datasets and\\nre-benchmark it with our Transformer-based model. This approach could be\\napplied to other existing MD datasets as well, since the metaphoricity\\nannotations in these benchmark datasets may be outdated. Future research\\nefforts are also necessary to build an up-to-date and well-annotated dataset\\nconsisting of longer and more complex texts.\\n', 'publish_date': 'Mon, 7 Dec 2020 08:17:42 GMT'}\n",
      "ID: 2305.17268, Distance: 0.3013506829738617, Content: {'title': 'Metaphor Detection via Explicit Basic Meanings Modelling', 'doi': None, 'categories': 'cs.CL', 'abstract': '  One noticeable trend in metaphor detection is the embrace of linguistic\\ntheories such as the metaphor identification procedure (MIP) for model\\narchitecture design. While MIP clearly defines that the metaphoricity of a\\nlexical unit is determined based on the contrast between its \\\\textit{contextual\\nmeaning} and its \\\\textit{basic meaning}, existing work does not strictly follow\\nthis principle, typically using the \\\\textit{aggregated meaning} to approximate\\nthe basic meaning of target words. In this paper, we propose a novel metaphor\\ndetection method, which models the basic meaning of the word based on literal\\nannotation from the training set, and then compares this with the contextual\\nmeaning in a target sentence to identify metaphors. Empirical results show that\\nour method outperforms the state-of-the-art method significantly by 1.0\\\\% in F1\\nscore. Moreover, our performance even reaches the theoretical upper bound on\\nthe VUA18 benchmark for targets with basic annotations, which demonstrates the\\nimportance of modelling basic meanings for metaphor detection.\\n', 'publish_date': 'Fri, 26 May 2023 21:25:05 GMT'}\n",
      "ID: 2009.12565, Distance: 0.3061826825141907, Content: {'title': 'Metaphor Detection using Deep Contextualized Word Embeddings', 'doi': None, 'categories': 'cs.CL cs.LG', 'abstract': '  Metaphors are ubiquitous in natural language, and their detection plays an\\nessential role in many natural language processing tasks, such as language\\nunderstanding, sentiment analysis, etc. Most existing approaches for metaphor\\ndetection rely on complex, hand-crafted and fine-tuned feature pipelines, which\\ngreatly limit their applicability. In this work, we present an end-to-end\\nmethod composed of deep contextualized word embeddings, bidirectional LSTMs and\\nmulti-head attention mechanism to address the task of automatic metaphor\\ndetection. Our method, unlike many other existing approaches, requires only the\\nraw text sequences as input features to detect the metaphoricity of a phrase.\\nWe compare the performance of our method against the existing baselines on two\\nbenchmark datasets, TroFi, and MOH-X respectively. Experimental evaluations\\nconfirm the effectiveness of our approach.\\n', 'publish_date': 'Sat, 26 Sep 2020 11:00:35 GMT'}\n",
      "ID: 2311.00790, Distance: 0.3397301435470581, Content: {'title': 'Construction Artifacts in Metaphor Identification Datasets', 'doi': None, 'categories': 'cs.CL', 'abstract': '  Metaphor identification aims at understanding whether a given expression is\\nused figuratively in context. However, in this paper we show how existing\\nmetaphor identification datasets can be gamed by fully ignoring the potential\\nmetaphorical expression or the context in which it occurs. We test this\\nhypothesis in a variety of datasets and settings, and show that metaphor\\nidentification systems based on language models without complete information\\ncan be competitive with those using the full context. This is due to the\\nconstruction procedures to build such datasets, which introduce unwanted biases\\nfor positive and negative classes. Finally, we test the same hypothesis on\\ndatasets that are carefully sampled from natural corpora and where this bias is\\nnot present, making these datasets more challenging and reliable.\\n', 'publish_date': 'Wed, 1 Nov 2023 19:21:55 GMT'}\n",
      "ID: 1808.09653, Distance: 0.34305039048194885, Content: {'title': 'Neural Metaphor Detection in Context', 'doi': None, 'categories': 'cs.CL', 'abstract': '  We present end-to-end neural models for detecting metaphorical word use in\\ncontext. We show that relatively standard BiLSTM models which operate on\\ncomplete sentences work well in this setting, in comparison to previous work\\nthat used more restricted forms of linguistic context. These models establish a\\nnew state-of-the-art on existing verb metaphor detection benchmarks, and show\\nstrong performance on jointly predicting the metaphoricity of all words in a\\nrunning text.\\n', 'publish_date': 'Wed, 29 Aug 2018 06:32:47 GMT'}\n",
      "\n",
      "Answer:\n",
      "\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "生成嵌入中: 100%|██████████| 1/1 [00:00<00:00, 11.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "cost 0.1743 seconds\n",
      "\n",
      "\n",
      "You must reply in English. You are a professional academic assistant specializing in the cs.CL domain. Strictly based on the provided literature information, answer the user's query. You may slightly expand and refine the response using existing knowledge, but avoid fabricating information. The response language must be English. Ensure the response is concise and limited to 500 words. Include a citation with the corresponding literature link (like [text](f\"https://arxiv.org/abs/{{id}}\")) and adjust the response style based on the temperature parameter (from 0.0 to 1.0). If the temperature is low (e.g., 0.2), provide a strict and precise answer. If the temperature is high (e.g., 0.8), provide a more creative and exploratory answer.\n",
      "\n",
      "Temperature: 0.2\n",
      "Question: 针对低资源语言，如何利用迁移学习来提高自然语言处理任务的性能？\n",
      "Relevant Literature: \n",
      "ID: 2208.09180, Distance: 0.3016367256641388, Content: {'title': 'Effective Transfer Learning for Low-Resource Natural Language\\n  Understanding', 'doi': None, 'categories': 'cs.CL cs.AI', 'abstract': \"  Natural language understanding (NLU) is the task of semantic decoding of\\nhuman languages by machines. NLU models rely heavily on large training data to\\nensure good performance. However, substantial languages and domains have very\\nfew data resources and domain experts. It is necessary to overcome the data\\nscarcity challenge, when very few or even zero training samples are available.\\nIn this thesis, we focus on developing cross-lingual and cross-domain methods\\nto tackle the low-resource issues. First, we propose to improve the model's\\ncross-lingual ability by focusing on the task-related keywords, enhancing the\\nmodel's robustness and regularizing the representations. We find that the\\nrepresentations for low-resource languages can be easily and greatly improved\\nby focusing on just the keywords. Second, we present Order-Reduced Modeling\\nmethods for the cross-lingual adaptation, and find that modeling partial word\\norders instead of the whole sequence can improve the robustness of the model\\nagainst word order differences between languages and task knowledge transfer to\\nlow-resource languages. Third, we propose to leverage different levels of\\ndomain-related corpora and additional masking of data in the pre-training for\\nthe cross-domain adaptation, and discover that more challenging pre-training\\ncan better address the domain discrepancy issue in the task knowledge transfer.\\nFinally, we introduce a coarse-to-fine framework, Coach, and a cross-lingual\\nand cross-domain parsing framework, X2Parser. Coach decomposes the\\nrepresentation learning process into a coarse-grained and a fine-grained\\nfeature learning, and X2Parser simplifies the hierarchical task structures into\\nflattened ones. We observe that simplifying task structures makes the\\nrepresentation learning more effective for low-resource languages and domains.\\n\", 'publish_date': 'Fri, 19 Aug 2022 06:59:00 GMT'}\n",
      "ID: 2010.03179, Distance: 0.32798051834106445, Content: {'title': 'Transfer Learning and Distant Supervision for Multilingual Transformer\\n  Models: A Study on African Languages', 'doi': None, 'categories': 'cs.CL cs.LG', 'abstract': \"  Multilingual transformer models like mBERT and XLM-RoBERTa have obtained\\ngreat improvements for many NLP tasks on a variety of languages. However,\\nrecent works also showed that results from high-resource languages could not be\\neasily transferred to realistic, low-resource scenarios. In this work, we study\\ntrends in performance for different amounts of available resources for the\\nthree African languages Hausa, isiXhosa and Yor\\\\`ub\\\\'a on both NER and topic\\nclassification. We show that in combination with transfer learning or distant\\nsupervision, these models can achieve with as little as 10 or 100 labeled\\nsentences the same performance as baselines with much more supervised training\\ndata. However, we also find settings where this does not hold. Our discussions\\nand additional experiments on assumptions such as time and hardware\\nrestrictions highlight challenges and opportunities in low-resource learning.\\n\", 'publish_date': 'Wed, 7 Oct 2020 05:23:27 GMT'}\n",
      "ID: 2309.05311, Distance: 0.3384498953819275, Content: {'title': 'Analysing Cross-Lingual Transfer in Low-Resourced African Named Entity\\n  Recognition', 'doi': None, 'categories': 'cs.CL', 'abstract': '  Transfer learning has led to large gains in performance for nearly all NLP\\ntasks while making downstream models easier and faster to train. This has also\\nbeen extended to low-resourced languages, with some success. We investigate the\\nproperties of cross-lingual transfer learning between ten low-resourced\\nlanguages, from the perspective of a named entity recognition task. We\\nspecifically investigate how much adaptive fine-tuning and the choice of\\ntransfer language affect zero-shot transfer performance. We find that models\\nthat perform well on a single language often do so at the expense of\\ngeneralising to others, while models with the best generalisation to other\\nlanguages suffer in individual language performance. Furthermore, the amount of\\ndata overlap between the source and target datasets is a better predictor of\\ntransfer performance than either the geographical or genetic distance between\\nthe languages.\\n', 'publish_date': 'Mon, 11 Sep 2023 08:56:47 GMT'}\n",
      "ID: 2211.05015, Distance: 0.33989882469177246, Content: {'title': 'Detecting Languages Unintelligible to Multilingual Models through Local\\n  Structure Probes', 'doi': None, 'categories': 'cs.CL', 'abstract': '  Providing better language tools for low-resource and endangered languages is\\nimperative for equitable growth. Recent progress with massively multilingual\\npretrained models has proven surprisingly effective at performing zero-shot\\ntransfer to a wide variety of languages. However, this transfer is not\\nuniversal, with many languages not currently understood by multilingual\\napproaches. It is estimated that only 72 languages possess a \"small set of\\nlabeled datasets\" on which we could test a model\\'s performance, the vast\\nmajority of languages not having the resources available to simply evaluate\\nperformances on. In this work, we attempt to clarify which languages do and do\\nnot currently benefit from such transfer. To that end, we develop a general\\napproach that requires only unlabelled text to detect which languages are not\\nwell understood by a cross-lingual model. Our approach is derived from the\\nhypothesis that if a model\\'s understanding is insensitive to perturbations to\\ntext in a language, it is likely to have a limited understanding of that\\nlanguage. We construct a cross-lingual sentence similarity task to evaluate our\\napproach empirically on 350, primarily low-resource, languages.\\n', 'publish_date': 'Wed, 9 Nov 2022 16:45:16 GMT'}\n",
      "ID: 2311.05741, Distance: 0.3460055887699127, Content: {'title': 'Efficiently Adapting Pretrained Language Models To New Languages', 'doi': None, 'categories': 'cs.CL cs.AI cs.LG', 'abstract': '  Recent large language models (LLM) exhibit sub-optimal performance on\\nlow-resource languages, as the training data of these models is usually\\ndominated by English and other high-resource languages. Furthermore, it is\\nchallenging to train models for low-resource languages, especially from\\nscratch, due to a lack of high quality training data. Adapting pretrained LLMs\\nreduces the need for data in the new language while also providing cross\\nlingual transfer capabilities. However, naively adapting to new languages leads\\nto catastrophic forgetting and poor tokenizer efficiency. In this work, we\\nstudy how to efficiently adapt any existing pretrained LLM to a new language\\nwithout running into these issues. In particular, we improve the encoding\\nefficiency of the tokenizer by adding new tokens from the target language and\\nstudy the data mixing recipe to mitigate forgetting. Our experiments on\\nadapting an English LLM to Hungarian and Thai show that our recipe can reach\\nbetter performance than open source models on the target language, with minimal\\nregressions on English.\\n', 'publish_date': 'Thu, 9 Nov 2023 20:59:08 GMT'}\n",
      "\n",
      "Answer:\n",
      "\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "生成嵌入中: 100%|██████████| 1/1 [00:00<00:00, 13.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "cost 0.1304 seconds\n",
      "\n",
      "\n",
      "You must reply in English. You are a professional academic assistant specializing in the cs.CL domain. Strictly based on the provided literature information, answer the user's query. You may slightly expand and refine the response using existing knowledge, but avoid fabricating information. The response language must be English. Ensure the response is concise and limited to 500 words. Include a citation with the corresponding literature link (like [text](f\"https://arxiv.org/abs/{{id}}\")) and adjust the response style based on the temperature parameter (from 0.0 to 1.0). If the temperature is low (e.g., 0.2), provide a strict and precise answer. If the temperature is high (e.g., 0.8), provide a more creative and exploratory answer.\n",
      "\n",
      "Temperature: 0.2\n",
      "Question: 在自然语言处理中，如何处理文本数据中的歧义问题？\n",
      "Relevant Literature: \n",
      "ID: cmp-lg/9406034, Distance: 0.38253289461135864, Content: {'title': 'Decision Lists for Lexical Ambiguity Resolution: Application to Accent\\n  Restoration in Spanish and French', 'doi': None, 'categories': 'cmp-lg cs.CL', 'abstract': '  This paper presents a statistical decision procedure for lexical ambiguity\\nresolution. The algorithm exploits both local syntactic patterns and more\\ndistant collocational evidence, generating an efficient, effective, and highly\\nperspicuous recipe for resolving a given ambiguity. By identifying and\\nutilizing only the single best disambiguating evidence in a target context, the\\nalgorithm avoids the problematic complex modeling of statistical dependencies.\\nAlthough directly applicable to a wide class of ambiguities, the algorithm is\\ndescribed and evaluated in a realistic case study, the problem of restoring\\nmissing accents in Spanish and French text.\\n', 'publish_date': 'Thu, 23 Jun 1994 03:34:55 GMT'}\n",
      "ID: cmp-lg/9605030, Distance: 0.4226163625717163, Content: {'title': 'Incremental Centering and Center Ambiguity', 'doi': None, 'categories': 'cmp-lg cs.CL', 'abstract': '  In this paper, we present a model of anaphor resolution within the framework\\nof the centering model. The consideration of an incremental processing mode\\nintroduces the need to manage structural ambiguity at the center level. Hence,\\nthe centering framework is further refined to account for local and global\\nparsing ambiguities which propagate up to the level of center representations,\\nyielding moderately adapted data structures for the centering algorithm.\\n', 'publish_date': 'Thu, 16 May 1996 08:10:27 GMT'}\n",
      "ID: cmp-lg/9502033, Distance: 0.4263451099395752, Content: {'title': 'An Algorithm to Co-Ordinate Anaphora Resolution and PPS Disambiguation\\n  Process', 'doi': None, 'categories': 'cmp-lg cs.CL', 'abstract': '  This paper concerns both anaphora resolution and prepositional phrase (PP)\\nattachment that are the most frequent ambiguities in natural language\\nprocessing. Several methods have been proposed to deal with each phenomenon\\nseparately, however none of proposed systems has considered the way of dealing\\nboth phenomena. We tackle this issue, proposing an algorithm to co-ordinate the\\ntreatment of these two problems efficiently, i.e., the aim is also to exploit\\nat each step all the results that each component can provide.\\n', 'publish_date': 'Fri, 24 Feb 1995 19:34:10 GMT'}\n",
      "ID: 2109.10013, Distance: 0.43988338112831116, Content: {'title': 'Negation-Instance Based Evaluation of End-to-End Negation Resolution', 'doi': None, 'categories': 'cs.CL', 'abstract': '  In this paper, we revisit the task of negation resolution, which includes the\\nsubtasks of cue detection (e.g. \"not\", \"never\") and scope resolution. In the\\ncontext of previous shared tasks, a variety of evaluation metrics have been\\nproposed. Subsequent works usually use different subsets of these, including\\nvariations and custom implementations, rendering meaningful comparisons between\\nsystems difficult. Examining the problem both from a linguistic perspective and\\nfrom a downstream viewpoint, we here argue for a negation-instance based\\napproach to evaluating negation resolution. Our proposed metrics correspond to\\nexpectations over per-instance scores and hence are intuitively interpretable.\\nTo render research comparable and to foster future work, we provide results for\\na set of current state-of-the-art systems for negation resolution on three\\nEnglish corpora, and make our implementation of the evaluation scripts publicly\\navailable.\\n', 'publish_date': 'Tue, 21 Sep 2021 07:49:41 GMT'}\n",
      "ID: 2202.12645, Distance: 0.4459282159805298, Content: {'title': 'Exploring Multi-Modal Representations for Ambiguity Detection &\\n  Coreference Resolution in the SIMMC 2.0 Challenge', 'doi': None, 'categories': 'cs.CL cs.AI', 'abstract': \"  Anaphoric expressions, such as pronouns and referential descriptions, are\\nsituated with respect to the linguistic context of prior turns, as well as, the\\nimmediate visual environment. However, a speaker's referential descriptions do\\nnot always uniquely identify the referent, leading to ambiguities in need of\\nresolution through subsequent clarificational exchanges. Thus, effective\\nAmbiguity Detection and Coreference Resolution are key to task success in\\nConversational AI. In this paper, we present models for these two tasks as part\\nof the SIMMC 2.0 Challenge (Kottur et al. 2021). Specifically, we use TOD-BERT\\nand LXMERT based models, compare them to a number of baselines and provide\\nablation experiments. Our results show that (1) language models are able to\\nexploit correlations in the data to detect ambiguity; and (2) unimodal\\ncoreference resolution models can avoid the need for a vision component,\\nthrough the use of smart object representations.\\n\", 'publish_date': 'Fri, 25 Feb 2022 12:10:02 GMT'}\n",
      "\n",
      "Answer:\n",
      "\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "生成嵌入中: 100%|██████████| 1/1 [00:00<00:00, 13.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "cost 0.1371 seconds\n",
      "\n",
      "\n",
      "You must reply in English. You are a professional academic assistant specializing in the cs.CL domain. Strictly based on the provided literature information, answer the user's query. You may slightly expand and refine the response using existing knowledge, but avoid fabricating information. The response language must be English. Ensure the response is concise and limited to 500 words. Include a citation with the corresponding literature link (like [text](f\"https://arxiv.org/abs/{{id}}\")) and adjust the response style based on the temperature parameter (from 0.0 to 1.0). If the temperature is low (e.g., 0.2), provide a strict and precise answer. If the temperature is high (e.g., 0.8), provide a more creative and exploratory answer.\n",
      "\n",
      "Temperature: 0.2\n",
      "Question: 如何提高对古文字的识别和释读能力？\n",
      "Relevant Literature: \n",
      "ID: 1501.01894, Distance: 0.428060919046402, Content: {'title': 'Quantifying Scripts: Defining metrics of characters for quantitative and\\n  descriptive analysis', 'doi': None, 'categories': 'cs.CL', 'abstract': '  Analysis of scripts plays an important role in paleography and in\\nquantitative linguistics. Especially in the field of digital paleography\\nquantitative features are much needed to differentiate glyphs. We describe an\\nelaborate set of metrics that quantify qualitative information contained in\\ncharacters and hence indirectly also quantify the scribal features. We broadly\\ndivide the metrics into several categories and describe each individual metric\\nwith its underlying qualitative significance. The metrics are largely derived\\nfrom the related area of gesture design and recognition. We also propose\\nseveral novel metrics. The proposed metrics are soundly grounded on the\\nprinciples of handwriting production and handwriting analysis. These computed\\nmetrics could serve as descriptors for scripts and also be used for comparing\\nand analyzing scripts. We illustrate some quantitative analysis based on the\\nproposed metrics by applying it to the paleographic evolution of the medieval\\nTamil script from Brahmi. We also outline future work.\\n', 'publish_date': 'Thu, 8 Jan 2015 16:12:34 GMT'}\n",
      "ID: 1702.00523, Distance: 0.435880571603775, Content: {'title': 'Deep Learning the Indus Script', 'doi': None, 'categories': 'cs.CV cs.CL cs.LG', 'abstract': '  Standardized corpora of undeciphered scripts, a necessary starting point for\\ncomputational epigraphy, requires laborious human effort for their preparation\\nfrom raw archaeological records. Automating this process through machine\\nlearning algorithms can be of significant aid to epigraphical research. Here,\\nwe take the first steps in this direction and present a deep learning pipeline\\nthat takes as input images of the undeciphered Indus script, as found in\\narchaeological artifacts, and returns as output a string of graphemes, suitable\\nfor inclusion in a standard corpus. The image is first decomposed into regions\\nusing Selective Search and these regions are classified as containing textual\\nand/or graphical information using a convolutional neural network. Regions\\nclassified as potentially containing text are hierarchically merged and trimmed\\nto remove non-textual information. The remaining textual part of the image is\\nsegmented using standard image processing techniques to isolate individual\\ngraphemes. This set is finally passed to a second convolutional neural network\\nto classify the graphemes, based on a standard corpus. The classifier can\\nidentify the presence or absence of the most frequent Indus grapheme, the \"jar\"\\nsign, with an accuracy of 92%. Our results demonstrate the great potential of\\ndeep learning approaches in computational epigraphy and, more generally, in the\\ndigital humanities.\\n', 'publish_date': 'Thu, 2 Feb 2017 01:56:22 GMT'}\n",
      "ID: 1608.02153, Distance: 0.4391481876373291, Content: {'title': 'OCR of historical printings with an application to building diachronic\\n  corpora: A case study using the RIDGES herbal corpus', 'doi': None, 'categories': 'cs.CL cs.DL', 'abstract': '  This article describes the results of a case study that applies Neural\\nNetwork-based Optical Character Recognition (OCR) to scanned images of books\\nprinted between 1487 and 1870 by training the OCR engine OCRopus\\n[@breuel2013high] on the RIDGES herbal text corpus [@OdebrechtEtAlSubmitted].\\nTraining specific OCR models was possible because the necessary *ground truth*\\nis available as error-corrected diplomatic transcriptions. The OCR results have\\nbeen evaluated for accuracy against the ground truth of unseen test sets.\\nCharacter and word accuracies (percentage of correctly recognized items) for\\nthe resulting machine-readable texts of individual documents range from 94% to\\nmore than 99% (character level) and from 76% to 97% (word level). This includes\\nthe earliest printed books, which were thought to be inaccessible by OCR\\nmethods until recently. Furthermore, OCR models trained on one part of the\\ncorpus consisting of books with different printing dates and different typesets\\n*(mixed models)* have been tested for their predictive power on the books from\\nthe other part containing yet other fonts, mostly yielding character accuracies\\nwell above 90%. It therefore seems possible to construct generalized models\\ntrained on a range of fonts that can be applied to a wide variety of historical\\nprintings still giving good results. A moderate postcorrection effort of some\\npages will then enable the training of individual models with even better\\naccuracies. Using this method, diachronic corpora including early printings can\\nbe constructed much faster and cheaper than by manual transcription. The OCR\\nmethods reported here open up the possibility of transforming our printed\\ntextual cultural heritage into electronic text by largely automatic means,\\nwhich is a prerequisite for the mass conversion of scanned books.\\n', 'publish_date': 'Sat, 6 Aug 2016 20:51:53 GMT'}\n",
      "ID: 1509.01978, Distance: 0.439435213804245, Content: {'title': 'An Approach to the Analysis of the South Slavic Medieval Labels Using\\n  Image Texture', 'doi': None, 'categories': 'cs.CV cs.AI cs.CL', 'abstract': '  The paper presents a new script classification method for the discrimination\\nof the South Slavic medieval labels. It consists in the textural analysis of\\nthe script types. In the first step, each letter is coded by the equivalent\\nscript type, which is defined by its typographical features. Obtained coded\\ntext is subjected to the run-length statistical analysis and to the adjacent\\nlocal binary pattern analysis in order to extract the features. The result\\nshows a diversity between the extracted features of the scripts, which makes\\nthe feature classification more effective. It is the basis for the\\nclassification process of the script identification by using an extension of a\\nstate-of-the-art approach for document clustering. The proposed method is\\nevaluated on an example of hand-engraved in stone and hand-printed in paper\\nlabels in old Cyrillic, angular and round Glagolitic. Experiments demonstrate\\nvery positive results, which prove the effectiveness of the proposed method.\\n', 'publish_date': 'Mon, 7 Sep 2015 10:39:20 GMT'}\n",
      "ID: 1507.04908, Distance: 0.4463350772857666, Content: {'title': 'Analysis of the South Slavic Scripts by Run-Length Features of the Image\\n  Texture', 'doi': '10.5755/j01.eee.21.4.12785', 'categories': 'cs.CV cs.CL', 'abstract': '  The paper proposes an algorithm for the script recognition based on the\\ntexture characteristics. The image texture is achieved by coding each letter\\nwith the equivalent script type (number code) according to its position in the\\ntext line. Each code is transformed into equivalent gray level pixel creating\\nan 1-D image. Then, the image texture is subjected to the run-length analysis.\\nThis analysis extracts the run-length features, which are classified to make a\\ndistinction between the scripts under consideration. In the experiment, a\\ncustom oriented database is subject to the proposed algorithm. The database\\nconsists of some text documents written in Cyrillic, Latin and Glagolitic\\nscripts. Furthermore, it is divided into training and test parts. The results\\nof the experiment show that 3 out of 5 run-length features can be used for\\neffective differentiation between the analyzed South Slavic scripts.\\n', 'publish_date': 'Fri, 17 Jul 2015 10:34:23 GMT'}\n",
      "\n",
      "Answer:\n",
      "\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "生成嵌入中: 100%|██████████| 1/1 [00:00<00:00, 12.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "cost 0.1402 seconds\n",
      "\n",
      "\n",
      "You must reply in English. You are a professional academic assistant specializing in the cs.CL domain. Strictly based on the provided literature information, answer the user's query. You may slightly expand and refine the response using existing knowledge, but avoid fabricating information. The response language must be English. Ensure the response is concise and limited to 500 words. Include a citation with the corresponding literature link (like [text](f\"https://arxiv.org/abs/{{id}}\")) and adjust the response style based on the temperature parameter (from 0.0 to 1.0). If the temperature is low (e.g., 0.2), provide a strict and precise answer. If the temperature is high (e.g., 0.8), provide a more creative and exploratory answer.\n",
      "\n",
      "Temperature: 0.2\n",
      "Question: 在文本风格迁移任务中，如何准确地将文本从一种风格转换为另一种风格？\n",
      "Relevant Literature: \n",
      "ID: 2407.14822, Distance: 0.4025307297706604, Content: {'title': 'Text Style Transfer: An Introductory Overview', 'doi': None, 'categories': 'cs.CL', 'abstract': '  Text Style Transfer (TST) is a pivotal task in natural language generation to\\nmanipulate text style attributes while preserving style-independent content.\\nThe attributes targeted in TST can vary widely, including politeness,\\nauthorship, mitigation of offensive language, modification of feelings, and\\nadjustment of text formality. TST has become a widely researched topic with\\nsubstantial advancements in recent years. This paper provides an introductory\\noverview of TST, addressing its challenges, existing approaches, datasets,\\nevaluation measures, subtasks, and applications. This fundamental overview\\nimproves understanding of the background and fundamentals of text style\\ntransfer.\\n', 'publish_date': 'Sat, 20 Jul 2024 09:54:55 GMT'}\n",
      "ID: 2010.12742, Distance: 0.40275296568870544, Content: {'title': 'Text Style Transfer: A Review and Experimental Evaluation', 'doi': None, 'categories': 'cs.CL', 'abstract': '  The stylistic properties of text have intrigued computational linguistics\\nresearchers in recent years. Specifically, researchers have investigated the\\nText Style Transfer (TST) task, which aims to change the stylistic properties\\nof the text while retaining its style independent content. Over the last few\\nyears, many novel TST algorithms have been developed, while the industry has\\nleveraged these algorithms to enable exciting TST applications. The field of\\nTST research has burgeoned because of this symbiosis. This article aims to\\nprovide a comprehensive review of recent research efforts on text style\\ntransfer. More concretely, we create a taxonomy to organize the TST models and\\nprovide a comprehensive summary of the state of the art. We review the existing\\nevaluation methodologies for TST tasks and conduct a large-scale\\nreproducibility study where we experimentally benchmark 19 state-of-the-art TST\\nalgorithms on two publicly available datasets. Finally, we expand on current\\ntrends and provide new perspectives on the new and exciting developments in the\\nTST field.\\n', 'publish_date': 'Sat, 24 Oct 2020 02:02:58 GMT'}\n",
      "ID: 2005.00136, Distance: 0.41681936383247375, Content: {'title': 'Contextual Text Style Transfer', 'doi': None, 'categories': 'cs.CL cs.LG', 'abstract': '  We introduce a new task, Contextual Text Style Transfer - translating a\\nsentence into a desired style with its surrounding context taken into account.\\nThis brings two key challenges to existing style transfer approaches: ($i$) how\\nto preserve the semantic meaning of target sentence and its consistency with\\nsurrounding context during transfer; ($ii$) how to train a robust model with\\nlimited labeled data accompanied with context. To realize high-quality style\\ntransfer with natural context preservation, we propose a Context-Aware Style\\nTransfer (CAST) model, which uses two separate encoders for each input sentence\\nand its surrounding context. A classifier is further trained to ensure\\ncontextual consistency of the generated sentence. To compensate for the lack of\\nparallel data, additional self-reconstruction and back-translation losses are\\nintroduced to leverage non-parallel data in a semi-supervised fashion. Two new\\nbenchmarks, Enron-Context and Reddit-Context, are introduced for formality and\\noffensiveness style transfer. Experimental results on these datasets\\ndemonstrate the effectiveness of the proposed CAST model over state-of-the-art\\nmethods across style accuracy, content preservation and contextual consistency\\nmetrics.\\n', 'publish_date': 'Thu, 30 Apr 2020 23:01:12 GMT'}\n",
      "ID: 2002.06525, Distance: 0.42682838439941406, Content: {'title': 'Learning to Generate Multiple Style Transfer Outputs for an Input\\n  Sentence', 'doi': None, 'categories': 'cs.CL cs.LG', 'abstract': '  Text style transfer refers to the task of rephrasing a given text in a\\ndifferent style. While various methods have been proposed to advance the state\\nof the art, they often assume the transfer output follows a delta distribution,\\nand thus their models cannot generate different style transfer results for a\\ngiven input text. To address the limitation, we propose a one-to-many text\\nstyle transfer framework. In contrast to prior works that learn a one-to-one\\nmapping that converts an input sentence to one output sentence, our approach\\nlearns a one-to-many mapping that can convert an input sentence to multiple\\ndifferent output sentences, while preserving the input content. This is\\nachieved by applying adversarial training with a latent decomposition scheme.\\nSpecifically, we decompose the latent representation of the input sentence to a\\nstyle code that captures the language style variation and a content code that\\nencodes the language style-independent content. We then combine the content\\ncode with the style code for generating a style transfer output. By combining\\nthe same content code with a different style code, we generate a different\\nstyle transfer output. Extensive experimental results with comparisons to\\nseveral text style transfer approaches on multiple public datasets using a\\ndiverse set of performance metrics validate effectiveness of the proposed\\napproach.\\n', 'publish_date': 'Sun, 16 Feb 2020 07:10:45 GMT'}\n",
      "ID: 2306.00539, Distance: 0.42831653356552124, Content: {'title': 'A Call for Standardization and Validation of Text Style Transfer\\n  Evaluation', 'doi': None, 'categories': 'cs.LG cs.CL', 'abstract': '  Text Style Transfer (TST) evaluation is, in practice, inconsistent.\\nTherefore, we conduct a meta-analysis on human and automated TST evaluation and\\nexperimentation that thoroughly examines existing literature in the field. The\\nmeta-analysis reveals a substantial standardization gap in human and automated\\nevaluation. In addition, we also find a validation gap: only few automated\\nmetrics have been validated using human experiments. To this end, we thoroughly\\nscrutinize both the standardization and validation gap and reveal the resulting\\npitfalls. This work also paves the way to close the standardization and\\nvalidation gap in TST evaluation by calling out requirements to be met by\\nfuture research.\\n', 'publish_date': 'Thu, 1 Jun 2023 10:46:08 GMT'}\n",
      "\n",
      "Answer:\n",
      "\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "生成嵌入中: 100%|██████████| 1/1 [00:00<00:00, 14.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "cost 0.1304 seconds\n",
      "\n",
      "\n",
      "You must reply in English. You are a professional academic assistant specializing in the cs.CL domain. Strictly based on the provided literature information, answer the user's query. You may slightly expand and refine the response using existing knowledge, but avoid fabricating information. The response language must be English. Ensure the response is concise and limited to 500 words. Include a citation with the corresponding literature link (like [text](f\"https://arxiv.org/abs/{{id}}\")) and adjust the response style based on the temperature parameter (from 0.0 to 1.0). If the temperature is low (e.g., 0.2), provide a strict and precise answer. If the temperature is high (e.g., 0.8), provide a more creative and exploratory answer.\n",
      "\n",
      "Temperature: 0.2\n",
      "Question: 如何利用计算语言学技术对文学作品进行风格分析？\n",
      "Relevant Literature: \n",
      "ID: 2109.00601, Distance: 0.3944563567638397, Content: {'title': 'Latin writing styles analysis with Machine Learning: New approach to old\\n  questions', 'doi': None, 'categories': 'cs.CL', 'abstract': '  In the Middle Ages texts were learned by heart and spread using oral means of\\ncommunication from generation to generation. Adaptation of the art of prose and\\npoems allowed keeping particular descriptions and compositions characteristic\\nfor many literary genres. Taking into account such a specific construction of\\nliterature composed in Latin, we can search for and indicate the probability\\npatterns of familiar sources of specific narrative texts. Consideration of\\nNatural Language Processing tools allowed us the transformation of textual\\nobjects into numerical ones and then application of machine learning algorithms\\nto extract information from the dataset. We carried out the task consisting of\\nthe practical use of those concepts and observation to create a tool for\\nanalyzing narrative texts basing on open-source databases. The tool focused on\\ncreating specific search tools resources which could enable us detailed\\nsearching throughout the text. The main objectives of the study take into\\naccount finding similarities between sentences and between documents. Next, we\\napplied machine learning algorithms on chosen texts to calculate specific\\nfeatures of them (for instance authorship or centuries) and to recognize\\nsources of anonymous texts with a certain percentage.\\n', 'publish_date': 'Wed, 1 Sep 2021 20:21:45 GMT'}\n",
      "ID: 2201.04356, Distance: 0.40893232822418213, Content: {'title': 'Computational analyses of the topics, sentiments, literariness,\\n  creativity and beauty of texts in a large Corpus of English Literature', 'doi': None, 'categories': 'cs.CL', 'abstract': \"  The Gutenberg Literary English Corpus (GLEC, Jacobs, 2018a) provides a rich\\nsource of textual data for research in digital humanities, computational\\nlinguistics or neurocognitive poetics. In this study we address differences\\namong the different literature categories in GLEC, as well as differences\\nbetween authors. We report the results of three studies providing i) topic and\\nsentiment analyses for six text categories of GLEC (i.e., children and youth,\\nessays, novels, plays, poems, stories) and its >100 authors, ii) novel measures\\nof semantic complexity as indices of the literariness, creativity and book\\nbeauty of the works in GLEC (e.g., Jane Austen's six novels), and iii) two\\nexperiments on text classification and authorship recognition using novel\\nfeatures of semantic complexity. The data on two novel measures estimating a\\ntext's literariness, intratextual variance and stepwise distance (van\\nCranenburgh et al., 2019) revealed that plays are the most literary texts in\\nGLEC, followed by poems and novels. Computation of a novel index of text\\ncreativity (Gray et al., 2016) revealed poems and plays as the most creative\\ncategories with the most creative authors all being poets (Milton, Pope, Keats,\\nByron, or Wordsworth). We also computed a novel index of perceived beauty of\\nverbal art (Kintsch, 2012) for the works in GLEC and predict that Emma is the\\ntheoretically most beautiful of Austen's novels. Finally, we demonstrate that\\nthese novel measures of semantic complexity are important features for text\\nclassification and authorship recognition with overall predictive accuracies in\\nthe range of .75 to .97. Our data pave the way for future computational and\\nempirical studies of literature or experiments in reading psychology and offer\\nmultiple baselines and benchmarks for analysing and validating other book\\ncorpora.\\n\", 'publish_date': 'Wed, 12 Jan 2022 08:16:52 GMT'}\n",
      "ID: 1901.00519, Distance: 0.41809970140457153, Content: {'title': 'Pull out all the stops: Textual analysis via punctuation sequences', 'doi': '10.1017/S0956792520000157', 'categories': 'cs.CL cs.LG physics.soc-ph', 'abstract': '  Whether enjoying the lucid prose of a favorite author or slogging through\\nsome other writer\\'s cumbersome, heavy-set prattle (full of parentheses, em\\ndashes, compound adjectives, and Oxford commas), readers will notice stylistic\\nsignatures not only in word choice and grammar, but also in punctuation itself.\\nIndeed, visual sequences of punctuation from different authors produce\\nmarvelously different (and visually striking) sequences. Punctuation is a\\nlargely overlooked stylistic feature in \"stylometry\", the quantitative analysis\\nof written text. In this paper, we examine punctuation sequences in a corpus of\\nliterary documents and ask the following questions: Are the properties of such\\nsequences a distinctive feature of different authors? Is it possible to\\ndistinguish literary genres based on their punctuation sequences? Do the\\npunctuation styles of authors evolve over time? Are we on to something\\ninteresting in trying to do stylometry without words, or are we full of sound\\nand fury (signifying nothing)?\\n', 'publish_date': 'Mon, 31 Dec 2018 18:48:20 GMT'}\n",
      "ID: 1501.00841, Distance: 0.41813206672668457, Content: {'title': 'Chasing the Ghosts of Ibsen: A computational stylistic analysis of drama\\n  in translation', 'doi': None, 'categories': 'cs.CL', 'abstract': \"  Research into the stylistic properties of translations is an issue which has\\nreceived some attention in computational stylistics. Previous work by Rybicki\\n(2006) on the distinguishing of character idiolects in the work of Polish\\nauthor Henryk Sienkiewicz and two corresponding English translations using\\nBurrow's Delta method concluded that idiolectal differences could be observed\\nin the source texts and this variation was preserved to a large degree in both\\ntranslations. This study also found that the two translations were also highly\\ndistinguishable from one another. Burrows (2002) examined English translations\\nof Juvenal also using the Delta method, results of this work suggest that some\\ntranslators are more adept at concealing their own style when translating the\\nworks of another author whereas other authors tend to imprint their own style\\nto a greater extent on the work they translate. Our work examines the writing\\nof a single author, Norwegian playwright Henrik Ibsen, and these writings\\ntranslated into both German and English from Norwegian, in an attempt to\\ninvestigate the preservation of characterization, defined here as the\\ndistinctiveness of textual contributions of characters.\\n\", 'publish_date': 'Mon, 5 Jan 2015 12:55:03 GMT'}\n",
      "ID: 1511.03053, Distance: 0.42559486627578735, Content: {'title': 'Investigating the stylistic relevance of adjective and verb simile\\n  markers', 'doi': None, 'categories': 'cs.CL', 'abstract': '  Similes play an important role in literary texts not only as rhetorical\\ndevices and as figures of speech but also because of their evocative power,\\ntheir aptness for description and the relative ease with which they can be\\ncombined with other figures of speech (Israel et al. 2004). Detecting all types\\nof simile constructions in a particular text therefore seems crucial when\\nanalysing the style of an author. Few research studies however have been\\ndedicated to the study of less prominent simile markers in fictional prose and\\ntheir relevance for stylistic studies. The present paper studies the frequency\\nof adjective and verb simile markers in a corpus of British and French novels\\nin order to determine which ones are really informative and worth including in\\na stylistic analysis. Furthermore, are those adjectives and verb simile markers\\nused differently in both languages?\\n', 'publish_date': 'Tue, 10 Nov 2015 10:33:47 GMT'}\n",
      "\n",
      "Answer:\n",
      "\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "生成嵌入中: 100%|██████████| 1/1 [00:00<00:00, 15.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "cost 0.1267 seconds\n",
      "\n",
      "\n",
      "You must reply in English. You are a professional academic assistant specializing in the cs.CL domain. Strictly based on the provided literature information, answer the user's query. You may slightly expand and refine the response using existing knowledge, but avoid fabricating information. The response language must be English. Ensure the response is concise and limited to 500 words. Include a citation with the corresponding literature link (like [text](f\"https://arxiv.org/abs/{{id}}\")) and adjust the response style based on the temperature parameter (from 0.0 to 1.0). If the temperature is low (e.g., 0.2), provide a strict and precise answer. If the temperature is high (e.g., 0.8), provide a more creative and exploratory answer.\n",
      "\n",
      "Temperature: 0.2\n",
      "Question: 如何设计一种能够准确识别文本中事件的模型？\n",
      "Relevant Literature: \n",
      "ID: 2006.10093, Distance: 0.3916696310043335, Content: {'title': 'Extensively Matching for Few-shot Learning Event Detection', 'doi': None, 'categories': 'cs.CL', 'abstract': '  Current event detection models under super-vised learning settings fail to\\ntransfer to newevent types. Few-shot learning has not beenexplored in event\\ndetection even though it al-lows a model to perform well with high\\ngener-alization on new event types. In this work, weformulate event detection\\nas a few-shot learn-ing problem to enable to extend event detec-tion to new\\nevent types. We propose two novelloss factors that matching examples in the\\nsup-port set to provide more training signals to themodel. Moreover, these\\ntraining signals can beapplied in many metric-based few-shot learn-ing models.\\nOur extensive experiments on theACE-2005 dataset (under a few-shot\\nlearningsetting) show that the proposed method can im-prove the performance of\\nfew-shot learning\\n', 'publish_date': 'Wed, 17 Jun 2020 18:30:30 GMT'}\n",
      "ID: 2002.05295, Distance: 0.3989177346229553, Content: {'title': 'Exploiting the Matching Information in the Support Set for Few Shot\\n  Event Classification', 'doi': '10.1007/978-3-030-47436-2_18', 'categories': 'cs.CL cs.LG stat.ML', 'abstract': '  The existing event classification (EC) work primarily focuseson the\\ntraditional supervised learning setting in which models are unableto extract\\nevent mentions of new/unseen event types. Few-shot learninghas not been\\ninvestigated in this area although it enables EC models toextend their\\noperation to unobserved event types. To fill in this gap, inthis work, we\\ninvestigate event classification under the few-shot learningsetting. We propose\\na novel training method for this problem that exten-sively exploit the support\\nset during the training process of a few-shotlearning model. In particular, in\\naddition to matching the query exam-ple with those in the support set for\\ntraining, we seek to further matchthe examples within the support set\\nthemselves. This method providesmore training signals for the models and can be\\napplied to every metric-learning-based few-shot learning methods. Our extensive\\nexperiments ontwo benchmark EC datasets show that the proposed method can\\nimprovethe best reported few-shot learning models by up to 10% on accuracyfor\\nevent classification\\n', 'publish_date': 'Thu, 13 Feb 2020 00:40:36 GMT'}\n",
      "ID: 1910.11368, Distance: 0.4219037890434265, Content: {'title': 'Extending Event Detection to New Types with Learning from Keywords', 'doi': None, 'categories': 'cs.LG cs.CL stat.ML', 'abstract': '  Traditional event detection classifies a word or a phrase in a given sentence\\nfor a set of predefined event types. The limitation of such predefined set is\\nthat it prevents the adaptation of the event detection models to new event\\ntypes. We study a novel formulation of event detection that describes types via\\nseveral keywords to match the contexts in documents. This facilitates the\\noperation of the models to new types. We introduce a novel feature-based\\nattention mechanism for convolutional neural networks for event detection in\\nthe new formulation. Our extensive experiments demonstrate the benefits of the\\nnew formulation for new type extension for event detection as well as the\\nproposed attention mechanism for this problem.\\n', 'publish_date': 'Thu, 24 Oct 2019 18:20:48 GMT'}\n",
      "ID: 1808.08504, Distance: 0.4275459349155426, Content: {'title': 'Event Detection with Neural Networks: A Rigorous Empirical Evaluation', 'doi': None, 'categories': 'cs.CL', 'abstract': '  Detecting events and classifying them into predefined types is an important\\nstep in knowledge extraction from natural language texts. While the neural\\nnetwork models have generally led the state-of-the-art, the differences in\\nperformance between different architectures have not been rigorously studied.\\nIn this paper we present a novel GRU-based model that combines syntactic\\ninformation along with temporal structure through an attention mechanism. We\\nshow that it is competitive with other neural network architectures through\\nempirical evaluations under different random initializations and\\ntraining-validation-test splits of ACE2005 dataset.\\n', 'publish_date': 'Sun, 26 Aug 2018 04:04:39 GMT'}\n",
      "ID: 2209.01979, Distance: 0.43190476298332214, Content: {'title': 'Few-shot Incremental Event Detection', 'doi': None, 'categories': 'cs.CL', 'abstract': '  Event detection tasks can enable the quick detection of events from texts and\\nprovide powerful support for downstream natural language processing tasks. Most\\nsuch methods can only detect a fixed set of predefined event classes. To extend\\nthem to detect a new class without losing the ability to detect old classes\\nrequires costly retraining of the model from scratch. Incremental learning can\\neffectively solve this problem, but it requires abundant data of new classes.\\nIn practice, however, the lack of high-quality labeled data of new event\\nclasses makes it difficult to obtain enough data for model training. To address\\nthe above mentioned issues, we define a new task, few-shot incremental event\\ndetection, which focuses on learning to detect a new event class with limited\\ndata, while retaining the ability to detect old classes to the extent possible.\\nWe created a benchmark dataset IFSED for the few-shot incremental event\\ndetection task based on FewEvent and propose two benchmarks, IFSED-K and\\nIFSED-KP. Experimental results show that our approach has a higher F1-score\\nthan baseline methods and is more stable.\\n', 'publish_date': 'Mon, 5 Sep 2022 14:21:26 GMT'}\n",
      "\n",
      "Answer:\n",
      "\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "生成嵌入中: 100%|██████████| 1/1 [00:00<00:00, 12.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "cost 0.1374 seconds\n",
      "\n",
      "\n",
      "You must reply in English. You are a professional academic assistant specializing in the cs.CL domain. Strictly based on the provided literature information, answer the user's query. You may slightly expand and refine the response using existing knowledge, but avoid fabricating information. The response language must be English. Ensure the response is concise and limited to 500 words. Include a citation with the corresponding literature link (like [text](f\"https://arxiv.org/abs/{{id}}\")) and adjust the response style based on the temperature parameter (from 0.0 to 1.0). If the temperature is low (e.g., 0.2), provide a strict and precise answer. If the temperature is high (e.g., 0.8), provide a more creative and exploratory answer.\n",
      "\n",
      "Temperature: 0.2\n",
      "Question: 如何通过计算语言学方法挖掘文学作品中的叙事结构和情节发展？\n",
      "Relevant Literature: \n",
      "ID: 1902.01109, Distance: 0.3887389302253723, Content: {'title': 'Strategies for Structuring Story Generation', 'doi': None, 'categories': 'cs.CL', 'abstract': '  Writers generally rely on plans or sketches to write long stories, but most\\ncurrent language models generate word by word from left to right. We explore\\ncoarse-to-fine models for creating narrative texts of several hundred words,\\nand introduce new models which decompose stories by abstracting over actions\\nand entities. The model first generates the predicate-argument structure of the\\ntext, where different mentions of the same entity are marked with placeholder\\ntokens. It then generates a surface realization of the predicate-argument\\nstructure, and finally replaces the entity placeholders with context-sensitive\\nnames and references. Human judges prefer the stories from our models to a wide\\nrange of previous approaches to hierarchical text generation. Extensive\\nanalysis shows that our methods can help improve the diversity and coherence of\\nevents and entities in generated stories.\\n', 'publish_date': 'Mon, 4 Feb 2019 10:23:39 GMT'}\n",
      "ID: 2103.12872, Distance: 0.39508679509162903, Content: {'title': 'Towards a Formal Model of Narratives', 'doi': None, 'categories': 'cs.CL cs.AI cs.LO', 'abstract': \"  In this paper, we propose the beginnings of a formal framework for modeling\\nnarrative \\\\textit{qua} narrative. Our framework affords the ability to discuss\\nkey qualities of stories and their communication, including the flow of\\ninformation from a Narrator to a Reader, the evolution of a Reader's story\\nmodel over time, and Reader uncertainty. We demonstrate its applicability to\\ncomputational narratology by giving explicit algorithms for measuring the\\naccuracy with which information was conveyed to the Reader and two novel\\nmeasurements of story coherence.\\n\", 'publish_date': 'Tue, 23 Mar 2021 22:33:23 GMT'}\n",
      "ID: 1604.03029, Distance: 0.3978700339794159, Content: {'title': 'Mapping Out Narrative Structures and Dynamics Using Networks and Textual\\n  Information', 'doi': '10.1371/journal.pone.0226025', 'categories': 'cs.CL cs.SI physics.soc-ph', 'abstract': '  Human communication is often executed in the form of a narrative, an account\\nof connected events composed of characters, actions, and settings. A coherent\\nnarrative structure is therefore a requisite for a well-formulated narrative --\\nbe it fictional or nonfictional -- for informative and effective communication,\\nopening up the possibility of a deeper understanding of a narrative by studying\\nits structural properties. In this paper we present a network-based framework\\nfor modeling and analyzing the structure of a narrative, which is further\\nexpanded by incorporating methods from computational linguistics to utilize the\\nnarrative text. Modeling a narrative as a dynamically unfolding system, we\\ncharacterize its progression via the growth patterns of the character network,\\nand use sentiment analysis and topic modeling to represent the actual content\\nof the narrative in the form of interaction maps between characters with\\nassociated sentiment values and keywords. This is a network framework advanced\\nbeyond the simple occurrence-based one most often used until now, allowing one\\nto utilize the unique characteristics of a given narrative to a high degree.\\nGiven the ubiquity and importance of narratives, such advanced network-based\\nrepresentation and analysis framework may lead to a more systematic modeling\\nand understanding of narratives for social interactions, expression of human\\nsentiments, and communication.\\n', 'publish_date': 'Thu, 24 Mar 2016 10:59:28 GMT'}\n",
      "ID: 2407.13248, Distance: 0.4000556468963623, Content: {'title': 'Are Large Language Models Capable of Generating Human-Level Narratives?', 'doi': None, 'categories': 'cs.CL', 'abstract': '  This paper investigates the capability of LLMs in storytelling, focusing on\\nnarrative development and plot progression. We introduce a novel computational\\nframework to analyze narratives through three discourse-level aspects: i) story\\narcs, ii) turning points, and iii) affective dimensions, including arousal and\\nvalence. By leveraging expert and automatic annotations, we uncover significant\\ndiscrepancies between the LLM- and human- written stories. While human-written\\nstories are suspenseful, arousing, and diverse in narrative structures, LLM\\nstories are homogeneously positive and lack tension. Next, we measure narrative\\nreasoning skills as a precursor to generative capacities, concluding that most\\nLLMs fall short of human abilities in discourse understanding. Finally, we show\\nthat explicit integration of aforementioned discourse features can enhance\\nstorytelling, as is demonstrated by over 40% improvement in neural storytelling\\nin terms of diversity, suspense, and arousal.\\n', 'publish_date': 'Thu, 18 Jul 2024 08:02:49 GMT'}\n",
      "ID: 2206.03021, Distance: 0.4123629629611969, Content: {'title': 'Plot Writing From Pre-Trained Language Models', 'doi': None, 'categories': 'cs.CL', 'abstract': \"  Pre-trained language models (PLMs) fail to generate long-form narrative text\\nbecause they do not consider global structure. As a result, the generated texts\\nare often incohesive, repetitive, or lack content. Recent work in story\\ngeneration reintroduced explicit content planning in the form of prompts,\\nkeywords, or semantic frames. Trained on large parallel corpora, these models\\ncan generate more logical event sequences and thus more contentful stories.\\nHowever, these intermediate representations are often not in natural language\\nand cannot be utilized by PLMs without fine-tuning. We propose generating story\\nplots using off-the-shelf PLMs while maintaining the benefit of content\\nplanning to generate cohesive and contentful stories. Our proposed method,\\nScratchPlot, first prompts a PLM to compose a content plan. Then, we generate\\nthe story's body and ending conditioned on the content plan. Furthermore, we\\ntake a generate-and-rank approach by using additional PLMs to rank the\\ngenerated (story, ending) pairs. We benchmark our method with various baselines\\nand achieved superior results in both human and automatic evaluation.\\n\", 'publish_date': 'Tue, 7 Jun 2022 05:30:46 GMT'}\n",
      "\n",
      "Answer:\n",
      "\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "生成嵌入中: 100%|██████████| 1/1 [00:00<00:00, 10.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "cost 0.1542 seconds\n",
      "\n",
      "\n",
      "You must reply in English. You are a professional academic assistant specializing in the cs.CL domain. Strictly based on the provided literature information, answer the user's query. You may slightly expand and refine the response using existing knowledge, but avoid fabricating information. The response language must be English. Ensure the response is concise and limited to 500 words. Include a citation with the corresponding literature link (like [text](f\"https://arxiv.org/abs/{{id}}\")) and adjust the response style based on the temperature parameter (from 0.0 to 1.0). If the temperature is low (e.g., 0.2), provide a strict and precise answer. If the temperature is high (e.g., 0.8), provide a more creative and exploratory answer.\n",
      "\n",
      "Temperature: 0.2\n",
      "Question: 如何构建一个高效的文本语义相似度计算模型，以支持信息检索和文本匹配？\n",
      "Relevant Literature: \n",
      "ID: 1903.10675, Distance: 0.37604469060897827, Content: {'title': 'Document Similarity for Texts of Varying Lengths via Hidden Topics', 'doi': None, 'categories': 'cs.CL', 'abstract': '  Measuring similarity between texts is an important task for several\\napplications. Available approaches to measure document similarity are\\ninadequate for document pairs that have non-comparable lengths, such as a long\\ndocument and its summary. This is because of the lexical, contextual and the\\nabstraction gaps between a long document of rich details and its concise\\nsummary of abstract information. In this paper, we present a document matching\\napproach to bridge this gap, by comparing the texts in a common space of hidden\\ntopics. We evaluate the matching algorithm on two matching tasks and find that\\nit consistently and widely outperforms strong baselines. We also highlight the\\nbenefits of incorporating domain knowledge to text matching.\\n', 'publish_date': 'Tue, 26 Mar 2019 04:42:17 GMT'}\n",
      "ID: 1910.09129, Distance: 0.3761413097381592, Content: {'title': 'A Comparison of Semantic Similarity Methods for Maximum Human\\n  Interpretability', 'doi': None, 'categories': 'cs.IR cs.CL cs.LG', 'abstract': \"  The inclusion of semantic information in any similarity measures improves the\\nefficiency of the similarity measure and provides human interpretable results\\nfor further analysis. The similarity calculation method that focuses on\\nfeatures related to the text's words only, will give less accurate results.\\nThis paper presents three different methods that not only focus on the text's\\nwords but also incorporates semantic information of texts in their feature\\nvector and computes semantic similarities. These methods are based on\\ncorpus-based and knowledge-based methods, which are: cosine similarity using\\ntf-idf vectors, cosine similarity using word embedding and soft cosine\\nsimilarity using word embedding. Among these three, cosine similarity using\\ntf-idf vectors performed best in finding similarities between short news texts.\\nThe similar texts given by the method are easy to interpret and can be used\\ndirectly in other information retrieval applications.\\n\", 'publish_date': 'Mon, 21 Oct 2019 03:09:02 GMT'}\n",
      "ID: 2304.01330, Distance: 0.3791707456111908, Content: {'title': 'A Comparison of Document Similarity Algorithms', 'doi': None, 'categories': 'cs.CL cs.AI', 'abstract': '  Document similarity is an important part of Natural Language Processing and\\nis most commonly used for plagiarism-detection and text summarization. Thus,\\nfinding the overall most effective document similarity algorithm could have a\\nmajor positive impact on the field of Natural Language Processing. This report\\nsets out to examine the numerous document similarity algorithms, and determine\\nwhich ones are the most useful. It addresses the most effective document\\nsimilarity algorithm by categorizing them into 3 types of document similarity\\nalgorithms: statistical algorithms, neural networks, and corpus/knowledge-based\\nalgorithms. The most effective algorithms in each category are also compared in\\nour work using a series of benchmark datasets and evaluations that test every\\npossible area that each algorithm could be used in.\\n', 'publish_date': 'Mon, 3 Apr 2023 19:50:55 GMT'}\n",
      "ID: 1910.03940, Distance: 0.3885693848133087, Content: {'title': 'Measuring Sentences Similarity: A Survey', 'doi': '10.17485/ijst/2019/v12i25/143977', 'categories': 'cs.CL cs.IR', 'abstract': '  This study is to review the approaches used for measuring sentences\\nsimilarity. Measuring similarity between natural language sentences is a\\ncrucial task for many Natural Language Processing applications such as text\\nclassification, information retrieval, question answering, and plagiarism\\ndetection. This survey classifies approaches of calculating sentences\\nsimilarity based on the adopted methodology into three categories. Word-to-word\\nbased, structure based, and vector-based are the most widely used approaches to\\nfind sentences similarity. Each approach measures relatedness between short\\ntexts based on a specific perspective. In addition, datasets that are mostly\\nused as benchmarks for evaluating techniques in this field are introduced to\\nprovide a complete view on this issue. The approaches that combine more than\\none perspective give better results. Moreover, structure based similarity that\\nmeasures similarity between sentences structures needs more investigation.\\n', 'publish_date': 'Sun, 6 Oct 2019 09:21:21 GMT'}\n",
      "ID: 1403.4024, Distance: 0.39371269941329956, Content: {'title': 'Measuring Global Similarity between Texts', 'doi': None, 'categories': 'cs.CL', 'abstract': '  We propose a new similarity measure between texts which, contrary to the\\ncurrent state-of-the-art approaches, takes a global view of the texts to be\\ncompared. We have implemented a tool to compute our textual distance and\\nconducted experiments on several corpuses of texts. The experiments show that\\nour methods can reliably identify different global types of texts.\\n', 'publish_date': 'Mon, 17 Mar 2014 08:22:54 GMT'}\n",
      "\n",
      "Answer:\n",
      "\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "生成嵌入中: 100%|██████████| 1/1 [00:00<00:00, 11.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "cost 0.1493 seconds\n",
      "\n",
      "\n",
      "You must reply in English. You are a professional academic assistant specializing in the cs.CL domain. Strictly based on the provided literature information, answer the user's query. You may slightly expand and refine the response using existing knowledge, but avoid fabricating information. The response language must be English. Ensure the response is concise and limited to 500 words. Include a citation with the corresponding literature link (like [text](f\"https://arxiv.org/abs/{{id}}\")) and adjust the response style based on the temperature parameter (from 0.0 to 1.0). If the temperature is low (e.g., 0.2), provide a strict and precise answer. If the temperature is high (e.g., 0.8), provide a more creative and exploratory answer.\n",
      "\n",
      "Temperature: 0.2\n",
      "Question: 深度学习中的 “灾难性遗忘” 在 NLP 任务中表现为何种形式？\n",
      "Relevant Literature: \n",
      "ID: 2406.04836, Distance: 0.3405640125274658, Content: {'title': 'Revisiting Catastrophic Forgetting in Large Language Model Tuning', 'doi': None, 'categories': 'cs.CL cs.AI', 'abstract': '  Catastrophic Forgetting (CF) means models forgetting previously acquired\\nknowledge when learning new data. It compromises the effectiveness of large\\nlanguage models (LLMs) during fine-tuning, yet the underlying causes have not\\nbeen thoroughly investigated. This paper takes the first step to reveal the\\ndirect link between the flatness of the model loss landscape and the extent of\\nCF in the field of LLMs. Based on this, we introduce the sharpness-aware\\nminimization to mitigate CF by flattening the loss landscape. Experiments on\\nthree widely-used fine-tuning datasets, spanning different model scales,\\ndemonstrate the effectiveness of our method in alleviating CF. Analyses show\\nthat we nicely complement the existing anti-forgetting strategies, further\\nenhancing the resistance of LLMs to CF.\\n', 'publish_date': 'Fri, 7 Jun 2024 11:09:13 GMT'}\n",
      "ID: 2203.03910, Distance: 0.3545580208301544, Content: {'title': 'Overcoming Catastrophic Forgetting beyond Continual Learning: Balanced\\n  Training for Neural Machine Translation', 'doi': None, 'categories': 'cs.CL cs.AI', 'abstract': '  Neural networks tend to gradually forget the previously learned knowledge\\nwhen learning multiple tasks sequentially from dynamic data distributions. This\\nproblem is called \\\\textit{catastrophic forgetting}, which is a fundamental\\nchallenge in the continual learning of neural networks. In this work, we\\nobserve that catastrophic forgetting not only occurs in continual learning but\\nalso affects the traditional static training. Neural networks, especially\\nneural machine translation models, suffer from catastrophic forgetting even if\\nthey learn from a static training set. To be specific, the final model pays\\nimbalanced attention to training samples, where recently exposed samples\\nattract more attention than earlier samples. The underlying cause is that\\ntraining samples do not get balanced training in each model update, so we name\\nthis problem \\\\textit{imbalanced training}. To alleviate this problem, we\\npropose Complementary Online Knowledge Distillation (COKD), which uses\\ndynamically updated teacher models trained on specific data orders to\\niteratively provide complementary knowledge to the student model. Experimental\\nresults on multiple machine translation tasks show that our method successfully\\nalleviates the problem of imbalanced training and achieves substantial\\nimprovements over strong baseline systems.\\n', 'publish_date': 'Tue, 8 Mar 2022 08:08:45 GMT'}\n",
      "ID: 2308.08747, Distance: 0.3688642680644989, Content: {'title': 'An Empirical Study of Catastrophic Forgetting in Large Language Models\\n  During Continual Fine-tuning', 'doi': None, 'categories': 'cs.CL', 'abstract': \"  Catastrophic forgetting (CF) is a phenomenon that occurs in machine learning\\nwhen a model forgets previously learned information while acquiring new\\nknowledge for achieving a satisfactory performance in downstream tasks. As\\nlarge language models (LLMs) have demonstrated remarkable performance, it is\\nintriguing to investigate whether CF exists during the continual instruction\\ntuning of LLMs. This study empirically evaluates the forgetting phenomenon in\\nLLMs' knowledge during continual instruction tuning from the perspectives of\\ndomain knowledge, reasoning, and reading comprehension. The experiments reveal\\nthat catastrophic forgetting is generally observed in LLMs ranging from 1b to\\n7b parameters. Surprisingly, as the model scale increases, the severity of\\nforgetting intensifies in such a model sale range which may result from the\\nmuch significant initial performance in the larger LLM. Comparing the\\ndecoder-only model BLOOMZ with the encoder-decoder model mT0, BLOOMZ exhibits\\nless forgetting and retains more knowledge. Interestingly, we also observe that\\nLLMs can mitigate language biases, such as gender bias, during continual\\nfine-tuning. Furthermore, our findings indicate that general instruction tuning\\ncan help alleviate the forgetting phenomenon in LLMs during subsequent\\nfine-tuning.\\n\", 'publish_date': 'Thu, 17 Aug 2023 02:53:23 GMT'}\n",
      "ID: 2502.10966, Distance: 0.3742675185203552, Content: {'title': 'Neural Networks Remember More: The Power of Parameter Isolation and\\n  Combination', 'doi': None, 'categories': 'cs.CL cs.AI', 'abstract': \"  Catastrophic forgetting is a pervasive issue for pre-trained language models\\n(PLMs) during continual learning, where models lose previously acquired\\nknowledge when sequentially trained on a series of tasks. The model's ability\\nto retain old tasks is referred to as stability, while its adaptability to new\\ntasks is called plasticity. Therefore, the key to solving this problem is to\\nfind a trade-off between the plasticity and stability of the model. To address\\nthis issue, in this paper, we propose a novel method to achieve a balance\\nbetween model stability and plasticity, thereby mitigating catastrophic\\nforgetting. More specifically, our proposed approach leverages parameter\\nisolation and a subsequent combination strategy. Initially, in the training\\nstage, the model adapts to each downstream task via a parameter isolation\\nmethod to prevent potential interference among different tasks. We then combine\\nall trained parameters, which contain acquired knowledge, using the task\\narithmetic method and finally apply them to the backbone model. Empirical\\nevaluations on continual language learning benchmarks substantiate the\\neffectiveness of our approach, revealing a marked enhancement over existing\\nstate-of-the-art approaches.\\n\", 'publish_date': 'Sun, 16 Feb 2025 02:58:57 GMT'}\n",
      "ID: 2504.01241, Distance: 0.37489748001098633, Content: {'title': 'Catastrophic Forgetting in LLMs: A Comparative Analysis Across Language\\n  Tasks', 'doi': None, 'categories': 'cs.CL', 'abstract': \"  Large Language Models (LLMs) have significantly advanced Natural Language\\nProcessing (NLP), particularly in Natural Language Understanding (NLU) tasks.\\nAs we progress toward an agentic world where LLM-based agents autonomously\\nhandle specialized tasks, it becomes crucial for these models to adapt to new\\ntasks without forgetting previously learned information - a challenge known as\\ncatastrophic forgetting. This study evaluates the continual fine-tuning of\\nvarious open-source LLMs with different parameter sizes (specifically models\\nunder 10 billion parameters) on key NLU tasks from the GLUE benchmark,\\nincluding SST-2, MRPC, CoLA, and MNLI. By employing prompt engineering and\\ntask-specific adjustments, we assess and compare the models' abilities to\\nretain prior knowledge while learning new tasks. Our results indicate that\\nmodels such as Phi-3.5-mini exhibit minimal forgetting while maintaining strong\\nlearning capabilities, making them well-suited for continual learning\\nenvironments. Additionally, models like Orca-2-7b and Qwen2.5-7B demonstrate\\nimpressive learning abilities and overall performance after fine-tuning. This\\nwork contributes to understanding catastrophic forgetting in LLMs and\\nhighlights prompting engineering to optimize model performance for continual\\nlearning scenarios.\\n\", 'publish_date': 'Tue, 1 Apr 2025 23:06:55 GMT'}\n",
      "\n",
      "Answer:\n",
      "\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "生成嵌入中: 100%|██████████| 1/1 [00:00<00:00, 12.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "cost 0.1406 seconds\n",
      "\n",
      "\n",
      "You must reply in English. You are a professional academic assistant specializing in the cs.CL domain. Strictly based on the provided literature information, answer the user's query. You may slightly expand and refine the response using existing knowledge, but avoid fabricating information. The response language must be English. Ensure the response is concise and limited to 500 words. Include a citation with the corresponding literature link (like [text](f\"https://arxiv.org/abs/{{id}}\")) and adjust the response style based on the temperature parameter (from 0.0 to 1.0). If the temperature is low (e.g., 0.2), provide a strict and precise answer. If the temperature is high (e.g., 0.8), provide a more creative and exploratory answer.\n",
      "\n",
      "Temperature: 0.2\n",
      "Question: 阅读障碍者的语言处理缺陷能否通过深度学习模型复现？\n",
      "Relevant Literature: \n",
      "ID: 2412.15785, Distance: 0.4120064973831177, Content: {'title': 'Learning from Impairment: Leveraging Insights from Clinical Linguistics\\n  in Language Modelling Research', 'doi': None, 'categories': 'cs.CL', 'abstract': '  This position paper investigates the potential of integrating insights from\\nlanguage impairment research and its clinical treatment to develop\\nhuman-inspired learning strategies and evaluation frameworks for language\\nmodels (LMs). We inspect the theoretical underpinnings underlying some\\ninfluential linguistically motivated training approaches derived from\\nneurolinguistics and, particularly, aphasiology, aimed at enhancing the\\nrecovery and generalization of linguistic skills in aphasia treatment, with a\\nprimary focus on those targeting the syntactic domain. We highlight how these\\ninsights can inform the design of rigorous assessments for LMs, specifically in\\ntheir handling of complex syntactic phenomena, as well as their implications\\nfor developing human-like learning strategies, aligning with efforts to create\\nmore sustainable and cognitively plausible natural language processing (NLP)\\nmodels.\\n', 'publish_date': 'Fri, 20 Dec 2024 10:53:21 GMT'}\n",
      "ID: 2211.05557, Distance: 0.46181079745292664, Content: {'title': 'Assistive Completion of Agrammatic Aphasic Sentences: A Transfer\\n  Learning Approach using Neurolinguistics-based Synthetic Dataset', 'doi': None, 'categories': 'q-bio.QM cs.CL', 'abstract': \"  Damage to the inferior frontal gyrus (Broca's area) can cause agrammatic\\naphasia wherein patients, although able to comprehend, lack the ability to form\\ncomplete sentences. This inability leads to communication gaps which cause\\ndifficulties in their daily lives. The usage of assistive devices can help in\\nmitigating these issues and enable the patients to communicate effectively.\\nHowever, due to lack of large scale studies of linguistic deficits in aphasia,\\nresearch on such assistive technology is relatively limited. In this work, we\\npresent two contributions that aim to re-initiate research and development in\\nthis field. Firstly, we propose a model that uses linguistic features from\\nsmall scale studies on aphasia patients and generates large scale datasets of\\nsynthetic aphasic utterances from grammatically correct datasets. We show that\\nthe mean length of utterance, the noun/verb ratio, and the simple/complex\\nsentence ratio of our synthetic datasets correspond to the reported features of\\naphasic speech. Further, we demonstrate how the synthetic datasets may be\\nutilized to develop assistive devices for aphasia patients. The pre-trained T5\\ntransformer is fine-tuned using the generated dataset to suggest 5 corrected\\nsentences given an aphasic utterance as input. We evaluate the efficacy of the\\nT5 model using the BLEU and cosine semantic similarity scores. Affirming\\nresults with BLEU score of 0.827/1.00 and semantic similarity of 0.904/1.00\\nwere obtained. These results provide a strong foundation for the concept that a\\nsynthetic dataset based on small scale studies on aphasia can be used to\\ndevelop effective assistive technology.\\n\", 'publish_date': 'Thu, 10 Nov 2022 13:24:02 GMT'}\n",
      "ID: 2110.15778, Distance: 0.4696163237094879, Content: {'title': 'Comparing Machine Learning-Centered Approaches for Forecasting Language\\n  Patterns During Frustration in Early Childhood', 'doi': None, 'categories': 'cs.CL cs.LG', 'abstract': '  When faced with self-regulation challenges, children have been known the use\\ntheir language to inhibit their emotions and behaviors. Yet, to date, there has\\nbeen a critical lack of evidence regarding what patterns in their speech\\nchildren use during these moments of frustration. In this paper, eXtreme\\nGradient Boosting, Random Forest, Long Short-Term Memory Recurrent Neural\\nNetworks, and Elastic Net Regression, have all been used to forecast these\\nlanguage patterns in children. Based on the results of a comparative analysis\\nbetween these methods, the study reveals that when dealing with\\nhigh-dimensional and dense data, with very irregular and abnormal\\ndistributions, as is the case with self-regulation patterns in children,\\ndecision tree-based algorithms are able to outperform traditional regression\\nand neural network methods in their shortcomings.\\n', 'publish_date': 'Fri, 29 Oct 2021 13:45:38 GMT'}\n",
      "ID: 2311.15054, Distance: 0.4710720181465149, Content: {'title': 'Detection of developmental language disorder in Cypriot Greek children\\n  using a neural network algorithm', 'doi': '10.1007/s41347-024-00460-4', 'categories': 'cs.CL cs.LG', 'abstract': '  Children with developmental language disorder (DLD) encounter difficulties in\\nacquiring various language structures. Early identification and intervention\\nare crucial to prevent negative long-term outcomes impacting the academic,\\nsocial, and emotional development of children. The study aims to develop an\\nautomated method for the identification of DLD using artificial intelligence,\\nspecifically a neural network machine learning algorithm. This protocol is\\napplied for the first time in a Cypriot Greek child population with DLD. The\\nneural network model was trained using perceptual and production data elicited\\nfrom 15 children with DLD and 15 healthy controls in the age range of 7;10\\nuntil 10;4. The k-fold technique was used to crossvalidate the algorithm. The\\nperformance of the model was evaluated using metrics such as accuracy,\\nprecision, recall, F1 score, and ROC/AUC curve to assess its ability to make\\naccurate predictions on a set of unseen data. The results demonstrated high\\nclassification values for all metrics, indicating the high accuracy of the\\nneural model in classifying children with DLD. Additionally, the variable\\nimportance analysis revealed that the language production skills of children\\nhad a more significant impact on the performance of the model compared to\\nperception skills. Machine learning paradigms provide effective discrimination\\nbetween children with DLD and those with TD, with the potential to enhance\\nclinical assessment and facilitate earlier and more efficient detection of the\\ndisorder.\\n', 'publish_date': 'Sat, 25 Nov 2023 15:23:46 GMT'}\n",
      "ID: 2407.11345, Distance: 0.4917356073856354, Content: {'title': 'Beyond Binary: Multiclass Paraphasia Detection with Generative\\n  Pretrained Transformers and End-to-End Models', 'doi': None, 'categories': 'cs.CL cs.SD eess.AS', 'abstract': '  Aphasia is a language disorder that can lead to speech errors known as\\nparaphasias, which involve the misuse, substitution, or invention of words.\\nAutomatic paraphasia detection can help those with Aphasia by facilitating\\nclinical assessment and treatment planning options. However, most automatic\\nparaphasia detection works have focused solely on binary detection, which\\ninvolves recognizing only the presence or absence of a paraphasia. Multiclass\\nparaphasia detection represents an unexplored area of research that focuses on\\nidentifying multiple types of paraphasias and where they occur in a given\\nspeech segment. We present novel approaches that use a generative pretrained\\ntransformer (GPT) to identify paraphasias from transcripts as well as two\\nend-to-end approaches that focus on modeling both automatic speech recognition\\n(ASR) and paraphasia classification as multiple sequences vs. a single\\nsequence. We demonstrate that a single sequence model outperforms GPT baselines\\nfor multiclass paraphasia detection.\\n', 'publish_date': 'Tue, 16 Jul 2024 03:24:51 GMT'}\n",
      "\n",
      "Answer:\n",
      "\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "生成嵌入中: 100%|██████████| 1/1 [00:00<00:00,  9.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "cost 0.1589 seconds\n",
      "\n",
      "\n",
      "You must reply in English. You are a professional academic assistant specializing in the cs.CL domain. Strictly based on the provided literature information, answer the user's query. You may slightly expand and refine the response using existing knowledge, but avoid fabricating information. The response language must be English. Ensure the response is concise and limited to 500 words. Include a citation with the corresponding literature link (like [text](f\"https://arxiv.org/abs/{{id}}\")) and adjust the response style based on the temperature parameter (from 0.0 to 1.0). If the temperature is low (e.g., 0.2), provide a strict and precise answer. If the temperature is high (e.g., 0.8), provide a more creative and exploratory answer.\n",
      "\n",
      "Temperature: 0.2\n",
      "Question: 如何进行代码生成？\n",
      "Relevant Literature: \n",
      "ID: 2204.05999, Distance: 0.3610247075557709, Content: {'title': 'InCoder: A Generative Model for Code Infilling and Synthesis', 'doi': None, 'categories': 'cs.SE cs.CL cs.LG', 'abstract': '  Code is seldom written in a single left-to-right pass and is instead\\nrepeatedly edited and refined. We introduce InCoder, a unified generative model\\nthat can perform program synthesis (via left-to-right generation) as well as\\nediting (via infilling). InCoder is trained to generate code files from a large\\ncorpus of permissively licensed code, where regions of code have been randomly\\nmasked and moved to the end of each file, allowing code infilling with\\nbidirectional context. Our model is the first generative model that is able to\\ndirectly perform zero-shot code infilling, which we evaluate on challenging\\ntasks such as type inference, comment generation, and variable re-naming. We\\nfind that the ability to condition on bidirectional context substantially\\nimproves performance on these tasks, while still performing comparably on\\nstandard program synthesis benchmarks in comparison to left-to-right only\\nmodels pretrained at similar scale. The InCoder models and code are publicly\\nreleased. https://sites.google.com/view/incoder-code-models\\n', 'publish_date': 'Tue, 12 Apr 2022 16:25:26 GMT'}\n",
      "ID: 2203.13474, Distance: 0.3621959686279297, Content: {'title': 'CodeGen: An Open Large Language Model for Code with Multi-Turn Program\\n  Synthesis', 'doi': None, 'categories': 'cs.LG cs.CL cs.PL', 'abstract': '  Program synthesis strives to generate a computer program as a solution to a\\ngiven problem specification, expressed with input-output examples or natural\\nlanguage descriptions. The prevalence of large language models advances the\\nstate-of-the-art for program synthesis, though limited training resources and\\ndata impede open access to such models. To democratize this, we train and\\nrelease a family of large language models up to 16.1B parameters, called\\nCODEGEN, on natural language and programming language data, and open source the\\ntraining library JAXFORMER. We show the utility of the trained model by\\ndemonstrating that it is competitive with the previous state-of-the-art on\\nzero-shot Python code generation on HumanEval. We further investigate the\\nmulti-step paradigm for program synthesis, where a single program is factorized\\ninto multiple prompts specifying subproblems. To this end, we construct an open\\nbenchmark, Multi-Turn Programming Benchmark (MTPB), consisting of 115 diverse\\nproblem sets that are factorized into multi-turn prompts. Our analysis on MTPB\\nshows that the same intent provided to CODEGEN in multi-turn fashion\\nsignificantly improves program synthesis over that provided as a single turn.\\nWe make the training library JAXFORMER and model checkpoints available as open\\nsource contribution: https://github.com/salesforce/CodeGen.\\n', 'publish_date': 'Fri, 25 Mar 2022 06:55:15 GMT'}\n",
      "ID: 1708.00098, Distance: 0.38170579075813293, Content: {'title': 'The Code2Text Challenge: Text Generation in Source Code Libraries', 'doi': '10.18653/v1/W17-3516', 'categories': 'cs.CL', 'abstract': '  We propose a new shared task for tactical data-to-text generation in the\\ndomain of source code libraries. Specifically, we focus on text generation of\\nfunction descriptions from example software projects. Data is drawn from\\nexisting resources used for studying the related problem of semantic parser\\ninduction (Richardson and Kuhn, 2017b; Richardson and Kuhn, 2017a), and spans a\\nwide variety of both natural languages and programming languages. In this\\npaper, we describe these existing resources, which will serve as training and\\ndevelopment data for the task, and discuss plans for building new independent\\ntest sets.\\n', 'publish_date': 'Mon, 31 Jul 2017 23:29:41 GMT'}\n",
      "ID: 2201.08810, Distance: 0.38896894454956055, Content: {'title': 'GAP-Gen: Guided Automatic Python Code Generation', 'doi': None, 'categories': 'cs.PL cs.CL cs.LG cs.SE', 'abstract': '  Automatic code generation from natural language descriptions can be highly\\nbeneficial during the process of software development. In this work, we propose\\nGAP-Gen, a Guided Automatic Python Code Generation method based on Python\\nsyntactic constraints and semantic constraints. We first introduce Python\\nsyntactic constraints in the form of Syntax-Flow, which is a simplified version\\nof Abstract Syntax Tree (AST) reducing the size and high complexity of Abstract\\nSyntax Tree but maintaining crucial syntactic information of Python code. In\\naddition to Syntax-Flow, we introduce Variable-Flow which abstracts variable\\nand function names consistently through out the code. In our work, rather than\\npretraining, we focus on modifying the finetuning process which reduces\\ncomputational requirements but retains high generation performance on automatic\\nPython code generation task. GAP-Gen fine-tunes the transformer based language\\nmodels T5 and CodeT5 using the Code-to-Docstring datasets CodeSearchNet,\\nCodeSearchNet AdvTest and Code-Docstring Corpus from EdinburghNLP. Our\\nexperiments show that GAP-Gen achieves better results on automatic Python code\\ngeneration task than previous works.\\n', 'publish_date': 'Wed, 19 Jan 2022 06:32:47 GMT'}\n",
      "ID: 2410.02749, Distance: 0.39678213000297546, Content: {'title': 'Training Language Models on Synthetic Edit Sequences Improves Code\\n  Synthesis', 'doi': None, 'categories': 'cs.LG cs.CL', 'abstract': '  Software engineers mainly write code by editing existing programs. In\\ncontrast, language models (LMs) autoregressively synthesize programs in a\\nsingle pass. One explanation for this is the scarcity of sequential edit data.\\nWhile high-quality instruction data for code synthesis is scarce, edit data for\\nsynthesis is even scarcer. To fill this gap, we develop a synthetic data\\ngeneration algorithm called LintSeq. This algorithm refactors programs into\\nsequences of synthetic edits by using a linter to procedurally sample across\\ninterdependent lines of source code. Synthetic edits sampled with LintSeq\\nreflect the syntax and semantics of their programming language. To test the\\nalgorithm, we use it to refactor a dataset of instruction + program pairs into\\ninstruction + program-diff-sequence tuples. Then, we fine-tune a series of\\nsmaller LMs ranging from 2.6B to 14B parameters on both the re-factored and\\noriginal versions of this dataset. We perform comprehensive evaluations\\ncomparing edit sequence code LMs against baselines on HumanEval, MBPP(+),\\nCodeContests, DS-1000, and BigCodeBench. We show that models fine-tuned to\\niteratively synthesize code match or outperform baselines on pass@1, and\\nexhibit better scaling across higher pass@k as a function of total test-time\\nFLOPs. Finally, we also pretrain our own tiny LMs for code understanding. We\\nshow that fine-tuning these models to synthesize code edit-by-edit results in\\nstrong performance on HumanEval and MBPP(+) compared to existing code language\\nmodels of similar scale such as CodeT5+, AlphaCode, and Codex.\\n', 'publish_date': 'Thu, 3 Oct 2024 17:57:22 GMT'}\n",
      "\n",
      "Answer:\n",
      "\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "生成嵌入中: 100%|██████████| 1/1 [00:00<00:00, 14.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "cost 0.1401 seconds\n",
      "\n",
      "\n",
      "You must reply in English. You are a professional academic assistant specializing in the cs.CL domain. Strictly based on the provided literature information, answer the user's query. You may slightly expand and refine the response using existing knowledge, but avoid fabricating information. The response language must be English. Ensure the response is concise and limited to 500 words. Include a citation with the corresponding literature link (like [text](f\"https://arxiv.org/abs/{{id}}\")) and adjust the response style based on the temperature parameter (from 0.0 to 1.0). If the temperature is low (e.g., 0.2), provide a strict and precise answer. If the temperature is high (e.g., 0.8), provide a more creative and exploratory answer.\n",
      "\n",
      "Temperature: 0.2\n",
      "Question: 语系差异显著的语言（如汉藏语系 vs. 印欧语系）在句法表征上的本质区别如何影响跨语言迁移？\n",
      "Relevant Literature: \n",
      "ID: 1906.02656, Distance: 0.34337860345840454, Content: {'title': 'Cross-Lingual Syntactic Transfer through Unsupervised Adaptation of\\n  Invertible Projections', 'doi': None, 'categories': 'cs.CL cs.LG', 'abstract': '  Cross-lingual transfer is an effective way to build syntactic analysis tools\\nin low-resource languages. However, transfer is difficult when transferring to\\ntypologically distant languages, especially when neither annotated target data\\nnor parallel corpora are available. In this paper, we focus on methods for\\ncross-lingual transfer to distant languages and propose to learn a generative\\nmodel with a structured prior that utilizes labeled source data and unlabeled\\ntarget data jointly. The parameters of source model and target model are softly\\nshared through a regularized log likelihood objective. An invertible projection\\nis employed to learn a new interlingual latent embedding space that compensates\\nfor imperfect cross-lingual word embedding input. We evaluate our method on two\\nsyntactic tasks: part-of-speech (POS) tagging and dependency parsing. On the\\nUniversal Dependency Treebanks, we use English as the only source corpus and\\ntransfer to a wide range of target languages. On the 10 languages in this\\ndataset that are distant from English, our method yields an average of 5.2%\\nabsolute improvement on POS tagging and 8.3% absolute improvement on dependency\\nparsing over a direct transfer method using state-of-the-art discriminative\\nmodels.\\n', 'publish_date': 'Thu, 6 Jun 2019 15:46:17 GMT'}\n",
      "ID: 2304.08823, Distance: 0.35134708881378174, Content: {'title': 'Transfer to a Low-Resource Language via Close Relatives: The Case Study\\n  on Faroese', 'doi': None, 'categories': 'cs.CL', 'abstract': \"  Multilingual language models have pushed state-of-the-art in cross-lingual\\nNLP transfer. The majority of zero-shot cross-lingual transfer, however, use\\none and the same massively multilingual transformer (e.g., mBERT or XLM-R) to\\ntransfer to all target languages, irrespective of their typological,\\netymological, and phylogenetic relations to other languages. In particular,\\nreadily available data and models of resource-rich sibling languages are often\\nignored. In this work, we empirically show, in a case study for Faroese -- a\\nlow-resource language from a high-resource language family -- that by\\nleveraging the phylogenetic information and departing from the\\n'one-size-fits-all' paradigm, one can improve cross-lingual transfer to\\nlow-resource languages. In particular, we leverage abundant resources of other\\nScandinavian languages (i.e., Danish, Norwegian, Swedish, and Icelandic) for\\nthe benefit of Faroese. Our evaluation results show that we can substantially\\nimprove the transfer performance to Faroese by exploiting data and models of\\nclosely-related high-resource languages. Further, we release a new web corpus\\nof Faroese and Faroese datasets for named entity recognition (NER), semantic\\ntext similarity (STS), and new language models trained on all Scandinavian\\nlanguages.\\n\", 'publish_date': 'Tue, 18 Apr 2023 08:42:38 GMT'}\n",
      "ID: 2003.14056, Distance: 0.35307958722114563, Content: {'title': 'Understanding Cross-Lingual Syntactic Transfer in Multilingual Recurrent\\n  Neural Networks', 'doi': None, 'categories': 'cs.CL cs.LG', 'abstract': '  It is now established that modern neural language models can be successfully\\ntrained on multiple languages simultaneously without changes to the underlying\\narchitecture. But what kind of knowledge is really shared among languages\\nwithin these models? Does multilingual training mostly lead to an alignment of\\nthe lexical representation spaces or does it also enable the sharing of purely\\ngrammatical knowledge? In this paper we dissect different forms of\\ncross-lingual transfer and look for its most determining factors, using a\\nvariety of models and probing tasks. We find that exposing our LMs to a related\\nlanguage does not always increase grammatical knowledge in the target language,\\nand that optimal conditions for lexical-semantic transfer may not be optimal\\nfor syntactic transfer.\\n', 'publish_date': 'Tue, 31 Mar 2020 09:48:25 GMT'}\n",
      "ID: 2503.03962, Distance: 0.35709017515182495, Content: {'title': 'On the Acquisition of Shared Grammatical Representations in Bilingual\\n  Language Models', 'doi': None, 'categories': 'cs.CL', 'abstract': \"  While crosslingual transfer is crucial to contemporary language models'\\nmultilingual capabilities, how it occurs is not well understood. In this paper,\\nwe ask what happens to a monolingual language model when it begins to be\\ntrained on a second language. Specifically, we train small bilingual models for\\nwhich we control the amount of data for each language and the order of language\\nexposure. To find evidence of shared multilingual representations, we turn to\\nstructural priming, a method used to study grammatical representations in\\nhumans. We first replicate previous crosslingual structural priming results and\\nfind that after controlling for training data quantity and language exposure,\\nthere are asymmetrical effects across language pairs and directions. We argue\\nthat this asymmetry may shape hypotheses about human structural priming\\neffects. We also find that structural priming effects are less robust for less\\nsimilar language pairs, highlighting potential limitations of crosslingual\\ntransfer learning and shared representations for typologically diverse\\nlanguages.\\n\", 'publish_date': 'Wed, 5 Mar 2025 23:27:58 GMT'}\n",
      "ID: 1811.00570, Distance: 0.36333853006362915, Content: {'title': 'On Difficulties of Cross-Lingual Transfer with Order Differences: A Case\\n  Study on Dependency Parsing', 'doi': None, 'categories': 'cs.CL cs.LG', 'abstract': '  Different languages might have different word orders. In this paper, we\\ninvestigate cross-lingual transfer and posit that an order-agnostic model will\\nperform better when transferring to distant foreign languages. To test our\\nhypothesis, we train dependency parsers on an English corpus and evaluate their\\ntransfer performance on 30 other languages. Specifically, we compare encoders\\nand decoders based on Recurrent Neural Networks (RNNs) and modified\\nself-attentive architectures. The former relies on sequential information while\\nthe latter is more flexible at modeling word order. Rigorous experiments and\\ndetailed analysis shows that RNN-based architectures transfer well to languages\\nthat are close to English, while self-attentive models have better overall\\ncross-lingual transferability and perform especially well on distant languages.\\n', 'publish_date': 'Thu, 1 Nov 2018 18:11:01 GMT'}\n",
      "\n",
      "Answer:\n",
      "\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "生成嵌入中: 100%|██████████| 1/1 [00:00<00:00,  7.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "cost 0.1851 seconds\n",
      "\n",
      "\n",
      "You must reply in English. You are a professional academic assistant specializing in the cs.CL domain. Strictly based on the provided literature information, answer the user's query. You may slightly expand and refine the response using existing knowledge, but avoid fabricating information. The response language must be English. Ensure the response is concise and limited to 500 words. Include a citation with the corresponding literature link (like [text](f\"https://arxiv.org/abs/{{id}}\")) and adjust the response style based on the temperature parameter (from 0.0 to 1.0). If the temperature is low (e.g., 0.2), provide a strict and precise answer. If the temperature is high (e.g., 0.8), provide a more creative and exploratory answer.\n",
      "\n",
      "Temperature: 0.2\n",
      "Question: 在多语言文本分类任务中，如何利用多任务学习来提高分类性能？\n",
      "Relevant Literature: \n",
      "ID: 1812.09617, Distance: 0.3566693365573883, Content: {'title': 'Exploiting Cross-Lingual Subword Similarities in Low-Resource Document\\n  Classification', 'doi': None, 'categories': 'cs.CL cs.LG', 'abstract': '  Text classification must sometimes be applied in a low-resource language with\\nno labeled training data. However, training data may be available in a related\\nlanguage. We investigate whether character-level knowledge transfer from a\\nrelated language helps text classification. We present a cross-lingual document\\nclassification framework (CACO) that exploits cross-lingual subword similarity\\nby jointly training a character-based embedder and a word-based classifier. The\\nembedder derives vector representations for input words from their written\\nforms, and the classifier makes predictions based on the word vectors. We use a\\njoint character representation for both the source language and the target\\nlanguage, which allows the embedder to generalize knowledge about source\\nlanguage words to target language words with similar forms. We propose a\\nmulti-task objective that can further improve the model if additional\\ncross-lingual or monolingual resources are available. Experiments confirm that\\ncharacter-level knowledge transfer is more data-efficient than word-level\\ntransfer between related languages.\\n', 'publish_date': 'Sat, 22 Dec 2018 22:53:19 GMT'}\n",
      "ID: 1906.09543, Distance: 0.36465710401535034, Content: {'title': 'Cross-lingual Data Transformation and Combination for Text\\n  Classification', 'doi': None, 'categories': 'cs.IR cs.CL', 'abstract': '  Text classification is a fundamental task for text data mining. In order to\\ntrain a generalizable model, a large volume of text must be collected. To\\naddress data insufficiency, cross-lingual data may occasionally be necessary.\\nCross-lingual data sources may however suffer from data incompatibility, as\\ntext written in different languages can hold distinct word sequences and\\nsemantic patterns. Machine translation and word embedding alignment provide an\\neffective way to transform and combine data for cross-lingual data training. To\\nthe best of our knowledge, there has been little work done on evaluating how\\nthe methodology used to conduct semantic space transformation and data\\ncombination affects the performance of classification models trained from\\ncross-lingual resources. In this paper, we systematically evaluated the\\nperformance of two commonly used CNN (Convolutional Neural Network) and RNN\\n(Recurrent Neural Network) text classifiers with differing data transformation\\nand combination strategies. Monolingual models were trained from English and\\nFrench alongside their translated and aligned embeddings. Our results suggested\\nthat semantic space transformation may conditionally promote the performance of\\nmonolingual models. Bilingual models were trained from a combination of both\\nEnglish and French. Our results indicate that a cross-lingual classification\\nmodel can significantly benefit from cross-lingual data by learning from\\ntranslated or aligned embedding spaces.\\n', 'publish_date': 'Sun, 23 Jun 2019 02:56:02 GMT'}\n",
      "ID: 2010.02562, Distance: 0.36618563532829285, Content: {'title': 'Cross-Lingual Text Classification with Minimal Resources by Transferring\\n  a Sparse Teacher', 'doi': None, 'categories': 'cs.CL', 'abstract': '  Cross-lingual text classification alleviates the need for manually labeled\\ndocuments in a target language by leveraging labeled documents from other\\nlanguages. Existing approaches for transferring supervision across languages\\nrequire expensive cross-lingual resources, such as parallel corpora, while less\\nexpensive cross-lingual representation learning approaches train classifiers\\nwithout target labeled documents. In this work, we propose a cross-lingual\\nteacher-student method, CLTS, that generates \"weak\" supervision in the target\\nlanguage using minimal cross-lingual resources, in the form of a small number\\nof word translations. Given a limited translation budget, CLTS extracts and\\ntransfers only the most important task-specific seed words across languages and\\ninitializes a teacher classifier based on the translated seed words. Then, CLTS\\niteratively trains a more powerful student that also exploits the context of\\nthe seed words in unlabeled target documents and outperforms the teacher. CLTS\\nis simple and surprisingly effective in 18 diverse languages: by transferring\\njust 20 seed words, even a bag-of-words logistic regression student outperforms\\nstate-of-the-art cross-lingual methods (e.g., based on multilingual BERT).\\nMoreover, CLTS can accommodate any type of student classifier: leveraging a\\nmonolingual BERT student leads to further improvements and outperforms even\\nmore expensive approaches by up to 12% in accuracy. Finally, CLTS addresses\\nemerging tasks in low-resource languages using just a small number of word\\ntranslations.\\n', 'publish_date': 'Tue, 6 Oct 2020 09:11:02 GMT'}\n",
      "ID: 1805.09821, Distance: 0.3667503893375397, Content: {'title': 'A Corpus for Multilingual Document Classification in Eight Languages', 'doi': None, 'categories': 'cs.CL', 'abstract': '  Cross-lingual document classification aims at training a document classifier\\non resources in one language and transferring it to a different language\\nwithout any additional resources. Several approaches have been proposed in the\\nliterature and the current best practice is to evaluate them on a subset of the\\nReuters Corpus Volume 2. However, this subset covers only few languages\\n(English, German, French and Spanish) and almost all published works focus on\\nthe the transfer between English and German. In addition, we have observed that\\nthe class prior distributions differ significantly between the languages. We\\nargue that this complicates the evaluation of the multilinguality. In this\\npaper, we propose a new subset of the Reuters corpus with balanced class priors\\nfor eight languages. By adding Italian, Russian, Japanese and Chinese, we cover\\nlanguages which are very different with respect to syntax, morphology, etc. We\\nprovide strong baselines for all language transfer directions using\\nmultilingual word and sentence embeddings respectively. Our goal is to offer a\\nfreely available framework to evaluate cross-lingual document classification,\\nand we hope to foster by these means, research in this important area.\\n', 'publish_date': 'Thu, 24 May 2018 10:36:20 GMT'}\n",
      "ID: 2402.17016, Distance: 0.3677421808242798, Content: {'title': 'Multi-Task Contrastive Learning for 8192-Token Bilingual Text Embeddings', 'doi': None, 'categories': 'cs.CL cs.AI cs.IR', 'abstract': '  We introduce a novel suite of state-of-the-art bilingual text embedding\\nmodels that are designed to support English and another target language. These\\nmodels are capable of processing lengthy text inputs with up to 8192 tokens,\\nmaking them highly versatile for a range of natural language processing tasks\\nsuch as text retrieval, clustering, and semantic textual similarity (STS)\\ncalculations.\\n  By focusing on bilingual models and introducing a unique multi-task learning\\nobjective, we have significantly improved the model performance on STS tasks,\\nwhich outperforms the capabilities of existing multilingual models in both\\ntarget language understanding and cross-lingual evaluation tasks. Moreover, our\\nbilingual models are more efficient, requiring fewer parameters and less memory\\ndue to their smaller vocabulary needs. Furthermore, we have expanded the\\nMassive Text Embedding Benchmark (MTEB) to include benchmarks for German and\\nSpanish embedding models. This integration aims to stimulate further research\\nand advancement in text embedding technologies for these languages.\\n', 'publish_date': 'Mon, 26 Feb 2024 20:53:12 GMT'}\n",
      "\n",
      "Answer:\n",
      "\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "生成嵌入中: 100%|██████████| 1/1 [00:00<00:00, 12.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "cost 0.1536 seconds\n",
      "\n",
      "\n",
      "You must reply in English. You are a professional academic assistant specializing in the cs.CL domain. Strictly based on the provided literature information, answer the user's query. You may slightly expand and refine the response using existing knowledge, but avoid fabricating information. The response language must be English. Ensure the response is concise and limited to 500 words. Include a citation with the corresponding literature link (like [text](f\"https://arxiv.org/abs/{{id}}\")) and adjust the response style based on the temperature parameter (from 0.0 to 1.0). If the temperature is low (e.g., 0.2), provide a strict and precise answer. If the temperature is high (e.g., 0.8), provide a more creative and exploratory answer.\n",
      "\n",
      "Temperature: 0.2\n",
      "Question: 如何区分大语言模型生成文本与人类文本？\n",
      "Relevant Literature: \n",
      "ID: 1603.07771, Distance: 0.3722044825553894, Content: {'title': 'Neural Text Generation from Structured Data with Application to the\\n  Biography Domain', 'doi': None, 'categories': 'cs.CL', 'abstract': '  This paper introduces a neural model for concept-to-text generation that\\nscales to large, rich domains. We experiment with a new dataset of biographies\\nfrom Wikipedia that is an order of magnitude larger than existing resources\\nwith over 700k samples. The dataset is also vastly more diverse with a 400k\\nvocabulary, compared to a few hundred words for Weathergov or Robocup. Our\\nmodel builds upon recent work on conditional neural language model for text\\ngeneration. To deal with the large vocabulary, we extend these models to mix a\\nfixed vocabulary with copy actions that transfer sample-specific words from the\\ninput database to the generated output sentence. Our neural model significantly\\nout-performs a classical Kneser-Ney language model adapted to this task by\\nnearly 15 BLEU.\\n', 'publish_date': 'Thu, 24 Mar 2016 22:40:00 GMT'}\n",
      "ID: 2102.02723, Distance: 0.4088231325149536, Content: {'title': 'Data-to-text Generation with Macro Planning', 'doi': None, 'categories': 'cs.CL', 'abstract': '  Recent approaches to data-to-text generation have adopted the very successful\\nencoder-decoder architecture or variants thereof. These models generate text\\nwhich is fluent (but often imprecise) and perform quite poorly at selecting\\nappropriate content and ordering it coherently. To overcome some of these\\nissues, we propose a neural model with a macro planning stage followed by a\\ngeneration stage reminiscent of traditional methods which embrace separate\\nmodules for planning and surface realization. Macro plans represent high level\\norganization of important content such as entities, events and their\\ninteractions; they are learnt from data and given as input to the generator.\\nExtensive experiments on two data-to-text benchmarks (RotoWire and MLB) show\\nthat our approach outperforms competitive baselines in terms of automatic and\\nhuman evaluation.\\n', 'publish_date': 'Thu, 4 Feb 2021 16:32:57 GMT'}\n",
      "ID: 2004.10188, Distance: 0.40919992327690125, Content: {'title': 'Residual Energy-Based Models for Text', 'doi': None, 'categories': 'cs.CL cs.LG stat.ML', 'abstract': '  Current large-scale auto-regressive language models display impressive\\nfluency and can generate convincing text. In this work we start by asking the\\nquestion: Can the generations of these models be reliably distinguished from\\nreal text by statistical discriminators? We find experimentally that the answer\\nis affirmative when we have access to the training data for the model, and\\nguardedly affirmative even if we do not.\\n  This suggests that the auto-regressive models can be improved by\\nincorporating the (globally normalized) discriminators into the generative\\nprocess. We give a formalism for this using the Energy-Based Model framework,\\nand show that it indeed improves the results of the generative models, measured\\nboth in terms of perplexity and in terms of human evaluation.\\n', 'publish_date': 'Mon, 6 Apr 2020 13:44:03 GMT'}\n",
      "ID: 2504.08697, Distance: 0.40951859951019287, Content: {'title': 'Large Language Models as Span Annotators', 'doi': None, 'categories': 'cs.CL', 'abstract': '  For high-quality texts, single-score metrics seldom provide actionable\\nfeedback. In contrast, span annotation - pointing out issues in the text by\\nannotating their spans - can guide improvements and provide insights. Until\\nrecently, span annotation was limited to human annotators or fine-tuned encoder\\nmodels. In this study, we automate span annotation with large language models\\n(LLMs). We compare expert or skilled crowdworker annotators with open and\\nproprietary LLMs on three tasks: data-to-text generation evaluation, machine\\ntranslation evaluation, and propaganda detection in human-written texts. In our\\nexperiments, we show that LLMs as span annotators are straightforward to\\nimplement and notably more cost-efficient than human annotators. The LLMs\\nachieve moderate agreement with skilled human annotators, in some scenarios\\ncomparable to the average agreement among the annotators themselves.\\nQualitative analysis shows that reasoning models outperform their\\ninstruction-tuned counterparts and provide more valid explanations for\\nannotations. We release the dataset of more than 40k model and human\\nannotations for further research.\\n', 'publish_date': 'Fri, 11 Apr 2025 17:04:51 GMT'}\n",
      "ID: 1707.08052, Distance: 0.41169100999832153, Content: {'title': 'Challenges in Data-to-Document Generation', 'doi': None, 'categories': 'cs.CL', 'abstract': '  Recent neural models have shown significant progress on the problem of\\ngenerating short descriptive texts conditioned on a small number of database\\nrecords. In this work, we suggest a slightly more difficult data-to-text\\ngeneration task, and investigate how effective current approaches are on this\\ntask. In particular, we introduce a new, large-scale corpus of data records\\npaired with descriptive documents, propose a series of extractive evaluation\\nmethods for analyzing performance, and obtain baseline results using current\\nneural generation methods. Experiments show that these models produce fluent\\ntext, but fail to convincingly approximate human-generated documents. Moreover,\\neven templated baselines exceed the performance of these neural models on some\\nmetrics, though copy- and reconstruction-based extensions lead to noticeable\\nimprovements.\\n', 'publish_date': 'Tue, 25 Jul 2017 15:42:25 GMT'}\n",
      "\n",
      "Answer:\n",
      "\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "生成嵌入中: 100%|██████████| 1/1 [00:00<00:00, 10.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "cost 0.1550 seconds\n",
      "\n",
      "\n",
      "You must reply in English. You are a professional academic assistant specializing in the cs.CL domain. Strictly based on the provided literature information, answer the user's query. You may slightly expand and refine the response using existing knowledge, but avoid fabricating information. The response language must be English. Ensure the response is concise and limited to 500 words. Include a citation with the corresponding literature link (like [text](f\"https://arxiv.org/abs/{{id}}\")) and adjust the response style based on the temperature parameter (from 0.0 to 1.0). If the temperature is low (e.g., 0.2), provide a strict and precise answer. If the temperature is high (e.g., 0.8), provide a more creative and exploratory answer.\n",
      "\n",
      "Temperature: 0.2\n",
      "Question: 对于由多个大模型生成文本与人类文本混合的数据集（数据集仅含两种标签），如何区分出大模型生成的文本？\n",
      "Relevant Literature: \n",
      "ID: 1603.07771, Distance: 0.3425133228302002, Content: {'title': 'Neural Text Generation from Structured Data with Application to the\\n  Biography Domain', 'doi': None, 'categories': 'cs.CL', 'abstract': '  This paper introduces a neural model for concept-to-text generation that\\nscales to large, rich domains. We experiment with a new dataset of biographies\\nfrom Wikipedia that is an order of magnitude larger than existing resources\\nwith over 700k samples. The dataset is also vastly more diverse with a 400k\\nvocabulary, compared to a few hundred words for Weathergov or Robocup. Our\\nmodel builds upon recent work on conditional neural language model for text\\ngeneration. To deal with the large vocabulary, we extend these models to mix a\\nfixed vocabulary with copy actions that transfer sample-specific words from the\\ninput database to the generated output sentence. Our neural model significantly\\nout-performs a classical Kneser-Ney language model adapted to this task by\\nnearly 15 BLEU.\\n', 'publish_date': 'Thu, 24 Mar 2016 22:40:00 GMT'}\n",
      "ID: 1707.08052, Distance: 0.3503926396369934, Content: {'title': 'Challenges in Data-to-Document Generation', 'doi': None, 'categories': 'cs.CL', 'abstract': '  Recent neural models have shown significant progress on the problem of\\ngenerating short descriptive texts conditioned on a small number of database\\nrecords. In this work, we suggest a slightly more difficult data-to-text\\ngeneration task, and investigate how effective current approaches are on this\\ntask. In particular, we introduce a new, large-scale corpus of data records\\npaired with descriptive documents, propose a series of extractive evaluation\\nmethods for analyzing performance, and obtain baseline results using current\\nneural generation methods. Experiments show that these models produce fluent\\ntext, but fail to convincingly approximate human-generated documents. Moreover,\\neven templated baselines exceed the performance of these neural models on some\\nmetrics, though copy- and reconstruction-based extensions lead to noticeable\\nimprovements.\\n', 'publish_date': 'Tue, 25 Jul 2017 15:42:25 GMT'}\n",
      "ID: 2402.08496, Distance: 0.3582933843135834, Content: {'title': 'A Systematic Review of Data-to-Text NLG', 'doi': None, 'categories': 'cs.CL cs.AI cs.LG', 'abstract': '  This systematic review undertakes a comprehensive analysis of current\\nresearch on data-to-text generation, identifying gaps, challenges, and future\\ndirections within the field. Relevant literature in this field on datasets,\\nevaluation metrics, application areas, multilingualism, language models, and\\nhallucination mitigation methods is reviewed. Various methods for producing\\nhigh-quality text are explored, addressing the challenge of hallucinations in\\ndata-to-text generation. These methods include re-ranking, traditional and\\nneural pipeline architecture, planning architectures, data cleaning, controlled\\ngeneration, and modification of models and training techniques. Their\\neffectiveness and limitations are assessed, highlighting the need for\\nuniversally applicable strategies to mitigate hallucinations. The review also\\nexamines the usage, popularity, and impact of datasets, alongside evaluation\\nmetrics, with an emphasis on both automatic and human assessment. Additionally,\\nthe evolution of data-to-text models, particularly the widespread adoption of\\ntransformer models, is discussed. Despite advancements in text quality, the\\nreview emphasizes the importance of research in low-resourced languages and the\\nengineering of datasets in these languages to promote inclusivity. Finally,\\nseveral application domains of data-to-text are highlighted, emphasizing their\\nrelevance in such domains. Overall, this review serves as a guiding framework\\nfor fostering innovation and advancing data-to-text generation.\\n', 'publish_date': 'Tue, 13 Feb 2024 14:51:45 GMT'}\n",
      "ID: 2102.02723, Distance: 0.3691689968109131, Content: {'title': 'Data-to-text Generation with Macro Planning', 'doi': None, 'categories': 'cs.CL', 'abstract': '  Recent approaches to data-to-text generation have adopted the very successful\\nencoder-decoder architecture or variants thereof. These models generate text\\nwhich is fluent (but often imprecise) and perform quite poorly at selecting\\nappropriate content and ordering it coherently. To overcome some of these\\nissues, we propose a neural model with a macro planning stage followed by a\\ngeneration stage reminiscent of traditional methods which embrace separate\\nmodules for planning and surface realization. Macro plans represent high level\\norganization of important content such as entities, events and their\\ninteractions; they are learnt from data and given as input to the generator.\\nExtensive experiments on two data-to-text benchmarks (RotoWire and MLB) show\\nthat our approach outperforms competitive baselines in terms of automatic and\\nhuman evaluation.\\n', 'publish_date': 'Thu, 4 Feb 2021 16:32:57 GMT'}\n",
      "ID: 2004.10188, Distance: 0.3714131712913513, Content: {'title': 'Residual Energy-Based Models for Text', 'doi': None, 'categories': 'cs.CL cs.LG stat.ML', 'abstract': '  Current large-scale auto-regressive language models display impressive\\nfluency and can generate convincing text. In this work we start by asking the\\nquestion: Can the generations of these models be reliably distinguished from\\nreal text by statistical discriminators? We find experimentally that the answer\\nis affirmative when we have access to the training data for the model, and\\nguardedly affirmative even if we do not.\\n  This suggests that the auto-regressive models can be improved by\\nincorporating the (globally normalized) discriminators into the generative\\nprocess. We give a formalism for this using the Energy-Based Model framework,\\nand show that it indeed improves the results of the generative models, measured\\nboth in terms of perplexity and in terms of human evaluation.\\n', 'publish_date': 'Mon, 6 Apr 2020 13:44:03 GMT'}\n",
      "\n",
      "Answer:\n",
      "\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "生成嵌入中: 100%|██████████| 1/1 [00:00<00:00, 13.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "cost 0.1372 seconds\n",
      "\n",
      "\n",
      "You must reply in English. You are a professional academic assistant specializing in the cs.CL domain. Strictly based on the provided literature information, answer the user's query. You may slightly expand and refine the response using existing knowledge, but avoid fabricating information. The response language must be English. Ensure the response is concise and limited to 500 words. Include a citation with the corresponding literature link (like [text](f\"https://arxiv.org/abs/{{id}}\")) and adjust the response style based on the temperature parameter (from 0.0 to 1.0). If the temperature is low (e.g., 0.2), provide a strict and precise answer. If the temperature is high (e.g., 0.8), provide a more creative and exploratory answer.\n",
      "\n",
      "Temperature: 0.2\n",
      "Question: 不同大模型生成文本的检测方法是否存在差异？如何针对性地进行检测？\n",
      "Relevant Literature: \n",
      "ID: 1906.04043, Distance: 0.3639647662639618, Content: {'title': 'GLTR: Statistical Detection and Visualization of Generated Text', 'doi': None, 'categories': 'cs.CL cs.AI cs.HC cs.LG', 'abstract': '  The rapid improvement of language models has raised the specter of abuse of\\ntext generation systems. This progress motivates the development of simple\\nmethods for detecting generated text that can be used by and explained to\\nnon-experts. We develop GLTR, a tool to support humans in detecting whether a\\ntext was generated by a model. GLTR applies a suite of baseline statistical\\nmethods that can detect generation artifacts across common sampling schemes. In\\na human-subjects study, we show that the annotation scheme provided by GLTR\\nimproves the human detection-rate of fake text from 54% to 72% without any\\nprior training. GLTR is open-source and publicly deployed, and has already been\\nwidely used to detect generated outputs\\n', 'publish_date': 'Mon, 10 Jun 2019 14:52:41 GMT'}\n",
      "ID: 2004.10188, Distance: 0.3990912437438965, Content: {'title': 'Residual Energy-Based Models for Text', 'doi': None, 'categories': 'cs.CL cs.LG stat.ML', 'abstract': '  Current large-scale auto-regressive language models display impressive\\nfluency and can generate convincing text. In this work we start by asking the\\nquestion: Can the generations of these models be reliably distinguished from\\nreal text by statistical discriminators? We find experimentally that the answer\\nis affirmative when we have access to the training data for the model, and\\nguardedly affirmative even if we do not.\\n  This suggests that the auto-regressive models can be improved by\\nincorporating the (globally normalized) discriminators into the generative\\nprocess. We give a formalism for this using the Energy-Based Model framework,\\nand show that it indeed improves the results of the generative models, measured\\nboth in terms of perplexity and in terms of human evaluation.\\n', 'publish_date': 'Mon, 6 Apr 2020 13:44:03 GMT'}\n",
      "ID: 2111.09509, Distance: 0.40223774313926697, Content: {'title': 'How much do language models copy from their training data? Evaluating\\n  linguistic novelty in text generation using RAVEN', 'doi': None, 'categories': 'cs.CL', 'abstract': \"  Current language models can generate high-quality text. Are they simply\\ncopying text they have seen before, or have they learned generalizable\\nlinguistic abstractions? To tease apart these possibilities, we introduce\\nRAVEN, a suite of analyses for assessing the novelty of generated text,\\nfocusing on sequential structure (n-grams) and syntactic structure. We apply\\nthese analyses to four neural language models (an LSTM, a Transformer,\\nTransformer-XL, and GPT-2). For local structure - e.g., individual dependencies\\n- model-generated text is substantially less novel than our baseline of\\nhuman-generated text from each model's test set. For larger-scale structure -\\ne.g., overall sentence structure - model-generated text is as novel or even\\nmore novel than the human-generated baseline, but models still sometimes copy\\nsubstantially, in some cases duplicating passages over 1,000 words long from\\nthe training set. We also perform extensive manual analysis showing that\\nGPT-2's novel text is usually well-formed morphologically and syntactically but\\nhas reasonably frequent semantic issues (e.g., being self-contradictory).\\n\", 'publish_date': 'Thu, 18 Nov 2021 04:07:09 GMT'}\n",
      "ID: 1603.07771, Distance: 0.40489792823791504, Content: {'title': 'Neural Text Generation from Structured Data with Application to the\\n  Biography Domain', 'doi': None, 'categories': 'cs.CL', 'abstract': '  This paper introduces a neural model for concept-to-text generation that\\nscales to large, rich domains. We experiment with a new dataset of biographies\\nfrom Wikipedia that is an order of magnitude larger than existing resources\\nwith over 700k samples. The dataset is also vastly more diverse with a 400k\\nvocabulary, compared to a few hundred words for Weathergov or Robocup. Our\\nmodel builds upon recent work on conditional neural language model for text\\ngeneration. To deal with the large vocabulary, we extend these models to mix a\\nfixed vocabulary with copy actions that transfer sample-specific words from the\\ninput database to the generated output sentence. Our neural model significantly\\nout-performs a classical Kneser-Ney language model adapted to this task by\\nnearly 15 BLEU.\\n', 'publish_date': 'Thu, 24 Mar 2016 22:40:00 GMT'}\n",
      "ID: 2504.08697, Distance: 0.40632402896881104, Content: {'title': 'Large Language Models as Span Annotators', 'doi': None, 'categories': 'cs.CL', 'abstract': '  For high-quality texts, single-score metrics seldom provide actionable\\nfeedback. In contrast, span annotation - pointing out issues in the text by\\nannotating their spans - can guide improvements and provide insights. Until\\nrecently, span annotation was limited to human annotators or fine-tuned encoder\\nmodels. In this study, we automate span annotation with large language models\\n(LLMs). We compare expert or skilled crowdworker annotators with open and\\nproprietary LLMs on three tasks: data-to-text generation evaluation, machine\\ntranslation evaluation, and propaganda detection in human-written texts. In our\\nexperiments, we show that LLMs as span annotators are straightforward to\\nimplement and notably more cost-efficient than human annotators. The LLMs\\nachieve moderate agreement with skilled human annotators, in some scenarios\\ncomparable to the average agreement among the annotators themselves.\\nQualitative analysis shows that reasoning models outperform their\\ninstruction-tuned counterparts and provide more valid explanations for\\nannotations. We release the dataset of more than 40k model and human\\nannotations for further research.\\n', 'publish_date': 'Fri, 11 Apr 2025 17:04:51 GMT'}\n",
      "\n",
      "Answer:\n",
      "\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "生成嵌入中: 100%|██████████| 1/1 [00:00<00:00, 11.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "cost 0.1516 seconds\n",
      "\n",
      "\n",
      "You must reply in English. You are a professional academic assistant specializing in the cs.CL domain. Strictly based on the provided literature information, answer the user's query. You may slightly expand and refine the response using existing knowledge, but avoid fabricating information. The response language must be English. Ensure the response is concise and limited to 500 words. Include a citation with the corresponding literature link (like [text](f\"https://arxiv.org/abs/{{id}}\")) and adjust the response style based on the temperature parameter (from 0.0 to 1.0). If the temperature is low (e.g., 0.2), provide a strict and precise answer. If the temperature is high (e.g., 0.8), provide a more creative and exploratory answer.\n",
      "\n",
      "Temperature: 0.2\n",
      "Question: 在大语言模型生成文本检测任务中，如何设计有效的特征提取方法，以区分大模型生成文本与人工编写文本的细微差别？\n",
      "Relevant Literature: \n",
      "ID: 1906.04043, Distance: 0.38884851336479187, Content: {'title': 'GLTR: Statistical Detection and Visualization of Generated Text', 'doi': None, 'categories': 'cs.CL cs.AI cs.HC cs.LG', 'abstract': '  The rapid improvement of language models has raised the specter of abuse of\\ntext generation systems. This progress motivates the development of simple\\nmethods for detecting generated text that can be used by and explained to\\nnon-experts. We develop GLTR, a tool to support humans in detecting whether a\\ntext was generated by a model. GLTR applies a suite of baseline statistical\\nmethods that can detect generation artifacts across common sampling schemes. In\\na human-subjects study, we show that the annotation scheme provided by GLTR\\nimproves the human detection-rate of fake text from 54% to 72% without any\\nprior training. GLTR is open-source and publicly deployed, and has already been\\nwidely used to detect generated outputs\\n', 'publish_date': 'Mon, 10 Jun 2019 14:52:41 GMT'}\n",
      "ID: 2402.14873, Distance: 0.4141862988471985, Content: {'title': 'Technical Report on the Pangram AI-Generated Text Classifier', 'doi': None, 'categories': 'cs.CL cs.AI', 'abstract': '  We present Pangram Text, a transformer-based neural network trained to\\ndistinguish text written by large language models from text written by humans.\\nPangram Text outperforms zero-shot methods such as DetectGPT as well as leading\\ncommercial AI detection tools with over 38 times lower error rates on a\\ncomprehensive benchmark comprised of 10 text domains (student writing, creative\\nwriting, scientific writing, books, encyclopedias, news, email, scientific\\npapers, short-form Q&A) and 8 open- and closed-source large language models. We\\npropose a training algorithm, hard negative mining with synthetic mirrors, that\\nenables our classifier to achieve orders of magnitude lower false positive\\nrates on high-data domains such as reviews. Finally, we show that Pangram Text\\nis not biased against nonnative English speakers and generalizes to domains and\\nmodels unseen during training.\\n', 'publish_date': 'Wed, 21 Feb 2024 17:13:41 GMT'}\n",
      "ID: 2004.10188, Distance: 0.42217177152633667, Content: {'title': 'Residual Energy-Based Models for Text', 'doi': None, 'categories': 'cs.CL cs.LG stat.ML', 'abstract': '  Current large-scale auto-regressive language models display impressive\\nfluency and can generate convincing text. In this work we start by asking the\\nquestion: Can the generations of these models be reliably distinguished from\\nreal text by statistical discriminators? We find experimentally that the answer\\nis affirmative when we have access to the training data for the model, and\\nguardedly affirmative even if we do not.\\n  This suggests that the auto-regressive models can be improved by\\nincorporating the (globally normalized) discriminators into the generative\\nprocess. We give a formalism for this using the Energy-Based Model framework,\\nand show that it indeed improves the results of the generative models, measured\\nboth in terms of perplexity and in terms of human evaluation.\\n', 'publish_date': 'Mon, 6 Apr 2020 13:44:03 GMT'}\n",
      "ID: 1603.07771, Distance: 0.4227270185947418, Content: {'title': 'Neural Text Generation from Structured Data with Application to the\\n  Biography Domain', 'doi': None, 'categories': 'cs.CL', 'abstract': '  This paper introduces a neural model for concept-to-text generation that\\nscales to large, rich domains. We experiment with a new dataset of biographies\\nfrom Wikipedia that is an order of magnitude larger than existing resources\\nwith over 700k samples. The dataset is also vastly more diverse with a 400k\\nvocabulary, compared to a few hundred words for Weathergov or Robocup. Our\\nmodel builds upon recent work on conditional neural language model for text\\ngeneration. To deal with the large vocabulary, we extend these models to mix a\\nfixed vocabulary with copy actions that transfer sample-specific words from the\\ninput database to the generated output sentence. Our neural model significantly\\nout-performs a classical Kneser-Ney language model adapted to this task by\\nnearly 15 BLEU.\\n', 'publish_date': 'Thu, 24 Mar 2016 22:40:00 GMT'}\n",
      "ID: 1707.08052, Distance: 0.4247818887233734, Content: {'title': 'Challenges in Data-to-Document Generation', 'doi': None, 'categories': 'cs.CL', 'abstract': '  Recent neural models have shown significant progress on the problem of\\ngenerating short descriptive texts conditioned on a small number of database\\nrecords. In this work, we suggest a slightly more difficult data-to-text\\ngeneration task, and investigate how effective current approaches are on this\\ntask. In particular, we introduce a new, large-scale corpus of data records\\npaired with descriptive documents, propose a series of extractive evaluation\\nmethods for analyzing performance, and obtain baseline results using current\\nneural generation methods. Experiments show that these models produce fluent\\ntext, but fail to convincingly approximate human-generated documents. Moreover,\\neven templated baselines exceed the performance of these neural models on some\\nmetrics, though copy- and reconstruction-based extensions lead to noticeable\\nimprovements.\\n', 'publish_date': 'Tue, 25 Jul 2017 15:42:25 GMT'}\n",
      "\n",
      "Answer:\n",
      "\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "生成嵌入中: 100%|██████████| 1/1 [00:00<00:00,  9.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "cost 0.1600 seconds\n",
      "\n",
      "\n",
      "You must reply in English. You are a professional academic assistant specializing in the cs.CL domain. Strictly based on the provided literature information, answer the user's query. You may slightly expand and refine the response using existing knowledge, but avoid fabricating information. The response language must be English. Ensure the response is concise and limited to 500 words. Include a citation with the corresponding literature link (like [text](f\"https://arxiv.org/abs/{{id}}\")) and adjust the response style based on the temperature parameter (from 0.0 to 1.0). If the temperature is low (e.g., 0.2), provide a strict and precise answer. If the temperature is high (e.g., 0.8), provide a more creative and exploratory answer.\n",
      "\n",
      "Temperature: 0.2\n",
      "Question: 是否可以利用对抗训练等技术提高大模型生成文本的可检测性，如果可以应该如何实现？\n",
      "Relevant Literature: \n",
      "ID: 2004.08994, Distance: 0.33217766880989075, Content: {'title': 'Adversarial Training for Large Neural Language Models', 'doi': None, 'categories': 'cs.CL', 'abstract': '  Generalization and robustness are both key desiderata for designing machine\\nlearning methods. Adversarial training can enhance robustness, but past work\\noften finds it hurts generalization. In natural language processing (NLP),\\npre-training large neural language models such as BERT have demonstrated\\nimpressive gain in generalization for a variety of tasks, with further\\nimprovement from adversarial fine-tuning. However, these models are still\\nvulnerable to adversarial attacks. In this paper, we show that adversarial\\npre-training can improve both generalization and robustness. We propose a\\ngeneral algorithm ALUM (Adversarial training for large neural LangUage Models),\\nwhich regularizes the training objective by applying perturbations in the\\nembedding space that maximizes the adversarial loss. We present the first\\ncomprehensive study of adversarial training in all stages, including\\npre-training from scratch, continual pre-training on a well-trained model, and\\ntask-specific fine-tuning. ALUM obtains substantial gains over BERT on a wide\\nrange of NLP tasks, in both regular and adversarial scenarios. Even for models\\nthat have been well trained on extremely large text corpora, such as RoBERTa,\\nALUM can still produce significant gains from continual pre-training, whereas\\nconventional non-adversarial methods can not. ALUM can be further combined with\\ntask-specific fine-tuning to attain additional gains. The ALUM code is publicly\\navailable at https://github.com/namisan/mt-dnn.\\n', 'publish_date': 'Mon, 20 Apr 2020 00:07:18 GMT'}\n",
      "ID: 2406.08050, Distance: 0.3420890271663666, Content: {'title': 'Adversarial Evasion Attack Efficiency against Large Language Models', 'doi': None, 'categories': 'cs.CL cs.LG', 'abstract': '  Large Language Models (LLMs) are valuable for text classification, but their\\nvulnerabilities must not be disregarded. They lack robustness against\\nadversarial examples, so it is pertinent to understand the impacts of different\\ntypes of perturbations, and assess if those attacks could be replicated by\\ncommon users with a small amount of perturbations and a small number of queries\\nto a deployed LLM. This work presents an analysis of the effectiveness,\\nefficiency, and practicality of three different types of adversarial attacks\\nagainst five different LLMs in a sentiment classification task. The obtained\\nresults demonstrated the very distinct impacts of the word-level and\\ncharacter-level attacks. The word attacks were more effective, but the\\ncharacter and more constrained attacks were more practical and required a\\nreduced number of perturbations and queries. These differences need to be\\nconsidered during the development of adversarial defense strategies to train\\nmore robust LLMs for intelligent text classification applications.\\n', 'publish_date': 'Wed, 12 Jun 2024 10:02:27 GMT'}\n",
      "ID: 1910.04618, Distance: 0.3452152609825134, Content: {'title': 'Universal Adversarial Perturbation for Text Classification', 'doi': None, 'categories': 'cs.CL cs.LG stat.ML', 'abstract': '  Given a state-of-the-art deep neural network text classifier, we show the\\nexistence of a universal and very small perturbation vector (in the embedding\\nspace) that causes natural text to be misclassified with high probability.\\nUnlike images on which a single fixed-size adversarial perturbation can be\\nfound, text is of variable length, so we define the \"universality\" as\\n\"token-agnostic\", where a single perturbation is applied to each token,\\nresulting in different perturbations of flexible sizes at the sequence level.\\nWe propose an algorithm to compute universal adversarial perturbations, and\\nshow that the state-of-the-art deep neural networks are highly vulnerable to\\nthem, even though they keep the neighborhood of tokens mostly preserved. We\\nalso show how to use these adversarial perturbations to generate adversarial\\ntext samples. The surprising existence of universal \"token-agnostic\"\\nadversarial perturbations may reveal important properties of a text classifier.\\n', 'publish_date': 'Thu, 10 Oct 2019 14:48:22 GMT'}\n",
      "ID: 1906.03805, Distance: 0.3533552289009094, Content: {'title': 'Improving Neural Language Modeling via Adversarial Training', 'doi': None, 'categories': 'cs.LG cs.CL stat.ML', 'abstract': '  Recently, substantial progress has been made in language modeling by using\\ndeep neural networks. However, in practice, large scale neural language models\\nhave been shown to be prone to overfitting. In this paper, we present a simple\\nyet highly effective adversarial training mechanism for regularizing neural\\nlanguage models. The idea is to introduce adversarial noise to the output\\nembedding layer while training the models. We show that the optimal adversarial\\nnoise yields a simple closed-form solution, thus allowing us to develop a\\nsimple and time efficient algorithm. Theoretically, we show that our\\nadversarial mechanism effectively encourages the diversity of the embedding\\nvectors, helping to increase the robustness of models. Empirically, we show\\nthat our method improves on the single model state-of-the-art results for\\nlanguage modeling on Penn Treebank (PTB) and Wikitext-2, achieving test\\nperplexity scores of 46.01 and 38.07, respectively. When applied to machine\\ntranslation, our method improves over various transformer-based translation\\nbaselines in BLEU scores on the WMT14 English-German and IWSLT14 German-English\\ntasks.\\n', 'publish_date': 'Mon, 10 Jun 2019 05:55:08 GMT'}\n",
      "ID: 2311.01873, Distance: 0.35835573077201843, Content: {'title': 'Efficient Black-Box Adversarial Attacks on Neural Text Detectors', 'doi': None, 'categories': 'cs.CL', 'abstract': '  Neural text detectors are models trained to detect whether a given text was\\ngenerated by a language model or written by a human. In this paper, we\\ninvestigate three simple and resource-efficient strategies (parameter tweaking,\\nprompt engineering, and character-level mutations) to alter texts generated by\\nGPT-3.5 that are unsuspicious or unnoticeable for humans but cause\\nmisclassification by neural text detectors. The results show that especially\\nparameter tweaking and character-level mutations are effective strategies.\\n', 'publish_date': 'Fri, 3 Nov 2023 12:29:32 GMT'}\n",
      "\n",
      "Answer:\n",
      "\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "生成嵌入中: 100%|██████████| 1/1 [00:00<00:00, 14.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "cost 0.1339 seconds\n",
      "\n",
      "\n",
      "You must reply in English. You are a professional academic assistant specializing in the cs.CL domain. Strictly based on the provided literature information, answer the user's query. You may slightly expand and refine the response using existing knowledge, but avoid fabricating information. The response language must be English. Ensure the response is concise and limited to 500 words. Include a citation with the corresponding literature link (like [text](f\"https://arxiv.org/abs/{{id}}\")) and adjust the response style based on the temperature parameter (from 0.0 to 1.0). If the temperature is low (e.g., 0.2), provide a strict and precise answer. If the temperature is high (e.g., 0.8), provide a more creative and exploratory answer.\n",
      "\n",
      "Temperature: 0.2\n",
      "Question: 如何构建大规模、高质量的大模型生成文本检测数据集？\n",
      "Relevant Literature: \n",
      "ID: 1707.08052, Distance: 0.3967449963092804, Content: {'title': 'Challenges in Data-to-Document Generation', 'doi': None, 'categories': 'cs.CL', 'abstract': '  Recent neural models have shown significant progress on the problem of\\ngenerating short descriptive texts conditioned on a small number of database\\nrecords. In this work, we suggest a slightly more difficult data-to-text\\ngeneration task, and investigate how effective current approaches are on this\\ntask. In particular, we introduce a new, large-scale corpus of data records\\npaired with descriptive documents, propose a series of extractive evaluation\\nmethods for analyzing performance, and obtain baseline results using current\\nneural generation methods. Experiments show that these models produce fluent\\ntext, but fail to convincingly approximate human-generated documents. Moreover,\\neven templated baselines exceed the performance of these neural models on some\\nmetrics, though copy- and reconstruction-based extensions lead to noticeable\\nimprovements.\\n', 'publish_date': 'Tue, 25 Jul 2017 15:42:25 GMT'}\n",
      "ID: 1906.04043, Distance: 0.3976016044616699, Content: {'title': 'GLTR: Statistical Detection and Visualization of Generated Text', 'doi': None, 'categories': 'cs.CL cs.AI cs.HC cs.LG', 'abstract': '  The rapid improvement of language models has raised the specter of abuse of\\ntext generation systems. This progress motivates the development of simple\\nmethods for detecting generated text that can be used by and explained to\\nnon-experts. We develop GLTR, a tool to support humans in detecting whether a\\ntext was generated by a model. GLTR applies a suite of baseline statistical\\nmethods that can detect generation artifacts across common sampling schemes. In\\na human-subjects study, we show that the annotation scheme provided by GLTR\\nimproves the human detection-rate of fake text from 54% to 72% without any\\nprior training. GLTR is open-source and publicly deployed, and has already been\\nwidely used to detect generated outputs\\n', 'publish_date': 'Mon, 10 Jun 2019 14:52:41 GMT'}\n",
      "ID: 2401.03946, Distance: 0.40528881549835205, Content: {'title': 'TextMachina: Seamless Generation of Machine-Generated Text Datasets', 'doi': None, 'categories': 'cs.CL', 'abstract': '  Recent advancements in Large Language Models (LLMs) have led to high-quality\\nMachine-Generated Text (MGT), giving rise to countless new use cases and\\napplications. However, easy access to LLMs is posing new challenges due to\\nmisuse. To address malicious usage, researchers have released datasets to\\neffectively train models on MGT-related tasks. Similar strategies are used to\\ncompile these datasets, but no tool currently unifies them. In this scenario,\\nwe introduce TextMachina, a modular and extensible Python framework, designed\\nto aid in the creation of high-quality, unbiased datasets to build robust\\nmodels for MGT-related tasks such as detection, attribution, mixcase, or\\nboundary detection. It provides a user-friendly pipeline that abstracts away\\nthe inherent intricacies of building MGT datasets, such as LLM integrations,\\nprompt templating, and bias mitigation. The quality of the datasets generated\\nby TextMachina has been assessed in previous works, including shared tasks\\nwhere more than one hundred teams trained robust MGT detectors.\\n', 'publish_date': 'Mon, 8 Jan 2024 15:05:32 GMT'}\n",
      "ID: 1809.00582, Distance: 0.4073987901210785, Content: {'title': 'Data-to-Text Generation with Content Selection and Planning', 'doi': None, 'categories': 'cs.CL', 'abstract': '  Recent advances in data-to-text generation have led to the use of large-scale\\ndatasets and neural network models which are trained end-to-end, without\\nexplicitly modeling what to say and in what order. In this work, we present a\\nneural network architecture which incorporates content selection and planning\\nwithout sacrificing end-to-end training. We decompose the generation task into\\ntwo stages. Given a corpus of data records (paired with descriptive documents),\\nwe first generate a content plan highlighting which information should be\\nmentioned and in which order and then generate the document while taking the\\ncontent plan into account. Automatic and human-based evaluation experiments\\nshow that our model outperforms strong baselines improving the state-of-the-art\\non the recently released RotoWire dataset.\\n', 'publish_date': 'Mon, 3 Sep 2018 12:41:44 GMT'}\n",
      "ID: 2402.08496, Distance: 0.4106740951538086, Content: {'title': 'A Systematic Review of Data-to-Text NLG', 'doi': None, 'categories': 'cs.CL cs.AI cs.LG', 'abstract': '  This systematic review undertakes a comprehensive analysis of current\\nresearch on data-to-text generation, identifying gaps, challenges, and future\\ndirections within the field. Relevant literature in this field on datasets,\\nevaluation metrics, application areas, multilingualism, language models, and\\nhallucination mitigation methods is reviewed. Various methods for producing\\nhigh-quality text are explored, addressing the challenge of hallucinations in\\ndata-to-text generation. These methods include re-ranking, traditional and\\nneural pipeline architecture, planning architectures, data cleaning, controlled\\ngeneration, and modification of models and training techniques. Their\\neffectiveness and limitations are assessed, highlighting the need for\\nuniversally applicable strategies to mitigate hallucinations. The review also\\nexamines the usage, popularity, and impact of datasets, alongside evaluation\\nmetrics, with an emphasis on both automatic and human assessment. Additionally,\\nthe evolution of data-to-text models, particularly the widespread adoption of\\ntransformer models, is discussed. Despite advancements in text quality, the\\nreview emphasizes the importance of research in low-resourced languages and the\\nengineering of datasets in these languages to promote inclusivity. Finally,\\nseveral application domains of data-to-text are highlighted, emphasizing their\\nrelevance in such domains. Overall, this review serves as a guiding framework\\nfor fostering innovation and advancing data-to-text generation.\\n', 'publish_date': 'Tue, 13 Feb 2024 14:51:45 GMT'}\n",
      "\n",
      "Answer:\n",
      "\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "生成嵌入中: 100%|██████████| 1/1 [00:00<00:00, 10.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "cost 0.1558 seconds\n",
      "\n",
      "\n",
      "You must reply in English. You are a professional academic assistant specializing in the cs.CL domain. Strictly based on the provided literature information, answer the user's query. You may slightly expand and refine the response using existing knowledge, but avoid fabricating information. The response language must be English. Ensure the response is concise and limited to 500 words. Include a citation with the corresponding literature link (like [text](f\"https://arxiv.org/abs/{{id}}\")) and adjust the response style based on the temperature parameter (from 0.0 to 1.0). If the temperature is low (e.g., 0.2), provide a strict and precise answer. If the temperature is high (e.g., 0.8), provide a more creative and exploratory answer.\n",
      "\n",
      "Temperature: 0.2\n",
      "Question: 如何应对大模型生成文本不断优化带来的检测挑战？\n",
      "Relevant Literature: \n",
      "ID: 1603.07771, Distance: 0.37713485956192017, Content: {'title': 'Neural Text Generation from Structured Data with Application to the\\n  Biography Domain', 'doi': None, 'categories': 'cs.CL', 'abstract': '  This paper introduces a neural model for concept-to-text generation that\\nscales to large, rich domains. We experiment with a new dataset of biographies\\nfrom Wikipedia that is an order of magnitude larger than existing resources\\nwith over 700k samples. The dataset is also vastly more diverse with a 400k\\nvocabulary, compared to a few hundred words for Weathergov or Robocup. Our\\nmodel builds upon recent work on conditional neural language model for text\\ngeneration. To deal with the large vocabulary, we extend these models to mix a\\nfixed vocabulary with copy actions that transfer sample-specific words from the\\ninput database to the generated output sentence. Our neural model significantly\\nout-performs a classical Kneser-Ney language model adapted to this task by\\nnearly 15 BLEU.\\n', 'publish_date': 'Thu, 24 Mar 2016 22:40:00 GMT'}\n",
      "ID: 1706.09433, Distance: 0.3856700658798218, Content: {'title': 'Data-driven Natural Language Generation: Paving the Road to Success', 'doi': None, 'categories': 'cs.CL', 'abstract': '  We argue that there are currently two major bottlenecks to the commercial use\\nof statistical machine learning approaches for natural language generation\\n(NLG): (a) The lack of reliable automatic evaluation metrics for NLG, and (b)\\nThe scarcity of high quality in-domain corpora. We address the first problem by\\nthoroughly analysing current evaluation metrics and motivating the need for a\\nnew, more reliable metric. The second problem is addressed by presenting a\\nnovel framework for developing and evaluating a high quality corpus for NLG\\ntraining.\\n', 'publish_date': 'Wed, 28 Jun 2017 18:17:30 GMT'}\n",
      "ID: 1906.04043, Distance: 0.3857758343219757, Content: {'title': 'GLTR: Statistical Detection and Visualization of Generated Text', 'doi': None, 'categories': 'cs.CL cs.AI cs.HC cs.LG', 'abstract': '  The rapid improvement of language models has raised the specter of abuse of\\ntext generation systems. This progress motivates the development of simple\\nmethods for detecting generated text that can be used by and explained to\\nnon-experts. We develop GLTR, a tool to support humans in detecting whether a\\ntext was generated by a model. GLTR applies a suite of baseline statistical\\nmethods that can detect generation artifacts across common sampling schemes. In\\na human-subjects study, we show that the annotation scheme provided by GLTR\\nimproves the human detection-rate of fake text from 54% to 72% without any\\nprior training. GLTR is open-source and publicly deployed, and has already been\\nwidely used to detect generated outputs\\n', 'publish_date': 'Mon, 10 Jun 2019 14:52:41 GMT'}\n",
      "ID: 2504.08697, Distance: 0.39249658584594727, Content: {'title': 'Large Language Models as Span Annotators', 'doi': None, 'categories': 'cs.CL', 'abstract': '  For high-quality texts, single-score metrics seldom provide actionable\\nfeedback. In contrast, span annotation - pointing out issues in the text by\\nannotating their spans - can guide improvements and provide insights. Until\\nrecently, span annotation was limited to human annotators or fine-tuned encoder\\nmodels. In this study, we automate span annotation with large language models\\n(LLMs). We compare expert or skilled crowdworker annotators with open and\\nproprietary LLMs on three tasks: data-to-text generation evaluation, machine\\ntranslation evaluation, and propaganda detection in human-written texts. In our\\nexperiments, we show that LLMs as span annotators are straightforward to\\nimplement and notably more cost-efficient than human annotators. The LLMs\\nachieve moderate agreement with skilled human annotators, in some scenarios\\ncomparable to the average agreement among the annotators themselves.\\nQualitative analysis shows that reasoning models outperform their\\ninstruction-tuned counterparts and provide more valid explanations for\\nannotations. We release the dataset of more than 40k model and human\\nannotations for further research.\\n', 'publish_date': 'Fri, 11 Apr 2025 17:04:51 GMT'}\n",
      "ID: 2102.02723, Distance: 0.3936072289943695, Content: {'title': 'Data-to-text Generation with Macro Planning', 'doi': None, 'categories': 'cs.CL', 'abstract': '  Recent approaches to data-to-text generation have adopted the very successful\\nencoder-decoder architecture or variants thereof. These models generate text\\nwhich is fluent (but often imprecise) and perform quite poorly at selecting\\nappropriate content and ordering it coherently. To overcome some of these\\nissues, we propose a neural model with a macro planning stage followed by a\\ngeneration stage reminiscent of traditional methods which embrace separate\\nmodules for planning and surface realization. Macro plans represent high level\\norganization of important content such as entities, events and their\\ninteractions; they are learnt from data and given as input to the generator.\\nExtensive experiments on two data-to-text benchmarks (RotoWire and MLB) show\\nthat our approach outperforms competitive baselines in terms of automatic and\\nhuman evaluation.\\n', 'publish_date': 'Thu, 4 Feb 2021 16:32:57 GMT'}\n",
      "\n",
      "Answer:\n",
      "\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "生成嵌入中: 100%|██████████| 1/1 [00:00<00:00, 10.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "cost 0.1611 seconds\n",
      "\n",
      "\n",
      "You must reply in English. You are a professional academic assistant specializing in the cs.CL domain. Strictly based on the provided literature information, answer the user's query. You may slightly expand and refine the response using existing knowledge, but avoid fabricating information. The response language must be English. Ensure the response is concise and limited to 500 words. Include a citation with the corresponding literature link (like [text](f\"https://arxiv.org/abs/{{id}}\")) and adjust the response style based on the temperature parameter (from 0.0 to 1.0). If the temperature is low (e.g., 0.2), provide a strict and precise answer. If the temperature is high (e.g., 0.8), provide a more creative and exploratory answer.\n",
      "\n",
      "Temperature: 0.2\n",
      "Question: 如何构建一个能够自适应不同领域和风格的文本生成模型？\n",
      "Relevant Literature: \n",
      "ID: cs/9812018, Distance: 0.39264920353889465, Content: {'title': 'A Flexible Shallow Approach to Text Generation', 'doi': None, 'categories': 'cs.CL', 'abstract': '  In order to support the efficient development of NL generation systems, two\\northogonal methods are currently pursued with emphasis: (1) reusable, general,\\nand linguistically motivated surface realization components, and (2) simple,\\ntask-oriented template-based techniques. In this paper we argue that, from an\\napplication-oriented perspective, the benefits of both are still limited. In\\norder to improve this situation, we suggest and evaluate shallow generation\\nmethods associated with increased flexibility. We advise a close connection\\nbetween domain-motivated and linguistic ontologies that supports the quick\\nadaptation to new tasks and domains, rather than the reuse of general\\nresources. Our method is especially designed for generating reports with\\nlimited linguistic variations.\\n', 'publish_date': 'Wed, 16 Dec 1998 16:37:01 GMT'}\n",
      "ID: 2307.09702, Distance: 0.3937809467315674, Content: {'title': 'Efficient Guided Generation for Large Language Models', 'doi': None, 'categories': 'cs.CL cs.LG', 'abstract': \"  In this article we show how the problem of neural text generation can be\\nconstructively reformulated in terms of transitions between the states of a\\nfinite-state machine. This framework leads to an efficient approach to guiding\\ntext generation with regular expressions and context-free grammars by allowing\\nthe construction of an index over a language model's vocabulary. The approach\\nis model agnostic, allows one to enforce domain-specific knowledge and\\nconstraints, and enables the construction of reliable interfaces by\\nguaranteeing the structure of the generated text. It adds little overhead to\\nthe token sequence generation process and significantly outperforms existing\\nsolutions. An implementation is provided in the open source Python library\\nOutlines\\n\", 'publish_date': 'Wed, 19 Jul 2023 01:14:49 GMT'}\n",
      "ID: 2005.01822, Distance: 0.4125930070877075, Content: {'title': 'Exploring Controllable Text Generation Techniques', 'doi': None, 'categories': 'cs.CL', 'abstract': '  Neural controllable text generation is an important area gaining attention\\ndue to its plethora of applications. Although there is a large body of prior\\nwork in controllable text generation, there is no unifying theme. In this work,\\nwe provide a new schema of the pipeline of the generation process by\\nclassifying it into five modules. The control of attributes in the generation\\nprocess requires modification of these modules. We present an overview of\\ndifferent techniques used to perform the modulation of these modules. We also\\nprovide an analysis on the advantages and disadvantages of these techniques. We\\nfurther pave ways to develop new architectures based on the combination of the\\nmodules described in this paper.\\n', 'publish_date': 'Mon, 4 May 2020 20:04:47 GMT'}\n",
      "ID: 2404.07117, Distance: 0.42169976234436035, Content: {'title': 'Continuous Language Model Interpolation for Dynamic and Controllable\\n  Text Generation', 'doi': None, 'categories': 'cs.CL cs.LG', 'abstract': '  As large language models (LLMs) have gained popularity for a variety of use\\ncases, making them adaptable and controllable has become increasingly\\nimportant, especially for user-facing applications. While the existing\\nliterature on LLM adaptation primarily focuses on finding a model (or models)\\nthat optimizes a single predefined objective, here we focus on the challenging\\ncase where the model must dynamically adapt to diverse -- and often changing --\\nuser preferences. For this, we leverage adaptation methods based on linear\\nweight interpolation, casting them as continuous multi-domain interpolators\\nthat produce models with specific prescribed generation characteristics\\non-the-fly. Specifically, we use low-rank updates to fine-tune a base model to\\nvarious different domains, yielding a set of anchor models with distinct\\ngeneration profiles. Then, we use the weight updates of these anchor models to\\nparametrize the entire (infinite) class of models contained within their convex\\nhull. We empirically show that varying the interpolation weights yields\\npredictable and consistent change in the model outputs with respect to all of\\nthe controlled attributes. We find that there is little entanglement between\\nmost attributes and identify and discuss the pairs of attributes for which this\\nis not the case. Our results suggest that linearly interpolating between the\\nweights of fine-tuned models facilitates predictable, fine-grained control of\\nmodel outputs with respect to multiple stylistic characteristics\\nsimultaneously.\\n', 'publish_date': 'Wed, 10 Apr 2024 15:55:07 GMT'}\n",
      "ID: 1909.00734, Distance: 0.4312938153743744, Content: {'title': 'Sentence-Level Content Planning and Style Specification for Neural Text\\n  Generation', 'doi': None, 'categories': 'cs.CL', 'abstract': '  Building effective text generation systems requires three critical\\ncomponents: content selection, text planning, and surface realization, and\\ntraditionally they are tackled as separate problems. Recent all-in-one style\\nneural generation models have made impressive progress, yet they often produce\\noutputs that are incoherent and unfaithful to the input. To address these\\nissues, we present an end-to-end trained two-step generation model, where a\\nsentence-level content planner first decides on the keyphrases to cover as well\\nas a desired language style, followed by a surface realization decoder that\\ngenerates relevant and coherent text. For experiments, we consider three tasks\\nfrom domains with diverse topics and varying language styles: persuasive\\nargument construction from Reddit, paragraph generation for normal and simple\\nversions of Wikipedia, and abstract generation for scientific articles.\\nAutomatic evaluation shows that our system can significantly outperform\\ncompetitive comparisons. Human judges further rate our system generated text as\\nmore fluent and correct, compared to the generations by its variants that do\\nnot consider language style.\\n', 'publish_date': 'Mon, 2 Sep 2019 14:29:36 GMT'}\n",
      "\n",
      "Answer:\n",
      "\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "生成嵌入中: 100%|██████████| 1/1 [00:00<00:00, 11.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "cost 0.1507 seconds\n",
      "\n",
      "\n",
      "You must reply in English. You are a professional academic assistant specializing in the cs.CL domain. Strictly based on the provided literature information, answer the user's query. You may slightly expand and refine the response using existing knowledge, but avoid fabricating information. The response language must be English. Ensure the response is concise and limited to 500 words. Include a citation with the corresponding literature link (like [text](f\"https://arxiv.org/abs/{{id}}\")) and adjust the response style based on the temperature parameter (from 0.0 to 1.0). If the temperature is low (e.g., 0.2), provide a strict and precise answer. If the temperature is high (e.g., 0.8), provide a more creative and exploratory answer.\n",
      "\n",
      "Temperature: 0.2\n",
      "Question: 在文本分类任务中，如何处理文本数据中的噪声和异常值，以提高模型的性能与鲁棒性？\n",
      "Relevant Literature: \n",
      "ID: 1904.08067, Distance: 0.3456064760684967, Content: {'title': 'Text Classification Algorithms: A Survey', 'doi': '10.3390/info10040150', 'categories': 'cs.LG cs.AI cs.CL cs.IR stat.ML', 'abstract': '  In recent years, there has been an exponential growth in the number of\\ncomplex documents and texts that require a deeper understanding of machine\\nlearning methods to be able to accurately classify texts in many applications.\\nMany machine learning approaches have achieved surpassing results in natural\\nlanguage processing. The success of these learning algorithms relies on their\\ncapacity to understand complex models and non-linear relationships within data.\\nHowever, finding suitable structures, architectures, and techniques for text\\nclassification is a challenge for researchers. In this paper, a brief overview\\nof text classification algorithms is discussed. This overview covers different\\ntext feature extractions, dimensionality reduction methods, existing algorithms\\nand techniques, and evaluations methods. Finally, the limitations of each\\ntechnique and their application in the real-world problem are discussed.\\n', 'publish_date': 'Wed, 17 Apr 2019 03:29:05 GMT'}\n",
      "ID: 2502.19801, Distance: 0.4182180166244507, Content: {'title': 'Text classification using machine learning methods', 'doi': None, 'categories': 'cs.CL cs.LG', 'abstract': '  In this paper we present the results of an experiment aimed to use machine\\nlearning methods to obtain models that can be used for the automatic\\nclassification of products. In order to apply automatic classification methods,\\nwe transformed the product names from a text representation to numeric vectors,\\na process called word embedding. We used several embedding methods: Count\\nVectorization, TF-IDF, Word2Vec, FASTTEXT, and GloVe. Having the product names\\nin a form of numeric vectors, we proceeded with a set of machine learning\\nmethods for automatic classification: Logistic Regression, Multinomial Naive\\nBayes, kNN, Artificial Neural Networks, Support Vector Machines, and Decision\\ntrees with several variants. The results show an impressive accuracy of the\\nclassification process for Support Vector Machines, Logistic Regression, and\\nRandom Forests. Regarding the word embedding methods, the best results were\\nobtained with the FASTTEXT technique.\\n', 'publish_date': 'Thu, 27 Feb 2025 06:20:38 GMT'}\n",
      "ID: 2010.02458, Distance: 0.41830217838287354, Content: {'title': 'Identifying Spurious Correlations for Robust Text Classification', 'doi': None, 'categories': 'cs.LG cs.CL cs.IR', 'abstract': '  The predictions of text classifiers are often driven by spurious correlations\\n-- e.g., the term `Spielberg\\' correlates with positively reviewed movies, even\\nthough the term itself does not semantically convey a positive sentiment. In\\nthis paper, we propose a method to distinguish spurious and genuine\\ncorrelations in text classification. We treat this as a supervised\\nclassification problem, using features derived from treatment effect estimators\\nto distinguish spurious correlations from \"genuine\" ones. Due to the generic\\nnature of these features and their small dimensionality, we find that the\\napproach works well even with limited training examples, and that it is\\npossible to transport the word classifier to new domains. Experiments on four\\ndatasets (sentiment classification and toxicity detection) suggest that using\\nthis approach to inform feature selection also leads to more robust\\nclassification, as measured by improved worst-case accuracy on the samples\\naffected by spurious correlations.\\n', 'publish_date': 'Tue, 6 Oct 2020 03:49:22 GMT'}\n",
      "ID: 1710.09085, Distance: 0.4185680150985718, Content: {'title': 'Re-evaluating the need for Modelling Term-Dependence in Text\\n  Classification Problems', 'doi': None, 'categories': 'cs.IR cs.CL', 'abstract': '  A substantial amount of research has been carried out in developing machine\\nlearning algorithms that account for term dependence in text classification.\\nThese algorithms offer acceptable performance in most cases but they are\\nassociated with a substantial cost. They require significantly greater\\nresources to operate. This paper argues against the justification of the higher\\ncosts of these algorithms, based on their performance in text classification\\nproblems. In order to prove the conjecture, the performance of one of the best\\ndependence models is compared to several well established algorithms in text\\nclassification. A very specific collection of datasets have been designed,\\nwhich would best reflect the disparity in the nature of text data, that are\\npresent in real world applications. The results show that even one of the best\\nterm dependence models, performs decent at best when compared to other\\nindependence models. Coupled with their substantially greater requirement for\\nhardware resources for operation, this makes them an impractical choice for\\nbeing used in real world scenarios.\\n', 'publish_date': 'Wed, 25 Oct 2017 06:26:28 GMT'}\n",
      "ID: 2005.09198, Distance: 0.4193064868450165, Content: {'title': 'Quantifying the Uncertainty of Precision Estimates for Rule based Text\\n  Classifiers', 'doi': None, 'categories': 'cs.LG cs.AI cs.CL stat.ML', 'abstract': \"  Rule based classifiers that use the presence and absence of key sub-strings\\nto make classification decisions have a natural mechanism for quantifying the\\nuncertainty of their precision. For a binary classifier, the key insight is to\\ntreat partitions of the sub-string set induced by the documents as Bernoulli\\nrandom variables. The mean value of each random variable is an estimate of the\\nclassifier's precision when presented with a document inducing that partition.\\nThese means can be compared, using standard statistical tests, to a desired or\\nexpected classifier precision. A set of binary classifiers can be combined into\\na single, multi-label classifier by an application of the Dempster-Shafer\\ntheory of evidence. The utility of this approach is demonstrated with a\\nbenchmark problem.\\n\", 'publish_date': 'Tue, 19 May 2020 03:51:47 GMT'}\n",
      "\n",
      "Answer:\n",
      "\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "生成嵌入中: 100%|██████████| 1/1 [00:00<00:00, 13.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "cost 0.1382 seconds\n",
      "\n",
      "\n",
      "You must reply in English. You are a professional academic assistant specializing in the cs.CL domain. Strictly based on the provided literature information, answer the user's query. You may slightly expand and refine the response using existing knowledge, but avoid fabricating information. The response language must be English. Ensure the response is concise and limited to 500 words. Include a citation with the corresponding literature link (like [text](f\"https://arxiv.org/abs/{{id}}\")) and adjust the response style based on the temperature parameter (from 0.0 to 1.0). If the temperature is low (e.g., 0.2), provide a strict and precise answer. If the temperature is high (e.g., 0.8), provide a more creative and exploratory answer.\n",
      "\n",
      "Temperature: 0.2\n",
      "Question: 什么是知识图谱，如何对文本数据构建知识图谱？\n",
      "Relevant Literature: \n",
      "ID: 2211.10511, Distance: 0.34266549348831177, Content: {'title': 'Knowledge Graph Generation From Text', 'doi': None, 'categories': 'cs.CL cs.LG', 'abstract': '  In this work we propose a novel end-to-end multi-stage Knowledge Graph (KG)\\ngeneration system from textual inputs, separating the overall process into two\\nstages. The graph nodes are generated first using pretrained language model,\\nfollowed by a simple edge construction head, enabling efficient KG extraction\\nfrom the text. For each stage we consider several architectural choices that\\ncan be used depending on the available training resources. We evaluated the\\nmodel on a recent WebNLG 2020 Challenge dataset, matching the state-of-the-art\\nperformance on text-to-RDF generation task, as well as on New York Times (NYT)\\nand a large-scale TekGen datasets, showing strong overall performance,\\noutperforming the existing baselines. We believe that the proposed system can\\nserve as a viable KG construction alternative to the existing linearization or\\nsampling-based graph generation approaches. Our code can be found at\\nhttps://github.com/IBM/Grapher\\n', 'publish_date': 'Fri, 18 Nov 2022 21:27:13 GMT'}\n",
      "ID: 2401.07683, Distance: 0.35198739171028137, Content: {'title': 'Assisted Knowledge Graph Authoring: Human-Supervised Knowledge Graph\\n  Construction from Natural Language', 'doi': '10.1145/3627508.3638340', 'categories': 'cs.CL', 'abstract': '  Encyclopedic knowledge graphs, such as Wikidata, host an extensive repository\\nof millions of knowledge statements. However, domain-specific knowledge from\\nfields such as history, physics, or medicine is significantly underrepresented\\nin those graphs. Although few domain-specific knowledge graphs exist (e.g.,\\nPubmed for medicine), developing specialized retrieval applications for many\\ndomains still requires constructing knowledge graphs from scratch. To\\nfacilitate knowledge graph construction, we introduce WAKA: a Web application\\nthat allows domain experts to create knowledge graphs through the medium with\\nwhich they are most familiar: natural language.\\n', 'publish_date': 'Mon, 15 Jan 2024 13:51:00 GMT'}\n",
      "ID: 2409.03284, Distance: 0.3576383888721466, Content: {'title': 'iText2KG: Incremental Knowledge Graphs Construction Using Large Language\\n  Models', 'doi': None, 'categories': 'cs.AI cs.CL cs.IR', 'abstract': \"  Most available data is unstructured, making it challenging to access valuable\\ninformation. Automatically building Knowledge Graphs (KGs) is crucial for\\nstructuring data and making it accessible, allowing users to search for\\ninformation effectively. KGs also facilitate insights, inference, and\\nreasoning. Traditional NLP methods, such as named entity recognition and\\nrelation extraction, are key in information retrieval but face limitations,\\nincluding the use of predefined entity types and the need for supervised\\nlearning. Current research leverages large language models' capabilities, such\\nas zero- or few-shot learning. However, unresolved and semantically duplicated\\nentities and relations still pose challenges, leading to inconsistent graphs\\nand requiring extensive post-processing. Additionally, most approaches are\\ntopic-dependent. In this paper, we propose iText2KG, a method for incremental,\\ntopic-independent KG construction without post-processing. This plug-and-play,\\nzero-shot method is applicable across a wide range of KG construction scenarios\\nand comprises four modules: Document Distiller, Incremental Entity Extractor,\\nIncremental Relation Extractor, and Graph Integrator and Visualization. Our\\nmethod demonstrates superior performance compared to baseline methods across\\nthree scenarios: converting scientific papers to graphs, websites to graphs,\\nand CVs to graphs.\\n\", 'publish_date': 'Thu, 5 Sep 2024 06:49:14 GMT'}\n",
      "ID: 2101.06111, Distance: 0.35831788182258606, Content: {'title': 'Knowledge Graphs and Natural-Language Processing', 'doi': '10.1007/978-3-030-48099-8', 'categories': 'cs.CY cs.CL', 'abstract': '  Emergency-relevant data comes in many varieties. It can be high volume and\\nhigh velocity, and reaction times are critical, calling for efficient and\\npowerful techniques for data analysis and management. Knowledge graphs\\nrepresent data in a rich, flexible, and uniform way that is well matched with\\nthe needs of emergency management. They build on existing standards, resources,\\ntechniques, and tools for semantic data and computing. This chapter explains\\nthe most important semantic technologies and how they support knowledge graphs.\\nWe proceed to discuss their benefits and challenges and give examples of\\nrelevant semantic data sources and vocabularies. Natural-language texts -- in\\nparticular those collected from social media such as Twitter -- is a type of\\ndata source that poses particular analysis challenges. We therefore include an\\noverview of techniques for processing natural-language texts.\\n', 'publish_date': 'Tue, 15 Dec 2020 16:53:28 GMT'}\n",
      "ID: 2106.01167, Distance: 0.36182737350463867, Content: {'title': 'End-to-End NLP Knowledge Graph Construction', 'doi': None, 'categories': 'cs.CL', 'abstract': '  This paper studies the end-to-end construction of an NLP Knowledge Graph (KG)\\nfrom scientific papers. We focus on extracting four types of relations:\\nevaluatedOn between tasks and datasets, evaluatedBy between tasks and\\nevaluation metrics, as well as coreferent and related relations between the\\nsame type of entities. For instance, F1-score is coreferent with F-measure. We\\nintroduce novel methods for each of these relation types and apply our final\\nframework (SciNLP-KG) to 30,000 NLP papers from ACL Anthology to build a\\nlarge-scale KG, which can facilitate automatically constructing scientific\\nleaderboards for the NLP community. The results of our experiments indicate\\nthat the resulting KG contains high-quality information.\\n', 'publish_date': 'Wed, 2 Jun 2021 14:03:06 GMT'}\n",
      "\n",
      "Answer:\n",
      "\n",
      "==================================================\n",
      "avg time: 0.1509 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "i = 0\n",
    "total_time = 0\n",
    "for extracted_data in extracted_datas:\n",
    "    query = [extracted_data]\n",
    "    orig_q = [search.prompts.questions[i]]\n",
    "    start_time = time()\n",
    "    identity, results = search.search(query, orig_q)\n",
    "    cost_time = time() - start_time\n",
    "    print(f\"\\n\\ncost {cost_time:.4f} seconds\\n\\n\")\n",
    "\n",
    "    print(identity)\n",
    "    print(results)\n",
    "    print(\"=\" * 50)\n",
    "    i += 1\n",
    "    total_time += cost_time\n",
    "    \n",
    "print(f\"avg time: {total_time / len(extracted_datas):.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "经过切分后共有90208个token\n"
     ]
    }
   ],
   "source": [
    "# 请确保已经安装了DashScope Python SDK\n",
    "from dashscope import get_tokenizer\n",
    "\n",
    "# 获取tokenizer对象，目前只支持通义千问系列模型\n",
    "tokenizer = get_tokenizer(\"qwen-turbo\")\n",
    "\n",
    "input_str = \"\"\"生成嵌入中: 100%|██████████| 1/1 [00:00<00:00, 13.84it/s]\n",
    "[INFO] Query vector shape: (1, 1, 768)\n",
    "\n",
    "\n",
    "cost 0.1376 seconds\n",
    "\n",
    "\n",
    "You are a professional academic assistant specializing in the cs.CL domain. Strictly based on the provided literature information, answer the user's query. You may slightly expand and refine the response using existing knowledge, but avoid fabricating information. The response language should match the user's query language (English for English queries, Chinese for Chinese queries). Ensure the response is concise and limited to 500 words. Include a citation with the corresponding literature link (like [text](f\"https://arxiv.org/abs/{{id}}\")) and adjust the response style based on the temperature parameter (from 0.0 to 1.0). If the temperature is low (e.g., 0.2), provide a strict and precise answer. If the temperature is high (e.g., 0.8), provide a more creative and exploratory answer.\n",
    "\n",
    "Temperature: 0.2\n",
    "Question: 什么是注意力机制？\n",
    "Relevant Literature: \n",
    "ID: 1810.10126, Distance: 0.3811461925506592, Content: {'title': 'Area Attention', 'doi': None, 'categories': 'cs.LG cs.AI cs.CL stat.ML', 'abstract': '  Existing attention mechanisms are trained to attend to individual items in a\\ncollection (the memory) with a predefined, fixed granularity, e.g., a word\\ntoken or an image grid. We propose area attention: a way to attend to areas in\\nthe memory, where each area contains a group of items that are structurally\\nadjacent, e.g., spatially for a 2D memory such as images, or temporally for a\\n1D memory such as natural language sentences. Importantly, the shape and the\\nsize of an area are dynamically determined via learning, which enables a model\\nto attend to information with varying granularity. Area attention can easily\\nwork with existing model architectures such as multi-head attention for\\nsimultaneously attending to multiple areas in the memory. We evaluate area\\nattention on two tasks: neural machine translation (both character and\\ntoken-level) and image captioning, and improve upon strong (state-of-the-art)\\nbaselines in all the cases. These improvements are obtainable with a basic form\\nof area attention that is parameter free.\\n', 'publish_date': 'Tue, 23 Oct 2018 23:14:27 GMT'}\n",
    "ID: 2211.03495, Distance: 0.40938127040863037, Content: {'title': 'How Much Does Attention Actually Attend? Questioning the Importance of\\n  Attention in Pretrained Transformers', 'doi': None, 'categories': 'cs.CL cs.LG', 'abstract': '  The attention mechanism is considered the backbone of the widely-used\\nTransformer architecture. It contextualizes the input by computing\\ninput-specific attention matrices. We find that this mechanism, while powerful\\nand elegant, is not as important as typically thought for pretrained language\\nmodels. We introduce PAPA, a new probing method that replaces the\\ninput-dependent attention matrices with constant ones -- the average attention\\nweights over multiple inputs. We use PAPA to analyze several established\\npretrained Transformers on six downstream tasks. We find that without any\\ninput-dependent attention, all models achieve competitive performance -- an\\naverage relative drop of only 8% from the probing baseline. Further, little or\\nno performance drop is observed when replacing half of the input-dependent\\nattention matrices with constant (input-independent) ones. Interestingly, we\\nshow that better-performing models lose more from applying our method than\\nweaker models, suggesting that the utilization of the input-dependent attention\\nmechanism might be a factor in their success. Our results motivate research on\\nsimpler alternatives to input-dependent attention, as well as on methods for\\nbetter utilization of this mechanism in the Transformer architecture.\\n', 'publish_date': 'Mon, 7 Nov 2022 12:37:54 GMT'}\n",
    "ID: 1904.05873, Distance: 0.4193165898323059, Content: {'title': 'An Empirical Study of Spatial Attention Mechanisms in Deep Networks', 'doi': None, 'categories': 'cs.CV cs.CL cs.LG', 'abstract': '  Attention mechanisms have become a popular component in deep neural networks,\\nyet there has been little examination of how different influencing factors and\\nmethods for computing attention from these factors affect performance. Toward a\\nbetter general understanding of attention mechanisms, we present an empirical\\nstudy that ablates various spatial attention elements within a generalized\\nattention formulation, encompassing the dominant Transformer attention as well\\nas the prevalent deformable convolution and dynamic convolution modules.\\nConducted on a variety of applications, the study yields significant findings\\nabout spatial attention in deep networks, some of which run counter to\\nconventional understanding. For example, we find that the query and key content\\ncomparison in Transformer attention is negligible for self-attention, but vital\\nfor encoder-decoder attention. A proper combination of deformable convolution\\nwith key content only saliency achieves the best accuracy-efficiency tradeoff\\nin self-attention. Our results suggest that there exists much room for\\nimprovement in the design of attention mechanisms.\\n', 'publish_date': 'Thu, 11 Apr 2019 17:58:37 GMT'}\n",
    "ID: 1905.09856, Distance: 0.4276387691497803, Content: {'title': 'Copy this Sentence', 'doi': None, 'categories': 'cs.LG cs.CL stat.ML', 'abstract': '  Attention is an operation that selects some largest element from some set,\\nwhere the notion of largest is defined elsewhere. Applying this operation to\\nsequence to sequence mapping results in significant improvements to the task at\\nhand. In this paper we provide the mathematical definition of attention and\\nexamine its application to sequence to sequence models. We highlight the exact\\ncorrespondences between machine learning implementations of attention and our\\nmathematical definition. We provide clear evidence of effectiveness of\\nattention mechanisms evaluating models with varying degrees of attention on a\\nvery simple task: copying a sentence. We find that models that make greater use\\nof attention perform much better on sequence to sequence mapping tasks,\\nconverge faster and are more stable.\\n', 'publish_date': 'Thu, 23 May 2019 18:25:35 GMT'}\n",
    "ID: 2211.07714, Distance: 0.43440112471580505, Content: {'title': 'Revisiting Attention Weights as Explanations from an Information\\n  Theoretic Perspective', 'doi': None, 'categories': 'cs.CL cs.AI cs.LG', 'abstract': '  Attention mechanisms have recently demonstrated impressive performance on a\\nrange of NLP tasks, and attention scores are often used as a proxy for model\\nexplainability. However, there is a debate on whether attention weights can, in\\nfact, be used to identify the most important inputs to a model. We approach\\nthis question from an information theoretic perspective by measuring the mutual\\ninformation between the model output and the hidden states. From extensive\\nexperiments, we draw the following conclusions: (i) Additive and Deep attention\\nmechanisms are likely to be better at preserving the information between the\\nhidden states and the model output (compared to Scaled Dot-product); (ii)\\nablation studies indicate that Additive attention can actively learn to explain\\nthe importance of its input hidden representations; (iii) when attention values\\nare nearly the same, the rank order of attention values is not consistent with\\nthe rank order of the mutual information(iv) Using Gumbel-Softmax with a\\ntemperature lower than one, tends to produce a more skewed attention score\\ndistribution compared to softmax and hence is a better choice for explainable\\ndesign; (v) some building blocks are better at preserving the correlation\\nbetween the ordered list of mutual information and attention weights order (for\\ne.g., the combination of BiLSTM encoder and Additive attention). Our findings\\nindicate that attention mechanisms do have the potential to function as a\\nshortcut to model explanations when they are carefully combined with other\\nmodel elements.\\n', 'publish_date': 'Mon, 31 Oct 2022 12:53:20 GMT'}\n",
    "\n",
    "Answer:\n",
    "\n",
    "==================================================\n",
    "生成嵌入中: 100%|██████████| 1/1 [00:00<00:00, 20.57it/s]\n",
    "[INFO] Query vector shape: (1, 1, 768)\n",
    "\n",
    "\n",
    "cost 0.1171 seconds\n",
    "\n",
    "\n",
    "You are a professional academic assistant specializing in the cs.CL domain. Strictly based on the provided literature information, answer the user's query. You may slightly expand and refine the response using existing knowledge, but avoid fabricating information. The response language should match the user's query language (English for English queries, Chinese for Chinese queries). Ensure the response is concise and limited to 500 words. Include a citation with the corresponding literature link (like [text](f\"https://arxiv.org/abs/{{id}}\")) and adjust the response style based on the temperature parameter (from 0.0 to 1.0). If the temperature is low (e.g., 0.2), provide a strict and precise answer. If the temperature is high (e.g., 0.8), provide a more creative and exploratory answer.\n",
    "\n",
    "Temperature: 0.2\n",
    "Question: 对比Transformer与RNN在机器翻译中的性能\n",
    "Relevant Literature: \n",
    "ID: 2502.00617, Distance: 0.33056533336639404, Content: {'title': 'Efficient Language Modeling for Low-Resource Settings with Hybrid\\n  RNN-Transformer Architectures', 'doi': None, 'categories': 'cs.CL', 'abstract': \"  Transformer-based language models have recently been at the forefront of\\nactive research in text generation. However, these models' advances come at the\\nprice of prohibitive training costs, with parameter counts in the billions and\\ncompute requirements measured in petaflop/s-decades. In this paper, we\\ninvestigate transformer-based architectures for improving model performance in\\na low-data regime by selectively replacing attention layers with feed-forward\\nand quasi-recurrent neural network layers. We test these architectures on the\\nstandard Enwik8 and Wikitext-103 corpora. Our results show that our reduced\\narchitectures outperform existing models with a comparable number of\\nparameters, and obtain comparable performance to larger models while\\nsignificantly reducing the number of parameters.\\n\", 'publish_date': 'Sun, 2 Feb 2025 01:05:09 GMT'}\n",
    "ID: 2403.01985, Distance: 0.3450363874435425, Content: {'title': \"Transformers for Low-Resource Languages: Is F\\\\'eidir Linn!\", 'doi': None, 'categories': 'cs.CL cs.AI', 'abstract': \"  The Transformer model is the state-of-the-art in Machine Translation.\\nHowever, in general, neural translation models often under perform on language\\npairs with insufficient training data. As a consequence, relatively few\\nexperiments have been carried out using this architecture on low-resource\\nlanguage pairs. In this study, hyperparameter optimization of Transformer\\nmodels in translating the low-resource English-Irish language pair is\\nevaluated. We demonstrate that choosing appropriate parameters leads to\\nconsiderable performance improvements. Most importantly, the correct choice of\\nsubword model is shown to be the biggest driver of translation performance.\\nSentencePiece models using both unigram and BPE approaches were appraised.\\nVariations on model architectures included modifying the number of layers,\\ntesting various regularisation techniques and evaluating the optimal number of\\nheads for attention. A generic 55k DGT corpus and an in-domain 88k public admin\\ncorpus were used for evaluation. A Transformer optimized model demonstrated a\\nBLEU score improvement of 7.8 points when compared with a baseline RNN model.\\nImprovements were observed across a range of metrics, including TER, indicating\\na substantially reduced post editing effort for Transformer optimized models\\nwith 16k BPE subword models. Bench-marked against Google Translate, our\\ntranslation engines demonstrated significant improvements. The question of\\nwhether or not Transformers can be used effectively in a low-resource setting\\nof English-Irish translation has been addressed. Is f\\\\'eidir linn - yes we can.\\n\", 'publish_date': 'Mon, 4 Mar 2024 12:29:59 GMT'}\n",
    "ID: 1807.03819, Distance: 0.3667772114276886, Content: {'title': 'Universal Transformers', 'doi': None, 'categories': 'cs.CL cs.LG stat.ML', 'abstract': '  Recurrent neural networks (RNNs) sequentially process data by updating their\\nstate with each new data point, and have long been the de facto choice for\\nsequence modeling tasks. However, their inherently sequential computation makes\\nthem slow to train. Feed-forward and convolutional architectures have recently\\nbeen shown to achieve superior results on some sequence modeling tasks such as\\nmachine translation, with the added advantage that they concurrently process\\nall inputs in the sequence, leading to easy parallelization and faster training\\ntimes. Despite these successes, however, popular feed-forward sequence models\\nlike the Transformer fail to generalize in many simple tasks that recurrent\\nmodels handle with ease, e.g. copying strings or even simple logical inference\\nwhen the string or formula lengths exceed those observed at training time. We\\npropose the Universal Transformer (UT), a parallel-in-time self-attentive\\nrecurrent sequence model which can be cast as a generalization of the\\nTransformer model and which addresses these issues. UTs combine the\\nparallelizability and global receptive field of feed-forward sequence models\\nlike the Transformer with the recurrent inductive bias of RNNs. We also add a\\ndynamic per-position halting mechanism and find that it improves accuracy on\\nseveral tasks. In contrast to the standard Transformer, under certain\\nassumptions, UTs can be shown to be Turing-complete. Our experiments show that\\nUTs outperform standard Transformers on a wide range of algorithmic and\\nlanguage understanding tasks, including the challenging LAMBADA language\\nmodeling task where UTs achieve a new state of the art, and machine translation\\nwhere UTs achieve a 0.9 BLEU improvement over Transformers on the WMT14 En-De\\ndataset.\\n', 'publish_date': 'Tue, 10 Jul 2018 18:39:15 GMT'}\n",
    "ID: 2103.13076, Distance: 0.38173413276672363, Content: {'title': 'Finetuning Pretrained Transformers into RNNs', 'doi': None, 'categories': 'cs.CL', 'abstract': \"  Transformers have outperformed recurrent neural networks (RNNs) in natural\\nlanguage generation. But this comes with a significant computational cost, as\\nthe attention mechanism's complexity scales quadratically with sequence length.\\nEfficient transformer variants have received increasing interest in recent\\nworks. Among them, a linear-complexity recurrent variant has proven well suited\\nfor autoregressive generation. It approximates the softmax attention with\\nrandomized or heuristic feature maps, but can be difficult to train and may\\nyield suboptimal accuracy. This work aims to convert a pretrained transformer\\ninto its efficient recurrent counterpart, improving efficiency while\\nmaintaining accuracy. Specifically, we propose a swap-then-finetune procedure:\\nin an off-the-shelf pretrained transformer, we replace the softmax attention\\nwith its linear-complexity recurrent alternative and then finetune. With a\\nlearned feature map, our approach provides an improved tradeoff between\\nefficiency and accuracy over the standard transformer and other recurrent\\nvariants. We also show that the finetuning process has lower training cost\\nrelative to training these recurrent variants from scratch. As many models for\\nnatural language tasks are increasingly dependent on large-scale pretrained\\ntransformers, this work presents a viable approach to improving inference\\nefficiency without repeating the expensive pretraining process.\\n\", 'publish_date': 'Wed, 24 Mar 2021 10:50:43 GMT'}\n",
    "ID: 1904.09408, Distance: 0.38406863808631897, Content: {'title': 'Language Models with Transformers', 'doi': None, 'categories': 'cs.CL cs.AI cs.LG', 'abstract': '  The Transformer architecture is superior to RNN-based models in computational\\nefficiency. Recently, GPT and BERT demonstrate the efficacy of Transformer\\nmodels on various NLP tasks using pre-trained language models on large-scale\\ncorpora. Surprisingly, these Transformer architectures are suboptimal for\\nlanguage model itself. Neither self-attention nor the positional encoding in\\nthe Transformer is able to efficiently incorporate the word-level sequential\\ncontext crucial to language modeling.\\n  In this paper, we explore effective Transformer architectures for language\\nmodel, including adding additional LSTM layers to better capture the sequential\\ncontext while still keeping the computation efficient. We propose Coordinate\\nArchitecture Search (CAS) to find an effective architecture through iterative\\nrefinement of the model. Experimental results on the PTB, WikiText-2, and\\nWikiText-103 show that CAS achieves perplexities between 20.42 and 34.11 on all\\nproblems, i.e. on average an improvement of 12.0 perplexity units compared to\\nstate-of-the-art LSTMs. The source code is publicly available.\\n', 'publish_date': 'Sat, 20 Apr 2019 06:43:14 GMT'}\n",
    "\n",
    "Answer:\n",
    "\n",
    "==================================================\n",
    "生成嵌入中: 100%|██████████| 1/1 [00:00<00:00, 11.56it/s]\n",
    "[INFO] Query vector shape: (1, 1, 768)\n",
    "\n",
    "\n",
    "cost 0.1527 seconds\n",
    "\n",
    "\n",
    "You are a professional academic assistant specializing in the cs.CL domain. Strictly based on the provided literature information, answer the user's query. You may slightly expand and refine the response using existing knowledge, but avoid fabricating information. The response language should match the user's query language (English for English queries, Chinese for Chinese queries). Ensure the response is concise and limited to 500 words. Include a citation with the corresponding literature link (like [text](f\"https://arxiv.org/abs/{{id}}\")) and adjust the response style based on the temperature parameter (from 0.0 to 1.0). If the temperature is low (e.g., 0.2), provide a strict and precise answer. If the temperature is high (e.g., 0.8), provide a more creative and exploratory answer.\n",
    "\n",
    "Temperature: 0.2\n",
    "Question: 大语言模型的 “幻觉” 问题本质是语义理解缺陷还是推理过程偏差？如何构建可靠性评估框架？\n",
    "Relevant Literature: \n",
    "ID: 2403.20009, Distance: 0.29913201928138733, Content: {'title': \"On Large Language Models' Hallucination with Regard to Known Facts\", 'doi': None, 'categories': 'cs.CL cs.LG', 'abstract': \"  Large language models are successful in answering factoid questions but are\\nalso prone to hallucination. We investigate the phenomenon of LLMs possessing\\ncorrect answer knowledge yet still hallucinating from the perspective of\\ninference dynamics, an area not previously covered in studies on\\nhallucinations. We are able to conduct this analysis via two key ideas. First,\\nwe identify the factual questions that query the same triplet knowledge but\\nresult in different answers. The difference between the model behaviors on the\\ncorrect and incorrect outputs hence suggests the patterns when hallucinations\\nhappen. Second, to measure the pattern, we utilize mappings from the residual\\nstreams to vocabulary space. We reveal the different dynamics of the output\\ntoken probabilities along the depths of layers between the correct and\\nhallucinated cases. In hallucinated cases, the output token's information\\nrarely demonstrates abrupt increases and consistent superiority in the later\\nstages of the model. Leveraging the dynamic curve as a feature, we build a\\nclassifier capable of accurately detecting hallucinatory predictions with an\\n88\\\\% success rate. Our study shed light on understanding the reasons for LLMs'\\nhallucinations on their known facts, and more importantly, on accurately\\npredicting when they are hallucinating.\\n\", 'publish_date': 'Fri, 29 Mar 2024 06:48:30 GMT'}\n",
    "ID: 2305.14552, Distance: 0.3027987480163574, Content: {'title': 'Sources of Hallucination by Large Language Models on Inference Tasks', 'doi': None, 'categories': 'cs.CL cs.AI', 'abstract': \"  Large Language Models (LLMs) are claimed to be capable of Natural Language\\nInference (NLI), necessary for applied tasks like question answering and\\nsummarization. We present a series of behavioral studies on several LLM\\nfamilies (LLaMA, GPT-3.5, and PaLM) which probe their behavior using controlled\\nexperiments. We establish two biases originating from pretraining which predict\\nmuch of their behavior, and show that these are major sources of hallucination\\nin generative LLMs. First, memorization at the level of sentences: we show\\nthat, regardless of the premise, models falsely label NLI test samples as\\nentailing when the hypothesis is attested in training data, and that entities\\nare used as ``indices'' to access the memorized data. Second, statistical\\npatterns of usage learned at the level of corpora: we further show a similar\\neffect when the premise predicate is less frequent than that of the hypothesis\\nin the training data, a bias following from previous studies. We demonstrate\\nthat LLMs perform significantly worse on NLI test samples which do not conform\\nto these biases than those which do, and we offer these as valuable controls\\nfor future LLM evaluation.\\n\", 'publish_date': 'Tue, 23 May 2023 22:24:44 GMT'}\n",
    "ID: 2402.10543, Distance: 0.30965283513069153, Content: {'title': 'Strong hallucinations from negation and how to fix them', 'doi': None, 'categories': 'cs.CL cs.AI', 'abstract': \"  Despite great performance on many tasks, language models (LMs) still struggle\\nwith reasoning, sometimes providing responses that cannot possibly be true\\nbecause they stem from logical incoherence. We call such responses\\n\\\\textit{strong hallucinations} and prove that they follow from an LM's\\ncomputation of its internal representations for logical operators and outputs\\nfrom those representations. Focusing on negation, we provide a novel solution\\nin which negation is treated not as another element of a latent representation,\\nbut as \\\\textit{an operation over an LM's latent representations that constrains\\nhow they may evolve}. We show that our approach improves model performance in\\ncloze prompting and natural language inference tasks with negation without\\nrequiring training on sparse negative data.\\n\", 'publish_date': 'Fri, 16 Feb 2024 10:11:20 GMT'}\n",
    "ID: 2311.14648, Distance: 0.31446754932403564, Content: {'title': 'Calibrated Language Models Must Hallucinate', 'doi': None, 'categories': 'cs.CL cs.AI', 'abstract': '  Recent language models generate false but plausible-sounding text with\\nsurprising frequency. Such \"hallucinations\" are an obstacle to the usability of\\nlanguage-based AI systems and can harm people who rely upon their outputs. This\\nwork shows that there is an inherent statistical lower-bound on the rate that\\npretrained language models hallucinate certain types of facts, having nothing\\nto do with the transformer LM architecture or data quality. For \"arbitrary\"\\nfacts whose veracity cannot be determined from the training data, we show that\\nhallucinations must occur at a certain rate for language models that satisfy a\\nstatistical calibration condition appropriate for generative language models.\\nSpecifically, if the maximum probability of any fact is bounded, we show that\\nthe probability of generating a hallucination is close to the fraction of facts\\nthat occur exactly once in the training data (a \"Good-Turing\" estimate), even\\nassuming ideal training data without errors.\\n  One conclusion is that models pretrained to be sufficiently good predictors\\n(i.e., calibrated) may require post-training to mitigate hallucinations on the\\ntype of arbitrary facts that tend to appear once in the training set. However,\\nour analysis also suggests that there is no statistical reason that pretraining\\nwill lead to hallucination on facts that tend to appear more than once in the\\ntraining data (like references to publications such as articles and books,\\nwhose hallucinations have been particularly notable and problematic) or on\\nsystematic facts (like arithmetic calculations). Therefore, different\\narchitectures and learning algorithms may mitigate these latter types of\\nhallucinations.\\n', 'publish_date': 'Fri, 24 Nov 2023 18:29:50 GMT'}\n",
    "ID: 2401.11817, Distance: 0.3182944655418396, Content: {'title': 'Hallucination is Inevitable: An Innate Limitation of Large Language\\n  Models', 'doi': None, 'categories': 'cs.CL cs.AI cs.LG', 'abstract': '  Hallucination has been widely recognized to be a significant drawback for\\nlarge language models (LLMs). There have been many works that attempt to reduce\\nthe extent of hallucination. These efforts have mostly been empirical so far,\\nwhich cannot answer the fundamental question whether it can be completely\\neliminated. In this paper, we formalize the problem and show that it is\\nimpossible to eliminate hallucination in LLMs. Specifically, we define a formal\\nworld where hallucination is defined as inconsistencies between a computable\\nLLM and a computable ground truth function. By employing results from learning\\ntheory, we show that LLMs cannot learn all the computable functions and will\\ntherefore inevitably hallucinate if used as general problem solvers. Since the\\nformal world is a part of the real world which is much more complicated,\\nhallucinations are also inevitable for real world LLMs. Furthermore, for real\\nworld LLMs constrained by provable time complexity, we describe the\\nhallucination-prone tasks and empirically validate our claims. Finally, using\\nthe formal world framework, we discuss the possible mechanisms and efficacies\\nof existing hallucination mitigators as well as the practical implications on\\nthe safe deployment of LLMs.\\n', 'publish_date': 'Mon, 22 Jan 2024 10:26:14 GMT'}\n",
    "\n",
    "Answer:\n",
    "\n",
    "==================================================\n",
    "生成嵌入中: 100%|██████████| 1/1 [00:00<00:00, 14.52it/s]\n",
    "[INFO] Query vector shape: (1, 1, 768)\n",
    "\n",
    "\n",
    "cost 0.1314 seconds\n",
    "\n",
    "\n",
    "You are a professional academic assistant specializing in the cs.CL domain. Strictly based on the provided literature information, answer the user's query. You may slightly expand and refine the response using existing knowledge, but avoid fabricating information. The response language should match the user's query language (English for English queries, Chinese for Chinese queries). Ensure the response is concise and limited to 500 words. Include a citation with the corresponding literature link (like [text](f\"https://arxiv.org/abs/{{id}}\")) and adjust the response style based on the temperature parameter (from 0.0 to 1.0). If the temperature is low (e.g., 0.2), provide a strict and precise answer. If the temperature is high (e.g., 0.8), provide a more creative and exploratory answer.\n",
    "\n",
    "Temperature: 0.2\n",
    "Question: 零样本学习中，提示词工程能否真正激活模型的 “潜在知识”？其理论边界在哪？\n",
    "Relevant Literature: \n",
    "ID: 2209.15206, Distance: 0.38994061946868896, Content: {'title': 'What Makes Pre-trained Language Models Better Zero-shot Learners?', 'doi': None, 'categories': 'cs.CL cs.AI', 'abstract': '  Current methods for prompt learning in zeroshot scenarios widely rely on a\\ndevelopment set with sufficient human-annotated data to select the\\nbest-performing prompt template a posteriori. This is not ideal because in a\\nrealworld zero-shot scenario of practical relevance, no labelled data is\\navailable. Thus, we propose a simple yet effective method for screening\\nreasonable prompt templates in zero-shot text classification: Perplexity\\nSelection (Perplection). We hypothesize that language discrepancy can be used\\nto measure the efficacy of prompt templates, and thereby develop a\\nsubstantiated perplexity-based scheme allowing for forecasting the performance\\nof prompt templates in advance. Experiments show that our method leads to\\nimproved prediction performance in a realistic zero-shot setting, eliminating\\nthe need for any labelled examples.\\n', 'publish_date': 'Fri, 30 Sep 2022 03:28:19 GMT'}\n",
    "ID: 2109.01247, Distance: 0.3977474570274353, Content: {'title': 'Do Prompt-Based Models Really Understand the Meaning of their Prompts?', 'doi': None, 'categories': 'cs.CL', 'abstract': '  Recently, a boom of papers has shown extraordinary progress in zero-shot and\\nfew-shot learning with various prompt-based models. It is commonly argued that\\nprompts help models to learn faster in the same way that humans learn faster\\nwhen provided with task instructions expressed in natural language. In this\\nstudy, we experiment with over 30 prompt templates manually written for natural\\nlanguage inference (NLI). We find that models learn just as fast with many\\nprompts that are intentionally irrelevant or even pathologically misleading as\\nthey do with instructively \"good\" prompts. Further, such patterns hold even for\\nmodels as large as 175 billion parameters (Brown et al., 2020) as well as the\\nrecently proposed instruction-tuned models which are trained on hundreds of\\nprompts (Sanh et al., 2022). That is, instruction-tuned models often produce\\ngood predictions with irrelevant and misleading prompts even at zero shots. In\\nsum, notwithstanding prompt-based models\\' impressive improvement, we find\\nevidence of serious limitations that question the degree to which such\\nimprovement is derived from models understanding task instructions in ways\\nanalogous to humans\\' use of task instructions.\\n', 'publish_date': 'Thu, 2 Sep 2021 23:46:36 GMT'}\n",
    "ID: 2305.14310, Distance: 0.4021965265274048, Content: {'title': 'Navigating Prompt Complexity for Zero-Shot Classification: A Study of\\n  Large Language Models in Computational Social Science', 'doi': None, 'categories': 'cs.CL', 'abstract': '  Instruction-tuned Large Language Models (LLMs) have exhibited impressive\\nlanguage understanding and the capacity to generate responses that follow\\nspecific prompts. However, due to the computational demands associated with\\ntraining these models, their applications often adopt a zero-shot setting. In\\nthis paper, we evaluate the zero-shot performance of two publicly accessible\\nLLMs, ChatGPT and OpenAssistant, in the context of six Computational Social\\nScience classification tasks, while also investigating the effects of various\\nprompting strategies. Our experiments investigate the impact of prompt\\ncomplexity, including the effect of incorporating label definitions into the\\nprompt; use of synonyms for label names; and the influence of integrating past\\nmemories during foundation model training. The findings indicate that in a\\nzero-shot setting, current LLMs are unable to match the performance of smaller,\\nfine-tuned baseline transformer models (such as BERT-large). Additionally, we\\nfind that different prompting strategies can significantly affect\\nclassification accuracy, with variations in accuracy and F1 scores exceeding\\n10\\\\%.\\n', 'publish_date': 'Tue, 23 May 2023 17:48:21 GMT'}\n",
    "ID: 2309.13205, Distance: 0.4152686297893524, Content: {'title': 'A Practical Survey on Zero-shot Prompt Design for In-context Learning', 'doi': '10.26615/978-954-452-092-2_069', 'categories': 'cs.CL cs.AI cs.ET cs.LG', 'abstract': '  The remarkable advancements in large language models (LLMs) have brought\\nabout significant improvements in Natural Language Processing(NLP) tasks. This\\npaper presents a comprehensive review of in-context learning techniques,\\nfocusing on different types of prompts, including discrete, continuous,\\nfew-shot, and zero-shot, and their impact on LLM performance. We explore\\nvarious approaches to prompt design, such as manual design, optimization\\nalgorithms, and evaluation methods, to optimize LLM performance across diverse\\ntasks. Our review covers key research studies in prompt engineering, discussing\\ntheir methodologies and contributions to the field. We also delve into the\\nchallenges faced in evaluating prompt performance, given the absence of a\\nsingle \"best\" prompt and the importance of considering multiple metrics. In\\nconclusion, the paper highlights the critical role of prompt design in\\nharnessing the full potential of LLMs and provides insights into the\\ncombination of manual design, optimization techniques, and rigorous evaluation\\nfor more effective and efficient use of LLMs in various NLP tasks.\\n', 'publish_date': 'Fri, 22 Sep 2023 23:00:34 GMT'}\n",
    "ID: 2210.14803, Distance: 0.41742053627967834, Content: {'title': \"Don't Prompt, Search! Mining-based Zero-Shot Learning with Language\\n  Models\", 'doi': None, 'categories': 'cs.CL cs.AI cs.LG', 'abstract': '  Masked language models like BERT can perform text classification in a\\nzero-shot fashion by reformulating downstream tasks as text infilling. However,\\nthis approach is highly sensitive to the template used to prompt the model, yet\\npractitioners are blind when designing them in strict zero-shot settings. In\\nthis paper, we propose an alternative mining-based approach for zero-shot\\nlearning. Instead of prompting language models, we use regular expressions to\\nmine labeled examples from unlabeled corpora, which can optionally be filtered\\nthrough prompting, and used to finetune a pretrained model. Our method is more\\nflexible and interpretable than prompting, and outperforms it on a wide range\\nof tasks when using comparable templates. Our results suggest that the success\\nof prompting can partly be explained by the model being exposed to similar\\nexamples during pretraining, which can be directly retrieved through regular\\nexpressions.\\n', 'publish_date': 'Wed, 26 Oct 2022 15:52:30 GMT'}\n",
    "\n",
    "Answer:\n",
    "\n",
    "==================================================\n",
    "生成嵌入中: 100%|██████████| 1/1 [00:00<00:00, 12.69it/s]\n",
    "[INFO] Query vector shape: (1, 1, 768)\n",
    "\n",
    "\n",
    "cost 0.1446 seconds\n",
    "\n",
    "\n",
    "You are a professional academic assistant specializing in the cs.CL domain. Strictly based on the provided literature information, answer the user's query. You may slightly expand and refine the response using existing knowledge, but avoid fabricating information. The response language should match the user's query language (English for English queries, Chinese for Chinese queries). Ensure the response is concise and limited to 500 words. Include a citation with the corresponding literature link (like [text](f\"https://arxiv.org/abs/{{id}}\")) and adjust the response style based on the temperature parameter (from 0.0 to 1.0). If the temperature is low (e.g., 0.2), provide a strict and precise answer. If the temperature is high (e.g., 0.8), provide a more creative and exploratory answer.\n",
    "\n",
    "Temperature: 0.2\n",
    "Question: 小样本场景下，元学习与预训练模型的适应性调整策略如何结合？\n",
    "Relevant Literature: \n",
    "ID: 2305.14521, Distance: 0.41722217202186584, Content: {'title': 'Few-shot Adaptation to Distribution Shifts By Mixing Source and Target\\n  Embeddings', 'doi': None, 'categories': 'cs.LG cs.CL cs.CV', 'abstract': '  Pretrained machine learning models need to be adapted to distribution shifts\\nwhen deployed in new target environments. When obtaining labeled data from the\\ntarget distribution is expensive, few-shot adaptation with only a few examples\\nfrom the target distribution becomes essential. In this work, we propose\\nMixPro, a lightweight and highly data-efficient approach for few-shot\\nadaptation. MixPro first generates a relatively large dataset by mixing\\n(linearly combining) pre-trained embeddings of large source data with those of\\nthe few target examples. This process preserves important features of both\\nsource and target distributions, while mitigating the specific noise in the\\nsmall target data. Then, it trains a linear classifier on the mixed embeddings\\nto effectively adapts the model to the target distribution without overfitting\\nthe small target data. Theoretically, we demonstrate the advantages of MixPro\\nover previous methods. Our experiments, conducted across various model\\narchitectures on 8 datasets featuring different types of distribution shifts,\\nreveal that MixPro can outperform baselines by up to 7\\\\%, with only 2-4 target\\nexamples.\\n', 'publish_date': 'Tue, 23 May 2023 20:49:45 GMT'}\n",
    "ID: 2204.03044, Distance: 0.42667075991630554, Content: {'title': 'Fusing finetuned models for better pretraining', 'doi': None, 'categories': 'cs.CL cs.CV cs.LG', 'abstract': '  Pretrained models are the standard starting point for training. This approach\\nconsistently outperforms the use of a random initialization. However,\\npretraining is a costly endeavour that few can undertake.\\n  In this paper, we create better base models at hardly any cost, by fusing\\nmultiple existing fine tuned models into one. Specifically, we fuse by\\naveraging the weights of these models. We show that the fused model results\\nsurpass the pretrained model ones. We also show that fusing is often better\\nthan intertraining.\\n  We find that fusing is less dependent on the target task. Furthermore, weight\\ndecay nullifies intertraining effects but not those of fusing.\\n', 'publish_date': 'Wed, 6 Apr 2022 18:54:48 GMT'}\n",
    "ID: 1908.03265, Distance: 0.4296816289424896, Content: {'title': 'On the Variance of the Adaptive Learning Rate and Beyond', 'doi': None, 'categories': 'cs.LG cs.CL stat.ML', 'abstract': '  The learning rate warmup heuristic achieves remarkable success in stabilizing\\ntraining, accelerating convergence and improving generalization for adaptive\\nstochastic optimization algorithms like RMSprop and Adam. Here, we study its\\nmechanism in details. Pursuing the theory behind warmup, we identify a problem\\nof the adaptive learning rate (i.e., it has problematically large variance in\\nthe early stage), suggest warmup works as a variance reduction technique, and\\nprovide both empirical and theoretical evidence to verify our hypothesis. We\\nfurther propose RAdam, a new variant of Adam, by introducing a term to rectify\\nthe variance of the adaptive learning rate. Extensive experimental results on\\nimage classification, language modeling, and neural machine translation verify\\nour intuition and demonstrate the effectiveness and robustness of our proposed\\nmethod. All implementations are available at:\\nhttps://github.com/LiyuanLucasLiu/RAdam.\\n', 'publish_date': 'Thu, 8 Aug 2019 20:51:17 GMT'}\n",
    "ID: 2311.11973, Distance: 0.45514047145843506, Content: {'title': 'Adaptive Training Distributions with Scalable Online Bilevel\\n  Optimization', 'doi': None, 'categories': 'cs.LG cs.CL', 'abstract': '  Large neural networks pretrained on web-scale corpora are central to modern\\nmachine learning. In this paradigm, the distribution of the large,\\nheterogeneous pretraining data rarely matches that of the application domain.\\nThis work considers modifying the pretraining distribution in the case where\\none has a small sample of data reflecting the targeted test conditions. We\\npropose an algorithm motivated by a recent formulation of this setting as an\\nonline, bilevel optimization problem. With scalability in mind, our algorithm\\nprioritizes computing gradients at training points which are likely to most\\nimprove the loss on the targeted distribution. Empirically, we show that in\\nsome cases this approach is beneficial over existing strategies from the domain\\nadaptation literature but may not succeed in other cases. We propose a simple\\ntest to evaluate when our approach can be expected to work well and point\\ntowards further research to address current limitations.\\n', 'publish_date': 'Mon, 20 Nov 2023 18:01:29 GMT'}\n",
    "ID: cmp-lg/9806003, Distance: 0.4585559070110321, Content: {'title': 'Lazy Transformation-Based Learning', 'doi': None, 'categories': 'cmp-lg cs.CL', 'abstract': '  We introduce a significant improvement for a relatively new machine learning\\nmethod called Transformation-Based Learning. By applying a Monte Carlo strategy\\nto randomly sample from the space of rules, rather than exhaustively analyzing\\nall possible rules, we drastically reduce the memory and time costs of the\\nalgorithm, without compromising accuracy on unseen data. This enables\\nTransformation- Based Learning to apply to a wider range of domains, as it can\\neffectively consider a larger number of different features and feature\\ninteractions in the data. In addition, the Monte Carlo improvement decreases\\nthe labor demands on the human developer, who no longer needs to develop a\\nminimal set of rule templates to maintain tractability.\\n', 'publish_date': 'Wed, 3 Jun 1998 16:47:37 GMT'}\n",
    "\n",
    "Answer:\n",
    "\n",
    "==================================================\n",
    "生成嵌入中: 100%|██████████| 1/1 [00:00<00:00, 13.16it/s]\n",
    "[INFO] Query vector shape: (1, 1, 768)\n",
    "\n",
    "\n",
    "cost 0.1351 seconds\n",
    "\n",
    "\n",
    "You are a professional academic assistant specializing in the cs.CL domain. Strictly based on the provided literature information, answer the user's query. You may slightly expand and refine the response using existing knowledge, but avoid fabricating information. The response language should match the user's query language (English for English queries, Chinese for Chinese queries). Ensure the response is concise and limited to 500 words. Include a citation with the corresponding literature link (like [text](f\"https://arxiv.org/abs/{{id}}\")) and adjust the response style based on the temperature parameter (from 0.0 to 1.0). If the temperature is low (e.g., 0.2), provide a strict and precise answer. If the temperature is high (e.g., 0.8), provide a more creative and exploratory answer.\n",
    "\n",
    "Temperature: 0.2\n",
    "Question: 预训练模型的 “涌现能力” 是否具有可预测性？如何从理论上解释其突现机制？\n",
    "Relevant Literature: \n",
    "ID: 1512.01926, Distance: 0.49014726281166077, Content: {'title': 'Thinking Required', 'doi': None, 'categories': 'cs.LG cs.AI cs.CL', 'abstract': '  There exists a theory of a single general-purpose learning algorithm which\\ncould explain the principles its operation. It assumes the initial rough\\narchitecture, a small library of simple innate circuits which are prewired at\\nbirth. and proposes that all significant mental algorithms are learned. Given\\ncurrent understanding and observations, this paper reviews and lists the\\ningredients of such an algorithm from architectural and functional\\nperspectives.\\n', 'publish_date': 'Mon, 7 Dec 2015 06:37:49 GMT'}\n",
    "ID: 2505.09855, Distance: 0.49173110723495483, Content: {'title': 'Predictability Shapes Adaptation: An Evolutionary Perspective on Modes\\n  of Learning in Transformers', 'doi': None, 'categories': 'cs.LG cs.AI cs.CL', 'abstract': \"  Transformer models learn in two distinct modes: in-weights learning (IWL),\\nencoding knowledge into model weights, and in-context learning (ICL), adapting\\nflexibly to context without weight modification. To better understand the\\ninterplay between these learning modes, we draw inspiration from evolutionary\\nbiology's analogous adaptive strategies: genetic encoding (akin to IWL,\\nadapting over generations and fixed within an individual's lifetime) and\\nphenotypic plasticity (akin to ICL, enabling flexible behavioral responses to\\nenvironmental cues). In evolutionary biology, environmental predictability\\ndictates the balance between these strategies: stability favors genetic\\nencoding, while reliable predictive cues promote phenotypic plasticity. We\\nexperimentally operationalize these dimensions of predictability and\\nsystematically investigate their influence on the ICL/IWL balance in\\nTransformers. Using regression and classification tasks, we show that high\\nenvironmental stability decisively favors IWL, as predicted, with a sharp\\ntransition at maximal stability. Conversely, high cue reliability enhances ICL\\nefficacy, particularly when stability is low. Furthermore, learning dynamics\\nreveal task-contingent temporal evolution: while a canonical ICL-to-IWL shift\\noccurs in some settings (e.g., classification with many classes), we\\ndemonstrate that scenarios with easier IWL (e.g., fewer classes) or slower ICL\\nacquisition (e.g., regression) can exhibit an initial IWL phase later yielding\\nto ICL dominance. These findings support a relative-cost hypothesis for\\nexplaining these learning mode transitions, establishing predictability as a\\ncritical factor governing adaptive strategies in Transformers, and offering\\nnovel insights for understanding ICL and guiding training methodologies.\\n\", 'publish_date': 'Wed, 14 May 2025 23:31:17 GMT'}\n",
    "ID: 1511.06379, Distance: 0.4974609911441803, Content: {'title': 'Dynamic Adaptive Network Intelligence', 'doi': None, 'categories': 'cs.CL cs.LG', 'abstract': '  Accurate representational learning of both the explicit and implicit\\nrelationships within data is critical to the ability of machines to perform\\nmore complex and abstract reasoning tasks. We describe the efficient weakly\\nsupervised learning of such inferences by our Dynamic Adaptive Network\\nIntelligence (DANI) model. We report state-of-the-art results for DANI over\\nquestion answering tasks in the bAbI dataset that have proved difficult for\\ncontemporary approaches to learning representation (Weston et al., 2015).\\n', 'publish_date': 'Thu, 19 Nov 2015 21:07:27 GMT'}\n",
    "ID: 2305.18390, Distance: 0.5104956030845642, Content: {'title': 'Emergent Modularity in Pre-trained Transformers', 'doi': None, 'categories': 'cs.CL cs.LG', 'abstract': '  This work examines the presence of modularity in pre-trained Transformers, a\\nfeature commonly found in human brains and thought to be vital for general\\nintelligence. In analogy to human brains, we consider two main characteristics\\nof modularity: (1) functional specialization of neurons: we evaluate whether\\neach neuron is mainly specialized in a certain function, and find that the\\nanswer is yes. (2) function-based neuron grouping: we explore finding a\\nstructure that groups neurons into modules by function, and each module works\\nfor its corresponding function. Given the enormous amount of possible\\nstructures, we focus on Mixture-of-Experts as a promising candidate, which\\npartitions neurons into experts and usually activates different experts for\\ndifferent inputs. Experimental results show that there are functional experts,\\nwhere clustered are the neurons specialized in a certain function. Moreover,\\nperturbing the activations of functional experts significantly affects the\\ncorresponding function. Finally, we study how modularity emerges during\\npre-training, and find that the modular structure is stabilized at the early\\nstage, which is faster than neuron stabilization. It suggests that Transformers\\nfirst construct the modular structure and then learn fine-grained neuron\\nfunctions. Our code and data are available at\\nhttps://github.com/THUNLP/modularity-analysis.\\n', 'publish_date': 'Sun, 28 May 2023 11:02:32 GMT'}\n",
    "ID: 2201.10222, Distance: 0.5105546712875366, Content: {'title': 'Explanatory Learning: Beyond Empiricism in Neural Networks', 'doi': None, 'categories': 'cs.LG cs.AI cs.CL physics.hist-ph', 'abstract': '  We introduce Explanatory Learning (EL), a framework to let machines use\\nexisting knowledge buried in symbolic sequences -- e.g. explanations written in\\nhieroglyphic -- by autonomously learning to interpret them. In EL, the burden\\nof interpreting symbols is not left to humans or rigid human-coded compilers,\\nas done in Program Synthesis. Rather, EL calls for a learned interpreter, built\\nupon a limited collection of symbolic sequences paired with observations of\\nseveral phenomena. This interpreter can be used to make predictions on a novel\\nphenomenon given its explanation, and even to find that explanation using only\\na handful of observations, like human scientists do. We formulate the EL\\nproblem as a simple binary classification task, so that common end-to-end\\napproaches aligned with the dominant empiricist view of machine learning could,\\nin principle, solve it. To these models, we oppose Critical Rationalist\\nNetworks (CRNs), which instead embrace a rationalist view on the acquisition of\\nknowledge. CRNs express several desired properties by construction, they are\\ntruly explainable, can adjust their processing at test-time for harder\\ninferences, and can offer strong confidence guarantees on their predictions. As\\na final contribution, we introduce Odeen, a basic EL environment that simulates\\na small flatland-style universe full of phenomena to explain. Using Odeen as a\\ntestbed, we show how CRNs outperform empiricist end-to-end approaches of\\nsimilar size and architecture (Transformers) in discovering explanations for\\nnovel phenomena.\\n', 'publish_date': 'Tue, 25 Jan 2022 10:21:53 GMT'}\n",
    "\n",
    "Answer:\n",
    "\n",
    "==================================================\n",
    "生成嵌入中: 100%|██████████| 1/1 [00:00<00:00, 10.63it/s]\n",
    "[INFO] Query vector shape: (1, 1, 768)\n",
    "\n",
    "\n",
    "cost 0.1550 seconds\n",
    "\n",
    "\n",
    "You are a professional academic assistant specializing in the cs.CL domain. Strictly based on the provided literature information, answer the user's query. You may slightly expand and refine the response using existing knowledge, but avoid fabricating information. The response language should match the user's query language (English for English queries, Chinese for Chinese queries). Ensure the response is concise and limited to 500 words. Include a citation with the corresponding literature link (like [text](f\"https://arxiv.org/abs/{{id}}\")) and adjust the response style based on the temperature parameter (from 0.0 to 1.0). If the temperature is low (e.g., 0.2), provide a strict and precise answer. If the temperature is high (e.g., 0.8), provide a more creative and exploratory answer.\n",
    "\n",
    "Temperature: 0.2\n",
    "Question: 大模型的上下文窗口扩展是否会引入 “长距离语义稀释”？如何设计高效的长期依赖建模机制？\n",
    "Relevant Literature: \n",
    "ID: 2405.17915, Distance: 0.3706572949886322, Content: {'title': 'Long Context is Not Long at All: A Prospector of Long-Dependency Data\\n  for Large Language Models', 'doi': None, 'categories': 'cs.CL', 'abstract': '  Long-context modeling capabilities are important for large language models\\n(LLMs) in various applications. However, directly training LLMs with long\\ncontext windows is insufficient to enhance this capability since some training\\nsamples do not exhibit strong semantic dependencies across long contexts. In\\nthis study, we propose a data mining framework \\\\textbf{ProLong} that can assign\\neach training sample with a long dependency score, which can be used to rank\\nand filter samples that are more advantageous for enhancing long-context\\nmodeling abilities in LLM training. Specifically, we first use delta perplexity\\nscores to measure the \\\\textit{Dependency Strength} between text segments in a\\ngiven document. Then we refine this metric based on the \\\\textit{Dependency\\nDistance} of these segments to incorporate spatial relationships across\\nlong-contexts. Final results are calibrated with a \\\\textit{Dependency\\nSpecificity} metric to prevent trivial dependencies introduced by repetitive\\npatterns. Moreover, a random sampling approach is proposed to optimize the\\ncomputational efficiency of ProLong. Comprehensive experiments on multiple\\nbenchmarks indicate that ProLong effectively identifies documents that carry\\nlong dependencies and LLMs trained on these documents exhibit significantly\\nenhanced long-context modeling capabilities.\\n', 'publish_date': 'Tue, 28 May 2024 07:36:56 GMT'}\n",
    "ID: 2312.09571, Distance: 0.3711542785167694, Content: {'title': 'Extending Context Window of Large Language Models via Semantic\\n  Compression', 'doi': None, 'categories': 'cs.CL cs.IT math.IT', 'abstract': '  Transformer-based Large Language Models (LLMs) often impose limitations on\\nthe length of the text input to ensure the generation of fluent and relevant\\nresponses. This constraint restricts their applicability in scenarios involving\\nlong texts. We propose a novel semantic compression method that enables\\ngeneralization to texts that are 6-8 times longer, without incurring\\nsignificant computational costs or requiring fine-tuning. Our proposed\\nframework draws inspiration from source coding in information theory and\\nemploys a pre-trained model to reduce the semantic redundancy of long inputs\\nbefore passing them to the LLMs for downstream tasks. Experimental results\\ndemonstrate that our method effectively extends the context window of LLMs\\nacross a range of tasks including question answering, summarization, few-shot\\nlearning, and information retrieval. Furthermore, the proposed semantic\\ncompression method exhibits consistent fluency in text generation while\\nreducing the associated computational overhead.\\n', 'publish_date': 'Fri, 15 Dec 2023 07:04:33 GMT'}\n",
    "ID: 2404.02060, Distance: 0.38668161630630493, Content: {'title': 'Long-context LLMs Struggle with Long In-context Learning', 'doi': None, 'categories': 'cs.CL cs.AI', 'abstract': '  Large Language Models (LLMs) have made significant strides in handling long\\nsequences. Some models like Gemini could even to be capable of dealing with\\nmillions of tokens. However, their performance evaluation has largely been\\nconfined to metrics like perplexity and synthetic tasks, which may not fully\\ncapture their true abilities in more challenging, real-world scenarios. We\\nintroduce a benchmark (LongICLBench) for long in-context learning in\\nextreme-label classification using six datasets with 28 to 174 classes and\\ninput lengths from 2K to 50K tokens. Our benchmark requires LLMs to comprehend\\nthe entire input to recognize the massive label spaces to make correct\\npredictions. We evaluate on 15 long-context LLMs and find that they perform\\nwell on less challenging classification tasks with smaller label space and\\nshorter demonstrations. However, they struggle with more challenging task like\\nDiscovery with 174 labels, suggesting a gap in their ability to process long,\\ncontext-rich sequences. Further analysis reveals a bias towards labels\\npresented later in the sequence and a need for improved reasoning over multiple\\npieces of information. Our study reveals that long context understanding and\\nreasoning is still a challenging task for the existing LLMs. We believe\\nLongICLBench could serve as a more realistic evaluation for the future\\nlong-context LLMs.\\n', 'publish_date': 'Tue, 2 Apr 2024 15:59:11 GMT'}\n",
    "ID: 2311.04939, Distance: 0.389009952545166, Content: {'title': 'LooGLE: Can Long-Context Language Models Understand Long Contexts?', 'doi': None, 'categories': 'cs.CL cs.AI', 'abstract': '  Large language models (LLMs), despite their impressive performance in various\\nlanguage tasks, are typically limited to processing texts within context-window\\nsize. This limitation has spurred significant research efforts to enhance LLMs\\'\\nlong-context understanding with high-quality long-sequence benchmarks. However,\\nprior datasets in this regard suffer from shortcomings, such as short context\\nlength compared to the context window of modern LLMs; outdated documents that\\nhave data leakage problems; and an emphasis on short dependency tasks rather\\nthan long dependency tasks. In this paper, we present LooGLE, a Long Context\\nGeneric Language Evaluation benchmark for LLMs\\' long context understanding.\\nLooGLE features relatively new documents post-2022, with over 24,000 tokens per\\ndocument and 6,000 newly generated questions spanning diverse domains. Human\\nannotators meticulously crafted more than 1,100 high-quality question-answer\\npairs to meet the long dependency requirements. These pairs underwent thorough\\ncross-validation, yielding the most precise assessment of LLMs\\' long dependency\\ncapabilities. The evaluation of eight state-of-the-art LLMs on LooGLE revealed\\nkey findings: (i) commercial models outperformed open-sourced models; (ii) LLMs\\nexcelled in short dependency tasks like short question-answering and cloze\\ntasks but struggled with more intricate long dependency tasks; (iii) in-context\\nlearning and chaining thoughts offered only marginal improvements; (iv)\\nretrieval-based techniques demonstrated substantial benefits for short\\nquestion-answering, while strategies for extending context window length had\\nlimited impact on long context understanding. As such, LooGLE not only provides\\na systematic and comprehensive evaluation schema on long-context LLMs, but also\\nsheds light on future development of enhanced models towards \"true long-context\\nunderstanding\".\\n', 'publish_date': 'Wed, 8 Nov 2023 01:45:37 GMT'}\n",
    "ID: 2212.10947, Distance: 0.3892439305782318, Content: {'title': 'Parallel Context Windows for Large Language Models', 'doi': None, 'categories': 'cs.CL', 'abstract': \"  When applied to processing long text, Large Language Models (LLMs) are\\nlimited by their context window. Existing efforts to address this limitation\\ninvolve training specialized architectures, and cannot be easily applied to\\noff-the-shelf LLMs. We present Parallel Context Windows (PCW), a method that\\nalleviates the context window restriction for any off-the-shelf LLM without\\nfurther training. The key to the approach is to carve a long context into\\nchunks (``windows''), restrict the attention mechanism to apply only within\\neach window, and re-use the positional embeddings across the windows. Our main\\nresults test the PCW approach on in-context learning with models that range in\\nsize between 750 million and 178 billion parameters, and show substantial\\nimprovements for tasks with diverse input and output spaces. We show additional\\nbenefits in other settings where long context windows may be beneficial:\\nmulti-hop questions and retrieval-augmented question answering with multiple\\nretrieved documents. Our results highlight Parallel Context Windows as a\\npromising method for applying off-the-shelf LLMs in a range of settings that\\nrequire long text sequences. We make our code publicly available at\\nhttps://github.com/ai21labs/parallel-context-windows.\\n\", 'publish_date': 'Wed, 21 Dec 2022 11:38:51 GMT'}\n",
    "\n",
    "Answer:\n",
    "\n",
    "==================================================\n",
    "生成嵌入中: 100%|██████████| 1/1 [00:00<00:00, 11.66it/s]\n",
    "[INFO] Query vector shape: (1, 1, 768)\n",
    "\n",
    "\n",
    "cost 0.1448 seconds\n",
    "\n",
    "\n",
    "You are a professional academic assistant specializing in the cs.CL domain. Strictly based on the provided literature information, answer the user's query. You may slightly expand and refine the response using existing knowledge, but avoid fabricating information. The response language should match the user's query language (English for English queries, Chinese for Chinese queries). Ensure the response is concise and limited to 500 words. Include a citation with the corresponding literature link (like [text](f\"https://arxiv.org/abs/{{id}}\")) and adjust the response style based on the temperature parameter (from 0.0 to 1.0). If the temperature is low (e.g., 0.2), provide a strict and precise answer. If the temperature is high (e.g., 0.8), provide a more creative and exploratory answer.\n",
    "\n",
    "Temperature: 0.2\n",
    "Question: 句法语义一体化建模中，成分结构与依存结构能否统一表示？\n",
    "Relevant Literature: \n",
    "ID: cmp-lg/9604021, Distance: 0.31888869404792786, Content: {'title': 'Extended Dependency Structures and their Formal Interpretation', 'doi': None, 'categories': 'cmp-lg cs.CL', 'abstract': \"  We describe two ``semantically-oriented'' dependency-structure formalisms,\\nU-forms and S-forms. U-forms have been previously used in machine translation\\nas interlingual representations, but without being provided with a formal\\ninterpretation. S-forms, which we introduce in this paper, are a scoped version\\nof U-forms, and we define a compositional semantics mechanism for them. Two\\ntypes of semantic composition are basic: complement incorporation and modifier\\nincorporation. Binding of variables is done at the time of incorporation,\\npermitting much flexibility in composition order and a simple account of the\\nsemantic effects of permuting several incorporations.\\n\", 'publish_date': 'Mon, 29 Apr 1996 16:00:01 GMT'}\n",
    "ID: cmp-lg/9405004, Distance: 0.34163472056388855, Content: {'title': 'Syntactic-Head-Driven Generation', 'doi': None, 'categories': 'cmp-lg cs.CL', 'abstract': \"  The previously proposed semantic-head-driven generation methods run into\\nproblems if none of the daughter constituents in the syntacto-semantic rule\\nschemata of a grammar fits the definition of a semantic head given in Shieber\\net al. 1990. This is the case for the semantic analysis rules of certain\\nconstraint-based semantic representations, e.g. Underspecified Discourse\\nRepresentation Structures (UDRSs) (Frank/Reyle 1992). Since head-driven\\ngeneration in general has its merits, we simply return to a syntactic\\ndefinition of `head' and demonstrate the feasibility of syntactic-head-driven\\ngeneration. In addition to its generality, a syntactic-head-driven algorithm\\nprovides a basis for a logically well-defined treatment of the movement of\\n(syntactic) heads, for which only ad-hoc solutions existed, so far.\\n\", 'publish_date': 'Tue, 3 May 1994 13:14:43 GMT'}\n",
    "ID: 1011.4155, Distance: 0.3422280251979828, Content: {'title': \"Motifs de graphe pour le calcul de d\\\\'ependances syntaxiques compl\\\\`etes\", 'doi': None, 'categories': 'cs.CL', 'abstract': '  This article describes a method to build syntactical dependencies starting\\nfrom the phrase structure parsing process. The goal is to obtain all the\\ninformation needed for a detailled semantical analysis. Interaction Grammars\\nare used for parsing; the saturation of polarities which is the core of this\\nformalism can be mapped to dependency relation. Formally, graph patterns are\\nused to express the set of constraints which control dependency creations.\\n', 'publish_date': 'Thu, 18 Nov 2010 08:59:55 GMT'}\n",
    "ID: cmp-lg/9808012, Distance: 0.36063557863235474, Content: {'title': 'Separating Surface Order and Syntactic Relations in a Dependency Grammar', 'doi': None, 'categories': 'cmp-lg cs.CL', 'abstract': '  This paper proposes decoupling the dependency tree from word order, such that\\nsurface ordering is not determined by traversing the dependency tree. We\\ndevelop the notion of a \\\\emph{word order domain structure}, which is linked but\\nstructurally dissimilar to the syntactic dependency tree. The proposal results\\nin a lexicalized, declarative, and formally precise description of word order;\\nfeatures which lack previous proposals for dependency grammars. Contrary to\\nother lexicalized approaches to word order, our proposal does not require\\nlexical ambiguities for ordering alternatives.\\n', 'publish_date': 'Tue, 25 Aug 1998 08:06:45 GMT'}\n",
    "ID: cmp-lg/9808009, Distance: 0.3881697356700897, Content: {'title': 'How to define a context-free backbone for DGs: Implementing a DG in the\\n  LFG formalism', 'doi': None, 'categories': 'cmp-lg cs.CL', 'abstract': \"  This paper presents a multidimensional Dependency Grammar (DG), which\\ndecouples the dependency tree from word order, such that surface ordering is\\nnot determined by traversing the dependency tree. We develop the notion of a\\n\\\\emph{word order domain structure}, which is linked but structurally dissimilar\\nto the syntactic dependency tree. We then discuss the implementation of such a\\nDG using constructs from a unification-based phrase-structure approach, namely\\nLexical-Functional Grammar (LFG). Particular attention is given to the analysis\\nof discontinuities in DG in terms of LFG's functional uncertainty.\\n\", 'publish_date': 'Fri, 21 Aug 1998 14:04:30 GMT'}\n",
    "\n",
    "Answer:\n",
    "\n",
    "==================================================\n",
    "生成嵌入中: 100%|██████████| 1/1 [00:00<00:00, 11.94it/s]\n",
    "[INFO] Query vector shape: (1, 1, 768)\n",
    "\n",
    "\n",
    "cost 0.1456 seconds\n",
    "\n",
    "\n",
    "You are a professional academic assistant specializing in the cs.CL domain. Strictly based on the provided literature information, answer the user's query. You may slightly expand and refine the response using existing knowledge, but avoid fabricating information. The response language should match the user's query language (English for English queries, Chinese for Chinese queries). Ensure the response is concise and limited to 500 words. Include a citation with the corresponding literature link (like [text](f\"https://arxiv.org/abs/{{id}}\")) and adjust the response style based on the temperature parameter (from 0.0 to 1.0). If the temperature is low (e.g., 0.2), provide a strict and precise answer. If the temperature is high (e.g., 0.8), provide a more creative and exploratory answer.\n",
    "\n",
    "Temperature: 0.2\n",
    "Question: Transformer模型的自注意力机制是否存在 “信息泄露” 风险？如何设计防护措施？\n",
    "Relevant Literature: \n",
    "ID: 2310.12462, Distance: 0.3729002773761749, Content: {'title': 'Unmasking Transformers: A Theoretical Approach to Data Recovery via\\n  Attention Weights', 'doi': None, 'categories': 'cs.LG cs.CL stat.ML', 'abstract': \"  In the realm of deep learning, transformers have emerged as a dominant\\narchitecture, particularly in natural language processing tasks. However, with\\ntheir widespread adoption, concerns regarding the security and privacy of the\\ndata processed by these models have arisen. In this paper, we address a pivotal\\nquestion: Can the data fed into transformers be recovered using their attention\\nweights and outputs? We introduce a theoretical framework to tackle this\\nproblem. Specifically, we present an algorithm that aims to recover the input\\ndata $X \\\\in \\\\mathbb{R}^{d \\\\times n}$ from given attention weights $W = QK^\\\\top\\n\\\\in \\\\mathbb{R}^{d \\\\times d}$ and output $B \\\\in \\\\mathbb{R}^{n \\\\times n}$ by\\nminimizing the loss function $L(X)$. This loss function captures the\\ndiscrepancy between the expected output and the actual output of the\\ntransformer. Our findings have significant implications for the Localized\\nLayer-wise Mechanism (LLM), suggesting potential vulnerabilities in the model's\\ndesign from a security and privacy perspective. This work underscores the\\nimportance of understanding and safeguarding the internal workings of\\ntransformers to ensure the confidentiality of processed data.\\n\", 'publish_date': 'Thu, 19 Oct 2023 04:41:01 GMT'}\n",
    "ID: 2302.07730, Distance: 0.4099262058734894, Content: {'title': 'Transformer models: an introduction and catalog', 'doi': None, 'categories': 'cs.CL', 'abstract': '  In the past few years we have seen the meteoric appearance of dozens of\\nfoundation models of the Transformer family, all of which have memorable and\\nsometimes funny, but not self-explanatory, names. The goal of this paper is to\\noffer a somewhat comprehensive but simple catalog and classification of the\\nmost popular Transformer models. The paper also includes an introduction to the\\nmost important aspects and innovations in Transformer models. Our catalog will\\ninclude models that are trained using self-supervised learning (e.g., BERT or\\nGPT3) as well as those that are further trained using a human-in-the-loop (e.g.\\nthe InstructGPT model used by ChatGPT).\\n', 'publish_date': 'Sun, 12 Feb 2023 01:26:49 GMT'}\n",
    "ID: 2009.06732, Distance: 0.41492292284965515, Content: {'title': 'Efficient Transformers: A Survey', 'doi': None, 'categories': 'cs.LG cs.AI cs.CL cs.CV cs.IR', 'abstract': '  Transformer model architectures have garnered immense interest lately due to\\ntheir effectiveness across a range of domains like language, vision and\\nreinforcement learning. In the field of natural language processing for\\nexample, Transformers have become an indispensable staple in the modern deep\\nlearning stack. Recently, a dizzying number of \"X-former\" models have been\\nproposed - Reformer, Linformer, Performer, Longformer, to name a few - which\\nimprove upon the original Transformer architecture, many of which make\\nimprovements around computational and memory efficiency. With the aim of\\nhelping the avid researcher navigate this flurry, this paper characterizes a\\nlarge and thoughtful selection of recent efficiency-flavored \"X-former\" models,\\nproviding an organized and comprehensive overview of existing work and models\\nacross multiple domains.\\n', 'publish_date': 'Mon, 14 Sep 2020 20:38:14 GMT'}\n",
    "ID: 2005.00743, Distance: 0.41934147477149963, Content: {'title': 'Synthesizer: Rethinking Self-Attention in Transformer Models', 'doi': None, 'categories': 'cs.CL cs.IR cs.LG', 'abstract': '  The dot product self-attention is known to be central and indispensable to\\nstate-of-the-art Transformer models. But is it really required? This paper\\ninvestigates the true importance and contribution of the dot product-based\\nself-attention mechanism on the performance of Transformer models. Via\\nextensive experiments, we find that (1) random alignment matrices surprisingly\\nperform quite competitively and (2) learning attention weights from token-token\\n(query-key) interactions is useful but not that important after all. To this\\nend, we propose \\\\textsc{Synthesizer}, a model that learns synthetic attention\\nweights without token-token interactions. In our experiments, we first show\\nthat simple Synthesizers achieve highly competitive performance when compared\\nagainst vanilla Transformer models across a range of tasks, including machine\\ntranslation, language modeling, text generation and GLUE/SuperGLUE benchmarks.\\nWhen composed with dot product attention, we find that Synthesizers\\nconsistently outperform Transformers. Moreover, we conduct additional\\ncomparisons of Synthesizers against Dynamic Convolutions, showing that simple\\nRandom Synthesizer is not only $60\\\\%$ faster but also improves perplexity by a\\nrelative $3.5\\\\%$. Finally, we show that simple factorized Synthesizers can\\noutperform Linformers on encoding only tasks.\\n', 'publish_date': 'Sat, 2 May 2020 08:16:19 GMT'}\n",
    "ID: 2210.05794, Distance: 0.4230707585811615, Content: {'title': 'Designing Robust Transformers using Robust Kernel Density Estimation', 'doi': None, 'categories': 'cs.LG cs.CL cs.CV', 'abstract': '  Recent advances in Transformer architectures have empowered their empirical\\nsuccess in a variety of tasks across different domains. However, existing works\\nmainly focus on predictive accuracy and computational cost, without considering\\nother practical issues, such as robustness to contaminated samples. Recent work\\nby Nguyen et al., (2022) has shown that the self-attention mechanism, which is\\nthe center of the Transformer architecture, can be viewed as a non-parametric\\nestimator based on kernel density estimation (KDE). This motivates us to\\nleverage a set of robust kernel density estimation methods for alleviating the\\nissue of data contamination. Specifically, we introduce a series of\\nself-attention mechanisms that can be incorporated into different Transformer\\narchitectures and discuss the special properties of each method. We then\\nperform extensive empirical studies on language modeling and image\\nclassification tasks. Our methods demonstrate robust performance in multiple\\nscenarios while maintaining competitive results on clean datasets.\\n', 'publish_date': 'Tue, 11 Oct 2022 21:39:52 GMT'}\n",
    "\n",
    "Answer:\n",
    "\n",
    "==================================================\n",
    "生成嵌入中: 100%|██████████| 1/1 [00:00<00:00, 11.00it/s]\n",
    "[INFO] Query vector shape: (1, 1, 768)\n",
    "\n",
    "\n",
    "cost 0.1504 seconds\n",
    "\n",
    "\n",
    "You are a professional academic assistant specializing in the cs.CL domain. Strictly based on the provided literature information, answer the user's query. You may slightly expand and refine the response using existing knowledge, but avoid fabricating information. The response language should match the user's query language (English for English queries, Chinese for Chinese queries). Ensure the response is concise and limited to 500 words. Include a citation with the corresponding literature link (like [text](f\"https://arxiv.org/abs/{{id}}\")) and adjust the response style based on the temperature parameter (from 0.0 to 1.0). If the temperature is low (e.g., 0.2), provide a strict and precise answer. If the temperature is high (e.g., 0.8), provide a more creative and exploratory answer.\n",
    "\n",
    "Temperature: 0.2\n",
    "Question: Transformer上的文本分类和基于CNN的图片分类遇到的问题及解决方案的异同？\n",
    "Relevant Literature: \n",
    "ID: 1804.00968, Distance: 0.33558589220046997, Content: {'title': 'In-depth Question classification using Convolutional Neural Networks', 'doi': None, 'categories': 'cs.CL', 'abstract': '  Convolutional neural networks for computer vision are fairly intuitive. In a\\ntypical CNN used in image classification, the first layers learn edges, and the\\nfollowing layers learn some filters that can identify an object. But CNNs for\\nNatural Language Processing are not used often and are not completely\\nintuitive. We have a good idea about what the convolution filters learn for the\\ntask of text classification, and to that, we propose a neural network structure\\nthat will be able to give good results in less time. We will be using\\nconvolutional neural networks to predict the primary or broader topic of a\\nquestion, and then use separate networks for each of these predicted topics to\\naccurately classify their sub-topics.\\n', 'publish_date': 'Sat, 31 Mar 2018 19:52:26 GMT'}\n",
    "ID: 1911.04115, Distance: 0.37048929929733276, Content: {'title': 'Text classification with pixel embedding', 'doi': None, 'categories': 'cs.CL', 'abstract': \"  We propose a novel framework to understand the text by converting sentences\\nor articles into video-like 3-dimensional tensors. Each frame, corresponding to\\na slice of the tensor, is a word image that is rendered by the word's shape.\\nThe length of the tensor equals to the number of words in the sentence or\\narticle. The proposed transformation from the text to a 3-dimensional tensor\\nmakes it very convenient to implement an $n$-gram model with convolutional\\nneural networks for text analysis. Concretely, we impose a 3-dimensional\\nconvolutional kernel on the 3-dimensional text tensor. The first two dimensions\\nof the convolutional kernel size equal the size of the word image and the last\\ndimension of the kernel size is $n$. That is, every time when we slide the\\n3-dimensional kernel over a word sequence, the convolution covers $n$ word\\nimages and outputs a scalar. By iterating this process continuously for each\\n$n$-gram along with the sentence or article with multiple kernels, we obtain a\\n2-dimensional feature map. A subsequent 1-dimensional max-over-time pooling is\\napplied to this feature map, and three fully-connected layers are used for\\nconducting text classification finally. Experiments of several text\\nclassification datasets demonstrate surprisingly superior performances using\\nthe proposed model in comparison with existing methods.\\n\", 'publish_date': 'Mon, 11 Nov 2019 07:28:25 GMT'}\n",
    "ID: 1606.01781, Distance: 0.3714621067047119, Content: {'title': 'Very Deep Convolutional Networks for Text Classification', 'doi': None, 'categories': 'cs.CL cs.LG cs.NE', 'abstract': '  The dominant approach for many NLP tasks are recurrent neural networks, in\\nparticular LSTMs, and convolutional neural networks. However, these\\narchitectures are rather shallow in comparison to the deep convolutional\\nnetworks which have pushed the state-of-the-art in computer vision. We present\\na new architecture (VDCNN) for text processing which operates directly at the\\ncharacter level and uses only small convolutions and pooling operations. We are\\nable to show that the performance of this model increases with depth: using up\\nto 29 convolutional layers, we report improvements over the state-of-the-art on\\nseveral public text classification tasks. To the best of our knowledge, this is\\nthe first time that very deep convolutional nets have been applied to text\\nprocessing.\\n', 'publish_date': 'Mon, 6 Jun 2016 15:14:50 GMT'}\n",
    "ID: 1809.08037, Distance: 0.37699344754219055, Content: {'title': 'Understanding Convolutional Neural Networks for Text Classification', 'doi': None, 'categories': 'cs.CL', 'abstract': '  We present an analysis into the inner workings of Convolutional Neural\\nNetworks (CNNs) for processing text. CNNs used for computer vision can be\\ninterpreted by projecting filters into image space, but for discrete sequence\\ninputs CNNs remain a mystery. We aim to understand the method by which the\\nnetworks process and classify text. We examine common hypotheses to this\\nproblem: that filters, accompanied by global max-pooling, serve as ngram\\ndetectors. We show that filters may capture several different semantic classes\\nof ngrams by using different activation patterns, and that global max-pooling\\ninduces behavior which separates important ngrams from the rest. Finally, we\\nshow practical use cases derived from our findings in the form of model\\ninterpretability (explaining a trained model by deriving a concrete identity\\nfor each filter, bridging the gap between visualization tools in vision tasks\\nand NLP) and prediction interpretability (explaining predictions). Code\\nimplementation is available online at\\ngithub.com/sayaendo/interpreting-cnn-for-text.\\n', 'publish_date': 'Fri, 21 Sep 2018 11:03:48 GMT'}\n",
    "ID: 1509.01626, Distance: 0.3779098391532898, Content: {'title': 'Character-level Convolutional Networks for Text Classification', 'doi': None, 'categories': 'cs.LG cs.CL', 'abstract': '  This article offers an empirical exploration on the use of character-level\\nconvolutional networks (ConvNets) for text classification. We constructed\\nseveral large-scale datasets to show that character-level convolutional\\nnetworks could achieve state-of-the-art or competitive results. Comparisons are\\noffered against traditional models such as bag of words, n-grams and their\\nTFIDF variants, and deep learning models such as word-based ConvNets and\\nrecurrent neural networks.\\n', 'publish_date': 'Fri, 4 Sep 2015 22:31:53 GMT'}\n",
    "\n",
    "Answer:\n",
    "\n",
    "==================================================\n",
    "生成嵌入中: 100%|██████████| 1/1 [00:00<00:00, 10.04it/s]\n",
    "[INFO] Query vector shape: (1, 1, 768)\n",
    "\n",
    "\n",
    "cost 0.1714 seconds\n",
    "\n",
    "\n",
    "You are a professional academic assistant specializing in the cs.CL domain. Strictly based on the provided literature information, answer the user's query. You may slightly expand and refine the response using existing knowledge, but avoid fabricating information. The response language should match the user's query language (English for English queries, Chinese for Chinese queries). Ensure the response is concise and limited to 500 words. Include a citation with the corresponding literature link (like [text](f\"https://arxiv.org/abs/{{id}}\")) and adjust the response style based on the temperature parameter (from 0.0 to 1.0). If the temperature is low (e.g., 0.2), provide a strict and precise answer. If the temperature is high (e.g., 0.8), provide a more creative and exploratory answer.\n",
    "\n",
    "Temperature: 0.2\n",
    "Question: 什么是对抗攻击，常用黑盒文本对抗攻击手段有哪些？\n",
    "Relevant Literature: \n",
    "ID: 2201.08555, Distance: 0.2739735245704651, Content: {'title': 'Identifying Adversarial Attacks on Text Classifiers', 'doi': None, 'categories': 'cs.CL cs.CR cs.LG', 'abstract': '  The landscape of adversarial attacks against text classifiers continues to\\ngrow, with new attacks developed every year and many of them available in\\nstandard toolkits, such as TextAttack and OpenAttack. In response, there is a\\ngrowing body of work on robust learning, which reduces vulnerability to these\\nattacks, though sometimes at a high cost in compute time or accuracy. In this\\npaper, we take an alternate approach -- we attempt to understand the attacker\\nby analyzing adversarial text to determine which methods were used to create\\nit. Our first contribution is an extensive dataset for attack detection and\\nlabeling: 1.5~million attack instances, generated by twelve adversarial attacks\\ntargeting three classifiers trained on six source datasets for sentiment\\nanalysis and abuse detection in English. As our second contribution, we use\\nthis dataset to develop and benchmark a number of classifiers for attack\\nidentification -- determining if a given text has been adversarially\\nmanipulated and by which attack. As a third contribution, we demonstrate the\\neffectiveness of three classes of features for these tasks: text properties,\\ncapturing content and presentation of text; language model properties,\\ndetermining which tokens are more or less probable throughout the input; and\\ntarget model properties, representing how the text classifier is influenced by\\nthe attack, including internal node activations. Overall, this represents a\\nfirst step towards forensics for adversarial attacks against text classifiers.\\n', 'publish_date': 'Fri, 21 Jan 2022 06:16:04 GMT'}\n",
    "ID: 2104.13484, Distance: 0.28071328997612, Content: {'title': 'Improved and Efficient Text Adversarial Attacks using Target Information', 'doi': None, 'categories': 'cs.LG cs.AI cs.CL cs.CR', 'abstract': '  There has been recently a growing interest in studying adversarial examples\\non natural language models in the black-box setting. These methods attack\\nnatural language classifiers by perturbing certain important words until the\\nclassifier label is changed. In order to find these important words, these\\nmethods rank all words by importance by querying the target model word by word\\nfor each input sentence, resulting in high query inefficiency. A new\\ninteresting approach was introduced that addresses this problem through\\ninterpretable learning to learn the word ranking instead of previous expensive\\nsearch. The main advantage of using this approach is that it achieves\\ncomparable attack rates to the state-of-the-art methods, yet faster and with\\nfewer queries, where fewer queries are desirable to avoid suspicion towards the\\nattacking agent. Nonetheless, this approach sacrificed the useful information\\nthat could be leveraged from the target classifier for that sake of query\\nefficiency. In this paper we study the effect of leveraging the target model\\noutputs and data on both attack rates and average number of queries, and we\\nshow that both can be improved, with a limited overhead of additional queries.\\n', 'publish_date': 'Tue, 27 Apr 2021 21:25:55 GMT'}\n",
    "ID: 2206.05015, Distance: 0.2830166816711426, Content: {'title': 'A Simple Yet Efficient Method for Adversarial Word-Substitute Attack', 'doi': None, 'categories': 'cs.CL cs.AI', 'abstract': '  NLP researchers propose different word-substitute black-box attacks that can\\nfool text classification models. In such attack, an adversary keeps sending\\ncrafted adversarial queries to the target model until it can successfully\\nachieve the intended outcome. State-of-the-art attack methods usually require\\nhundreds or thousands of queries to find one adversarial example. In this\\npaper, we study whether a sophisticated adversary can attack the system with\\nmuch less queries. We propose a simple yet efficient method that can reduce the\\naverage number of adversarial queries by 3-30 times and maintain the attack\\neffectiveness. This research highlights that an adversary can fool a deep NLP\\nmodel with much less cost.\\n', 'publish_date': 'Sat, 7 May 2022 14:20:57 GMT'}\n",
    "ID: 1909.07873, Distance: 0.28530609607696533, Content: {'title': 'Generating Black-Box Adversarial Examples for Text Classifiers Using a\\n  Deep Reinforced Model', 'doi': '10.1007/978-3-030-46147-8_43', 'categories': 'cs.LG cs.CL cs.IR stat.ML', 'abstract': \"  Recently, generating adversarial examples has become an important means of\\nmeasuring robustness of a deep learning model. Adversarial examples help us\\nidentify the susceptibilities of the model and further counter those\\nvulnerabilities by applying adversarial training techniques. In natural\\nlanguage domain, small perturbations in the form of misspellings or paraphrases\\ncan drastically change the semantics of the text. We propose a reinforcement\\nlearning based approach towards generating adversarial examples in black-box\\nsettings. We demonstrate that our method is able to fool well-trained models\\nfor (a) IMDB sentiment classification task and (b) AG's news corpus news\\ncategorization task with significantly high success rates. We find that the\\nadversarial examples generated are semantics-preserving perturbations to the\\noriginal text.\\n\", 'publish_date': 'Tue, 17 Sep 2019 15:05:31 GMT'}\n",
    "ID: 2405.03789, Distance: 0.2896369695663452, Content: {'title': 'On Adversarial Examples for Text Classification by Perturbing Latent\\n  Representations', 'doi': None, 'categories': 'cs.LG cs.AI cs.CL cs.CR', 'abstract': '  Recently, with the advancement of deep learning, several applications in text\\nclassification have advanced significantly. However, this improvement comes\\nwith a cost because deep learning is vulnerable to adversarial examples. This\\nweakness indicates that deep learning is not very robust. Fortunately, the\\ninput of a text classifier is discrete. Hence, it can prevent the classifier\\nfrom state-of-the-art attacks. Nonetheless, previous works have generated\\nblack-box attacks that successfully manipulate the discrete values of the input\\nto find adversarial examples. Therefore, instead of changing the discrete\\nvalues, we transform the input into its embedding vector containing real values\\nto perform the state-of-the-art white-box attacks. Then, we convert the\\nperturbed embedding vector back into a text and name it an adversarial example.\\nIn summary, we create a framework that measures the robustness of a text\\nclassifier by using the gradients of the classifier.\\n', 'publish_date': 'Mon, 6 May 2024 18:45:18 GMT'}\n",
    "\n",
    "Answer:\n",
    "\n",
    "==================================================\n",
    "生成嵌入中: 100%|██████████| 1/1 [00:00<00:00, 12.37it/s]\n",
    "[INFO] Query vector shape: (1, 1, 768)\n",
    "\n",
    "\n",
    "cost 0.1713 seconds\n",
    "\n",
    "\n",
    "You are a professional academic assistant specializing in the cs.CL domain. Strictly based on the provided literature information, answer the user's query. You may slightly expand and refine the response using existing knowledge, but avoid fabricating information. The response language should match the user's query language (English for English queries, Chinese for Chinese queries). Ensure the response is concise and limited to 500 words. Include a citation with the corresponding literature link (like [text](f\"https://arxiv.org/abs/{{id}}\")) and adjust the response style based on the temperature parameter (from 0.0 to 1.0). If the temperature is low (e.g., 0.2), provide a strict and precise answer. If the temperature is high (e.g., 0.8), provide a more creative and exploratory answer.\n",
    "\n",
    "Temperature: 0.2\n",
    "Question: 什么是对抗攻击，常用白盒文本对抗攻击手段有哪些？\n",
    "Relevant Literature: \n",
    "ID: 2008.05536, Distance: 0.3000302314758301, Content: {'title': 'Model Robustness with Text Classification: Semantic-preserving\\n  adversarial attacks', 'doi': None, 'categories': 'cs.CL cs.LG', 'abstract': '  We propose algorithms to create adversarial attacks to assess model\\nrobustness in text classification problems. They can be used to create white\\nbox attacks and black box attacks while at the same time preserving the\\nsemantics and syntax of the original text. The attacks cause significant number\\nof flips in white-box setting and same rule based can be used in black-box\\nsetting. In a black-box setting, the attacks created are able to reverse\\ndecisions of transformer based architectures.\\n', 'publish_date': 'Wed, 12 Aug 2020 19:17:46 GMT'}\n",
    "ID: 2201.08555, Distance: 0.30486223101615906, Content: {'title': 'Identifying Adversarial Attacks on Text Classifiers', 'doi': None, 'categories': 'cs.CL cs.CR cs.LG', 'abstract': '  The landscape of adversarial attacks against text classifiers continues to\\ngrow, with new attacks developed every year and many of them available in\\nstandard toolkits, such as TextAttack and OpenAttack. In response, there is a\\ngrowing body of work on robust learning, which reduces vulnerability to these\\nattacks, though sometimes at a high cost in compute time or accuracy. In this\\npaper, we take an alternate approach -- we attempt to understand the attacker\\nby analyzing adversarial text to determine which methods were used to create\\nit. Our first contribution is an extensive dataset for attack detection and\\nlabeling: 1.5~million attack instances, generated by twelve adversarial attacks\\ntargeting three classifiers trained on six source datasets for sentiment\\nanalysis and abuse detection in English. As our second contribution, we use\\nthis dataset to develop and benchmark a number of classifiers for attack\\nidentification -- determining if a given text has been adversarially\\nmanipulated and by which attack. As a third contribution, we demonstrate the\\neffectiveness of three classes of features for these tasks: text properties,\\ncapturing content and presentation of text; language model properties,\\ndetermining which tokens are more or less probable throughout the input; and\\ntarget model properties, representing how the text classifier is influenced by\\nthe attack, including internal node activations. Overall, this represents a\\nfirst step towards forensics for adversarial attacks against text classifiers.\\n', 'publish_date': 'Fri, 21 Jan 2022 06:16:04 GMT'}\n",
    "ID: 2405.03789, Distance: 0.30765044689178467, Content: {'title': 'On Adversarial Examples for Text Classification by Perturbing Latent\\n  Representations', 'doi': None, 'categories': 'cs.LG cs.AI cs.CL cs.CR', 'abstract': '  Recently, with the advancement of deep learning, several applications in text\\nclassification have advanced significantly. However, this improvement comes\\nwith a cost because deep learning is vulnerable to adversarial examples. This\\nweakness indicates that deep learning is not very robust. Fortunately, the\\ninput of a text classifier is discrete. Hence, it can prevent the classifier\\nfrom state-of-the-art attacks. Nonetheless, previous works have generated\\nblack-box attacks that successfully manipulate the discrete values of the input\\nto find adversarial examples. Therefore, instead of changing the discrete\\nvalues, we transform the input into its embedding vector containing real values\\nto perform the state-of-the-art white-box attacks. Then, we convert the\\nperturbed embedding vector back into a text and name it an adversarial example.\\nIn summary, we create a framework that measures the robustness of a text\\nclassifier by using the gradients of the classifier.\\n', 'publish_date': 'Mon, 6 May 2024 18:45:18 GMT'}\n",
    "ID: 2104.13484, Distance: 0.31862643361091614, Content: {'title': 'Improved and Efficient Text Adversarial Attacks using Target Information', 'doi': None, 'categories': 'cs.LG cs.AI cs.CL cs.CR', 'abstract': '  There has been recently a growing interest in studying adversarial examples\\non natural language models in the black-box setting. These methods attack\\nnatural language classifiers by perturbing certain important words until the\\nclassifier label is changed. In order to find these important words, these\\nmethods rank all words by importance by querying the target model word by word\\nfor each input sentence, resulting in high query inefficiency. A new\\ninteresting approach was introduced that addresses this problem through\\ninterpretable learning to learn the word ranking instead of previous expensive\\nsearch. The main advantage of using this approach is that it achieves\\ncomparable attack rates to the state-of-the-art methods, yet faster and with\\nfewer queries, where fewer queries are desirable to avoid suspicion towards the\\nattacking agent. Nonetheless, this approach sacrificed the useful information\\nthat could be leveraged from the target classifier for that sake of query\\nefficiency. In this paper we study the effect of leveraging the target model\\noutputs and data on both attack rates and average number of queries, and we\\nshow that both can be improved, with a limited overhead of additional queries.\\n', 'publish_date': 'Tue, 27 Apr 2021 21:25:55 GMT'}\n",
    "ID: 2211.06571, Distance: 0.3199475109577179, Content: {'title': 'Generating Textual Adversaries with Minimal Perturbation', 'doi': None, 'categories': 'cs.CL', 'abstract': '  Many word-level adversarial attack approaches for textual data have been\\nproposed in recent studies. However, due to the massive search space consisting\\nof combinations of candidate words, the existing approaches face the problem of\\npreserving the semantics of texts when crafting adversarial counterparts. In\\nthis paper, we develop a novel attack strategy to find adversarial texts with\\nhigh similarity to the original texts while introducing minimal perturbation.\\nThe rationale is that we expect the adversarial texts with small perturbation\\ncan better preserve the semantic meaning of original texts. Experiments show\\nthat, compared with state-of-the-art attack approaches, our approach achieves\\nhigher success rates and lower perturbation rates in four benchmark datasets.\\n', 'publish_date': 'Sat, 12 Nov 2022 04:46:07 GMT'}\n",
    "\n",
    "Answer:\n",
    "\n",
    "==================================================\n",
    "生成嵌入中: 100%|██████████| 1/1 [00:00<00:00, 16.36it/s]\n",
    "[INFO] Query vector shape: (1, 1, 768)\n",
    "\n",
    "\n",
    "cost 0.1269 seconds\n",
    "\n",
    "\n",
    "You are a professional academic assistant specializing in the cs.CL domain. Strictly based on the provided literature information, answer the user's query. You may slightly expand and refine the response using existing knowledge, but avoid fabricating information. The response language should match the user's query language (English for English queries, Chinese for Chinese queries). Ensure the response is concise and limited to 500 words. Include a citation with the corresponding literature link (like [text](f\"https://arxiv.org/abs/{{id}}\")) and adjust the response style based on the temperature parameter (from 0.0 to 1.0). If the temperature is low (e.g., 0.2), provide a strict and precise answer. If the temperature is high (e.g., 0.8), provide a more creative and exploratory answer.\n",
    "\n",
    "Temperature: 0.2\n",
    "Question: FGSM对抗攻击方法是否能以及如何迁移到文本对抗攻击领域？\n",
    "Relevant Literature: \n",
    "ID: 2008.03709, Distance: 0.343606173992157, Content: {'title': 'Adversarial Training with Fast Gradient Projection Method against\\n  Synonym Substitution based Text Attacks', 'doi': None, 'categories': 'cs.CL cs.LG', 'abstract': '  Adversarial training is the most empirically successful approach in improving\\nthe robustness of deep neural networks for image classification.For text\\nclassification, however, existing synonym substitution based adversarial\\nattacks are effective but not efficient to be incorporated into practical text\\nadversarial training. Gradient-based attacks, which are very efficient for\\nimages, are hard to be implemented for synonym substitution based text attacks\\ndue to the lexical, grammatical and semantic constraints and the discrete text\\ninput space. Thereby, we propose a fast text adversarial attack method called\\nFast Gradient Projection Method (FGPM) based on synonym substitution, which is\\nabout 20 times faster than existing text attack methods and could achieve\\nsimilar attack performance. We then incorporate FGPM with adversarial training\\nand propose a text defense method called Adversarial Training with FGPM\\nenhanced by Logit pairing (ATFL). Experiments show that ATFL could\\nsignificantly improve the model robustness and block the transferability of\\nadversarial examples.\\n', 'publish_date': 'Sun, 9 Aug 2020 11:02:06 GMT'}\n",
    "ID: 2201.08555, Distance: 0.3629932403564453, Content: {'title': 'Identifying Adversarial Attacks on Text Classifiers', 'doi': None, 'categories': 'cs.CL cs.CR cs.LG', 'abstract': '  The landscape of adversarial attacks against text classifiers continues to\\ngrow, with new attacks developed every year and many of them available in\\nstandard toolkits, such as TextAttack and OpenAttack. In response, there is a\\ngrowing body of work on robust learning, which reduces vulnerability to these\\nattacks, though sometimes at a high cost in compute time or accuracy. In this\\npaper, we take an alternate approach -- we attempt to understand the attacker\\nby analyzing adversarial text to determine which methods were used to create\\nit. Our first contribution is an extensive dataset for attack detection and\\nlabeling: 1.5~million attack instances, generated by twelve adversarial attacks\\ntargeting three classifiers trained on six source datasets for sentiment\\nanalysis and abuse detection in English. As our second contribution, we use\\nthis dataset to develop and benchmark a number of classifiers for attack\\nidentification -- determining if a given text has been adversarially\\nmanipulated and by which attack. As a third contribution, we demonstrate the\\neffectiveness of three classes of features for these tasks: text properties,\\ncapturing content and presentation of text; language model properties,\\ndetermining which tokens are more or less probable throughout the input; and\\ntarget model properties, representing how the text classifier is influenced by\\nthe attack, including internal node activations. Overall, this represents a\\nfirst step towards forensics for adversarial attacks against text classifiers.\\n', 'publish_date': 'Fri, 21 Jan 2022 06:16:04 GMT'}\n",
    "ID: 2011.08558, Distance: 0.3702811300754547, Content: {'title': 'On the Transferability of Adversarial Attacksagainst Neural Text\\n  Classifier', 'doi': None, 'categories': 'cs.LG cs.CL', 'abstract': '  Deep neural networks are vulnerable to adversarial attacks, where a small\\nperturbation to an input alters the model prediction. In many cases, malicious\\ninputs intentionally crafted for one model can fool another model. In this\\npaper, we present the first study to systematically investigate the\\ntransferability of adversarial examples for text classification models and\\nexplore how various factors, including network architecture, tokenization\\nscheme, word embedding, and model capacity, affect the transferability of\\nadversarial examples. Based on these studies, we propose a genetic algorithm to\\nfind an ensemble of models that can be used to induce adversarial examples to\\nfool almost all existing models. Such adversarial examples reflect the defects\\nof the learning process and the data bias in the training set. Finally, we\\nderive word replacement rules that can be used for model diagnostics from these\\nadversarial examples.\\n', 'publish_date': 'Tue, 17 Nov 2020 10:45:05 GMT'}\n",
    "ID: 1801.07175, Distance: 0.37107425928115845, Content: {'title': 'Adversarial Texts with Gradient Methods', 'doi': None, 'categories': 'cs.CL cs.CR cs.LG', 'abstract': \"  Adversarial samples for images have been extensively studied in the\\nliterature. Among many of the attacking methods, gradient-based methods are\\nboth effective and easy to compute. In this work, we propose a framework to\\nadapt the gradient attacking methods on images to text domain. The main\\ndifficulties for generating adversarial texts with gradient methods are i) the\\ninput space is discrete, which makes it difficult to accumulate small noise\\ndirectly in the inputs, and ii) the measurement of the quality of the\\nadversarial texts is difficult. We tackle the first problem by searching for\\nadversarials in the embedding space and then reconstruct the adversarial texts\\nvia nearest neighbor search. For the latter problem, we employ the Word Mover's\\nDistance (WMD) to quantify the quality of adversarial texts. Through extensive\\nexperiments on three datasets, IMDB movie reviews, Reuters-2 and Reuters-5\\nnewswires, we show that our framework can leverage gradient attacking methods\\nto generate very high-quality adversarial texts that are only a few words\\ndifferent from the original texts. There are many cases where we can change one\\nword to alter the label of the whole piece of text. We successfully incorporate\\nFGM and DeepFool into our framework. In addition, we empirically show that WMD\\nis closely related to the quality of adversarial texts.\\n\", 'publish_date': 'Mon, 22 Jan 2018 16:19:52 GMT'}\n",
    "ID: 2211.06571, Distance: 0.3799896836280823, Content: {'title': 'Generating Textual Adversaries with Minimal Perturbation', 'doi': None, 'categories': 'cs.CL', 'abstract': '  Many word-level adversarial attack approaches for textual data have been\\nproposed in recent studies. However, due to the massive search space consisting\\nof combinations of candidate words, the existing approaches face the problem of\\npreserving the semantics of texts when crafting adversarial counterparts. In\\nthis paper, we develop a novel attack strategy to find adversarial texts with\\nhigh similarity to the original texts while introducing minimal perturbation.\\nThe rationale is that we expect the adversarial texts with small perturbation\\ncan better preserve the semantic meaning of original texts. Experiments show\\nthat, compared with state-of-the-art attack approaches, our approach achieves\\nhigher success rates and lower perturbation rates in four benchmark datasets.\\n', 'publish_date': 'Sat, 12 Nov 2022 04:46:07 GMT'}\n",
    "\n",
    "Answer:\n",
    "\n",
    "==================================================\n",
    "生成嵌入中: 100%|██████████| 1/1 [00:00<00:00, 17.17it/s]\n",
    "[INFO] Query vector shape: (1, 1, 768)\n",
    "\n",
    "\n",
    "cost 0.1202 seconds\n",
    "\n",
    "\n",
    "You are a professional academic assistant specializing in the cs.CL domain. Strictly based on the provided literature information, answer the user's query. You may slightly expand and refine the response using existing knowledge, but avoid fabricating information. The response language should match the user's query language (English for English queries, Chinese for Chinese queries). Ensure the response is concise and limited to 500 words. Include a citation with the corresponding literature link (like [text](f\"https://arxiv.org/abs/{{id}}\")) and adjust the response style based on the temperature parameter (from 0.0 to 1.0). If the temperature is low (e.g., 0.2), provide a strict and precise answer. If the temperature is high (e.g., 0.8), provide a more creative and exploratory answer.\n",
    "\n",
    "Temperature: 0.2\n",
    "Question: 解释一下如何使用BERT生成黑盒对抗攻击样本\n",
    "Relevant Literature: \n",
    "ID: 2004.09984, Distance: 0.3013704717159271, Content: {'title': 'BERT-ATTACK: Adversarial Attack Against BERT Using BERT', 'doi': None, 'categories': 'cs.CL', 'abstract': '  Adversarial attacks for discrete data (such as texts) have been proved\\nsignificantly more challenging than continuous data (such as images) since it\\nis difficult to generate adversarial samples with gradient-based methods.\\nCurrent successful attack methods for texts usually adopt heuristic replacement\\nstrategies on the character or word level, which remains challenging to find\\nthe optimal solution in the massive space of possible combinations of\\nreplacements while preserving semantic consistency and language fluency. In\\nthis paper, we propose \\\\textbf{BERT-Attack}, a high-quality and effective\\nmethod to generate adversarial samples using pre-trained masked language models\\nexemplified by BERT. We turn BERT against its fine-tuned models and other deep\\nneural models in downstream tasks so that we can successfully mislead the\\ntarget models to predict incorrectly. Our method outperforms state-of-the-art\\nattack strategies in both success rate and perturb percentage, while the\\ngenerated adversarial samples are fluent and semantically preserved. Also, the\\ncost of calculation is low, thus possible for large-scale generations. The code\\nis available at https://github.com/LinyangLee/BERT-Attack.\\n', 'publish_date': 'Tue, 21 Apr 2020 13:30:02 GMT'}\n",
    "ID: 2003.04985, Distance: 0.338335245847702, Content: {'title': 'Adv-BERT: BERT is not robust on misspellings! Generating nature\\n  adversarial samples on BERT', 'doi': None, 'categories': 'cs.CL', 'abstract': '  There is an increasing amount of literature that claims the brittleness of\\ndeep neural networks in dealing with adversarial examples that are created\\nmaliciously. It is unclear, however, how the models will perform in realistic\\nscenarios where \\\\textit{natural rather than malicious} adversarial instances\\noften exist. This work systematically explores the robustness of BERT, the\\nstate-of-the-art Transformer-style model in NLP, in dealing with noisy data,\\nparticularly mistakes in typing the keyboard, that occur inadvertently.\\nIntensive experiments on sentiment analysis and question answering benchmarks\\nindicate that: (i) Typos in various words of a sentence do not influence\\nequally. The typos in informative words make severer damages; (ii) Mistype is\\nthe most damaging factor, compared with inserting, deleting, etc.; (iii) Humans\\nand machines have different focuses on recognizing adversarial attacks.\\n', 'publish_date': 'Thu, 27 Feb 2020 22:07:11 GMT'}\n",
    "ID: 2008.04203, Distance: 0.33911171555519104, Content: {'title': 'FireBERT: Hardening BERT-based classifiers against adversarial attack', 'doi': None, 'categories': 'cs.CL cs.LG', 'abstract': '  We present FireBERT, a set of three proof-of-concept NLP classifiers hardened\\nagainst TextFooler-style word-perturbation by producing diverse alternatives to\\noriginal samples. In one approach, we co-tune BERT against the training data\\nand synthetic adversarial samples. In a second approach, we generate the\\nsynthetic samples at evaluation time through substitution of words and\\nperturbation of embedding vectors. The diversified evaluation results are then\\ncombined by voting. A third approach replaces evaluation-time word substitution\\nwith perturbation of embedding vectors. We evaluate FireBERT for MNLI and IMDB\\nMovie Review datasets, in the original and on adversarial examples generated by\\nTextFooler. We also test whether TextFooler is less successful in creating new\\nadversarial samples when manipulating FireBERT, compared to working on\\nunhardened classifiers. We show that it is possible to improve the accuracy of\\nBERT-based models in the face of adversarial attacks without significantly\\nreducing the accuracy for regular benchmark samples. We present co-tuning with\\na synthetic data generator as a highly effective method to protect against 95%\\nof pre-manufactured adversarial samples while maintaining 98% of original\\nbenchmark performance. We also demonstrate evaluation-time perturbation as a\\npromising direction for further research, restoring accuracy up to 75% of\\nbenchmark performance for pre-made adversarials, and up to 65% (from a baseline\\nof 75% orig. / 12% attack) under active attack by TextFooler.\\n', 'publish_date': 'Mon, 10 Aug 2020 15:43:28 GMT'}\n",
    "ID: 2109.07403, Distance: 0.3467260003089905, Content: {'title': 'BERT is Robust! A Case Against Synonym-Based Adversarial Examples in\\n  Text Classification', 'doi': None, 'categories': 'cs.CL', 'abstract': '  Deep Neural Networks have taken Natural Language Processing by storm. While\\nthis led to incredible improvements across many tasks, it also initiated a new\\nresearch field, questioning the robustness of these neural networks by\\nattacking them. In this paper, we investigate four word substitution-based\\nattacks on BERT. We combine a human evaluation of individual word substitutions\\nand a probabilistic analysis to show that between 96% and 99% of the analyzed\\nattacks do not preserve semantics, indicating that their success is mainly\\nbased on feeding poor data to the model. To further confirm that, we introduce\\nan efficient data augmentation procedure and show that many adversarial\\nexamples can be prevented by including data similar to the attacks during\\ntraining. An additional post-processing step reduces the success rates of\\nstate-of-the-art attacks below 5%. Finally, by looking at more reasonable\\nthresholds on constraints for word substitutions, we conclude that BERT is a\\nlot more robust than research on attacks suggests.\\n', 'publish_date': 'Wed, 15 Sep 2021 16:15:16 GMT'}\n",
    "ID: 2103.10013, Distance: 0.35214006900787354, Content: {'title': 'Model Extraction and Adversarial Transferability, Your BERT is\\n  Vulnerable!', 'doi': None, 'categories': 'cs.CL', 'abstract': '  Natural language processing (NLP) tasks, ranging from text classification to\\ntext generation, have been revolutionised by the pre-trained language models,\\nsuch as BERT. This allows corporations to easily build powerful APIs by\\nencapsulating fine-tuned BERT models for downstream tasks. However, when a\\nfine-tuned BERT model is deployed as a service, it may suffer from different\\nattacks launched by malicious users. In this work, we first present how an\\nadversary can steal a BERT-based API service (the victim/target model) on\\nmultiple benchmark datasets with limited prior knowledge and queries. We\\nfurther show that the extracted model can lead to highly transferable\\nadversarial attacks against the victim model. Our studies indicate that the\\npotential vulnerabilities of BERT-based API services still hold, even when\\nthere is an architectural mismatch between the victim model and the attack\\nmodel. Finally, we investigate two defence strategies to protect the victim\\nmodel and find that unless the performance of the victim model is sacrificed,\\nboth model ex-traction and adversarial transferability can effectively\\ncompromise the target models\\n', 'publish_date': 'Thu, 18 Mar 2021 04:23:21 GMT'}\n",
    "\n",
    "Answer:\n",
    "\n",
    "==================================================\n",
    "生成嵌入中: 100%|██████████| 1/1 [00:00<00:00, 14.33it/s]\n",
    "[INFO] Query vector shape: (1, 1, 768)\n",
    "\n",
    "\n",
    "cost 0.1287 seconds\n",
    "\n",
    "\n",
    "You are a professional academic assistant specializing in the cs.CL domain. Strictly based on the provided literature information, answer the user's query. You may slightly expand and refine the response using existing knowledge, but avoid fabricating information. The response language should match the user's query language (English for English queries, Chinese for Chinese queries). Ensure the response is concise and limited to 500 words. Include a citation with the corresponding literature link (like [text](f\"https://arxiv.org/abs/{{id}}\")) and adjust the response style based on the temperature parameter (from 0.0 to 1.0). If the temperature is low (e.g., 0.2), provide a strict and precise answer. If the temperature is high (e.g., 0.8), provide a more creative and exploratory answer.\n",
    "\n",
    "Temperature: 0.2\n",
    "Question: 低资源语言的形态丰富性（如黏着语）如何影响少样本词义消歧？基于类型学的迁移学习是否有效？\n",
    "Relevant Literature: \n",
    "ID: 1909.12375, Distance: 0.28734734654426575, Content: {'title': 'On the Importance of Subword Information for Morphological Tasks in\\n  Truly Low-Resource Languages', 'doi': None, 'categories': 'cs.CL', 'abstract': '  Recent work has validated the importance of subword information for word\\nrepresentation learning. Since subwords increase parameter sharing ability in\\nneural models, their value should be even more pronounced in low-data regimes.\\nIn this work, we therefore provide a comprehensive analysis focused on the\\nusefulness of subwords for word representation learning in truly low-resource\\nscenarios and for three representative morphological tasks: fine-grained entity\\ntyping, morphological tagging, and named entity recognition. We conduct a\\nsystematic study that spans several dimensions of comparison: 1) type of data\\nscarcity which can stem from the lack of task-specific training data, or even\\nfrom the lack of unannotated data required to train word embeddings, or both;\\n2) language type by working with a sample of 16 typologically diverse languages\\nincluding some truly low-resource ones (e.g. Rusyn, Buryat, and Zulu); 3) the\\nchoice of the subword-informed word representation method. Our main results\\nshow that subword-informed models are universally useful across all language\\ntypes, with large gains over subword-agnostic embeddings. They also suggest\\nthat the effective use of subwords largely depends on the language (type) and\\nthe task at hand, as well as on the amount of available data for training the\\nembeddings and task-based models, where having sufficient in-task data is a\\nmore critical requirement.\\n', 'publish_date': 'Thu, 26 Sep 2019 20:26:51 GMT'}\n",
    "ID: 2004.13304, Distance: 0.29905104637145996, Content: {'title': 'Learning to Learn Morphological Inflection for Resource-Poor Languages', 'doi': None, 'categories': 'cs.CL', 'abstract': '  We propose to cast the task of morphological inflection - mapping a lemma to\\nan indicated inflected form - for resource-poor languages as a meta-learning\\nproblem. Treating each language as a separate task, we use data from\\nhigh-resource source languages to learn a set of model parameters that can\\nserve as a strong initialization point for fine-tuning on a resource-poor\\ntarget language. Experiments with two model architectures on 29 target\\nlanguages from 3 families show that our suggested approach outperforms all\\nbaselines. In particular, it obtains a 31.7% higher absolute accuracy than a\\npreviously proposed cross-lingual transfer model and outperforms the previous\\nstate of the art by 1.7% absolute accuracy on average over languages.\\n', 'publish_date': 'Tue, 28 Apr 2020 05:13:17 GMT'}\n",
    "ID: 1705.02314, Distance: 0.3131658732891083, Content: {'title': 'Building Morphological Chains for Agglutinative Languages', 'doi': None, 'categories': 'cs.CL', 'abstract': '  In this paper, we build morphological chains for agglutinative languages by\\nusing a log-linear model for the morphological segmentation task. The model is\\nbased on the unsupervised morphological segmentation system called\\nMorphoChains. We extend MorphoChains log linear model by expanding the\\ncandidate space recursively to cover more split points for agglutinative\\nlanguages such as Turkish, whereas in the original model candidates are\\ngenerated by considering only binary segmentation of each word. The results\\nshow that we improve the state-of-art Turkish scores by 12% having a F-measure\\nof 72% and we improve the English scores by 3% having a F-measure of 74%.\\nEventually, the system outperforms both MorphoChains and other well-known\\nunsupervised morphological segmentation systems. The results indicate that\\ncandidate generation plays an important role in such an unsupervised log-linear\\nmodel that is learned using contrastive estimation with negative samples.\\n', 'publish_date': 'Fri, 5 May 2017 17:30:50 GMT'}\n",
    "ID: 2411.14198, Distance: 0.31762588024139404, Content: {'title': 'Why do language models perform worse for morphologically complex\\n  languages?', 'doi': None, 'categories': 'cs.CL', 'abstract': '  Language models perform differently across languages. It has been previously\\nsuggested that morphological typology may explain some of this variability\\n(Cotterell et al., 2018). We replicate previous analyses and find additional\\nnew evidence for a performance gap between agglutinative and fusional\\nlanguages, where fusional languages, such as English, tend to have better\\nlanguage modeling performance than morphologically more complex languages like\\nTurkish. We then propose and test three possible causes for this performance\\ngap: morphological alignment of tokenizers, tokenization quality, and\\ndisparities in dataset sizes and measurement. To test the morphological\\nalignment hypothesis, we present MorphScore, a tokenizer evaluation metric, and\\nsupporting datasets for 22 languages. We find some evidence that tokenization\\nquality explains the performance gap, but none for the role of morphological\\nalignment. Instead we find that the performance gap is most reduced when\\ntraining datasets are of equivalent size across language types, but only when\\nscaled according to the so-called \"byte-premium\" -- the different encoding\\nefficiencies of different languages and orthographies. These results suggest\\nthat no language is harder or easier for a language model to learn on the basis\\nof its morphological typology. Differences in performance can be attributed to\\ndisparities in dataset size. These results bear on ongoing efforts to improve\\nperformance for low-performing and under-resourced languages.\\n', 'publish_date': 'Thu, 21 Nov 2024 15:06:51 GMT'}\n",
    "ID: 1707.09569, Distance: 0.3180468678474426, Content: {'title': 'Learning Language Representations for Typology Prediction', 'doi': None, 'categories': 'cs.CL', 'abstract': '  One central mystery of neural NLP is what neural models \"know\" about their\\nsubject matter. When a neural machine translation system learns to translate\\nfrom one language to another, does it learn the syntax or semantics of the\\nlanguages? Can this knowledge be extracted from the system to fill holes in\\nhuman scientific knowledge? Existing typological databases contain relatively\\nfull feature specifications for only a few hundred languages. Exploiting the\\nexistence of parallel texts in more than a thousand languages, we build a\\nmassive many-to-one neural machine translation (NMT) system from 1017 languages\\ninto English, and use this to predict information missing from typological\\ndatabases. Experiments show that the proposed method is able to infer not only\\nsyntactic, but also phonological and phonetic inventory features, and improves\\nover a baseline that has access to information about the languages\\' geographic\\nand phylogenetic neighbors.\\n', 'publish_date': 'Sat, 29 Jul 2017 23:38:25 GMT'}\n",
    "\n",
    "Answer:\n",
    "\n",
    "==================================================\n",
    "生成嵌入中: 100%|██████████| 1/1 [00:00<00:00, 11.54it/s]\n",
    "[INFO] Query vector shape: (1, 1, 768)\n",
    "\n",
    "\n",
    "cost 0.1452 seconds\n",
    "\n",
    "\n",
    "You are a professional academic assistant specializing in the cs.CL domain. Strictly based on the provided literature information, answer the user's query. You may slightly expand and refine the response using existing knowledge, but avoid fabricating information. The response language should match the user's query language (English for English queries, Chinese for Chinese queries). Ensure the response is concise and limited to 500 words. Include a citation with the corresponding literature link (like [text](f\"https://arxiv.org/abs/{{id}}\")) and adjust the response style based on the temperature parameter (from 0.0 to 1.0). If the temperature is low (e.g., 0.2), provide a strict and precise answer. If the temperature is high (e.g., 0.8), provide a more creative and exploratory answer.\n",
    "\n",
    "Temperature: 0.2\n",
    "Question: BGE模型如何将文本向量化\n",
    "Relevant Literature: \n",
    "ID: 2303.07203, Distance: 0.4288061559200287, Content: {'title': 'On the Robustness of Text Vectorizers', 'doi': None, 'categories': 'cs.CL cs.LG stat.ML', 'abstract': '  A fundamental issue in machine learning is the robustness of the model with\\nrespect to changes in the input. In natural language processing, models\\ntypically contain a first embedding layer, transforming a sequence of tokens\\ninto vector representations. While the robustness with respect to changes of\\ncontinuous inputs is well-understood, the situation is less clear when\\nconsidering discrete changes, for instance replacing a word by another in an\\ninput sentence. Our work formally proves that popular embedding schemes, such\\nas concatenation, TF-IDF, and Paragraph Vector (a.k.a. doc2vec), exhibit\\nrobustness in the H\\\\\"older or Lipschitz sense with respect to the Hamming\\ndistance. We provide quantitative bounds for these schemes and demonstrate how\\nthe constants involved are affected by the length of the document. These\\nfindings are exemplified through a series of numerical examples.\\n', 'publish_date': 'Thu, 9 Mar 2023 16:37:37 GMT'}\n",
    "ID: 1607.00534, Distance: 0.4422970712184906, Content: {'title': 'Text comparison using word vector representations and dimensionality\\n  reduction', 'doi': None, 'categories': 'cs.CL', 'abstract': '  This paper describes a technique to compare large text sources using word\\nvector representations (word2vec) and dimensionality reduction (t-SNE) and how\\nit can be implemented using Python. The technique provides a bird\\'s-eye view of\\ntext sources, e.g. text summaries and their source material, and enables users\\nto explore text sources like a geographical map. Word vector representations\\ncapture many linguistic properties such as gender, tense, plurality and even\\nsemantic concepts like \"capital city of\". Using dimensionality reduction, a 2D\\nmap can be computed where semantically similar words are close to each other.\\nThe technique uses the word2vec model from the gensim Python library and t-SNE\\nfrom scikit-learn.\\n', 'publish_date': 'Sat, 2 Jul 2016 17:17:22 GMT'}\n",
    "ID: 1709.05778, Distance: 0.4977453351020813, Content: {'title': 'Word Vector Enrichment of Low Frequency Words in the Bag-of-Words Model\\n  for Short Text Multi-class Classification Problems', 'doi': None, 'categories': 'cs.CL cs.LG', 'abstract': '  The bag-of-words model is a standard representation of text for many linear\\nclassifier learners. In many problem domains, linear classifiers are preferred\\nover more complex models due to their efficiency, robustness and\\ninterpretability, and the bag-of-words text representation can capture\\nsufficient information for linear classifiers to make highly accurate\\npredictions. However in settings where there is a large vocabulary, large\\nvariance in the frequency of terms in the training corpus, many classes and\\nvery short text (e.g., single sentences or document titles) the bag-of-words\\nrepresentation becomes extremely sparse, and this can reduce the accuracy of\\nclassifiers. A particular issue in such settings is that short texts tend to\\ncontain infrequently occurring or rare terms which lack class-conditional\\nevidence. In this work we introduce a method for enriching the bag-of-words\\nmodel by complementing such rare term information with related terms from both\\ngeneral and domain-specific Word Vector models. By reducing sparseness in the\\nbag-of-words models, our enrichment approach achieves improved classification\\nover several baseline classifiers in a variety of text classification problems.\\nOur approach is also efficient because it requires no change to the linear\\nclassifier before or during training, since bag-of-words enrichment applies\\nonly to text being classified.\\n', 'publish_date': 'Mon, 18 Sep 2017 05:00:34 GMT'}\n",
    "ID: cs/0412114, Distance: 0.5134426951408386, Content: {'title': 'State of the Art, Evaluation and Recommendations regarding \"Document\\n  Processing and Visualization Techniques\"', 'doi': None, 'categories': 'cs.CL', 'abstract': '  Several Networks of Excellence have been set up in the framework of the\\nEuropean FP5 research program. Among these Networks of Excellence, the NEMIS\\nproject focuses on the field of Text Mining.\\n  Within this field, document processing and visualization was identified as\\none of the key topics and the WG1 working group was created in the NEMIS\\nproject, to carry out a detailed survey of techniques associated with the text\\nmining process and to identify the relevant research topics in related research\\nareas.\\n  In this document we present the results of this comprehensive survey. The\\nreport includes a description of the current state-of-the-art and practice, a\\nroadmap for follow-up research in the identified areas, and recommendations for\\nanticipated technological development in the domain of text mining.\\n', 'publish_date': 'Wed, 29 Dec 2004 15:19:03 GMT'}\n",
    "ID: cmp-lg/9706016, Distance: 0.5153446197509766, Content: {'title': 'Text Segmentation Using Exponential Models', 'doi': None, 'categories': 'cmp-lg cs.CL', 'abstract': '  This paper introduces a new statistical approach to partitioning text\\nautomatically into coherent segments. Our approach enlists both short-range and\\nlong-range language models to help it sniff out likely sites of topic changes\\nin text. To aid its search, the system consults a set of simple lexical hints\\nit has learned to associate with the presence of boundaries through inspection\\nof a large corpus of annotated data. We also propose a new probabilistically\\nmotivated error metric for use by the natural language processing and\\ninformation retrieval communities, intended to supersede precision and recall\\nfor appraising segmentation algorithms. Qualitative assessment of our algorithm\\nas well as evaluation using this new metric demonstrate the effectiveness of\\nour approach in two very different domains, Wall Street Journal articles and\\nthe TDT Corpus, a collection of newswire articles and broadcast news\\ntranscripts.\\n', 'publish_date': 'Wed, 11 Jun 1997 20:16:04 GMT'}\n",
    "\n",
    "Answer:\n",
    "\n",
    "==================================================\n",
    "生成嵌入中: 100%|██████████| 1/1 [00:00<00:00, 12.45it/s]\n",
    "[INFO] Query vector shape: (1, 1, 768)\n",
    "\n",
    "\n",
    "cost 0.1396 seconds\n",
    "\n",
    "\n",
    "You are a professional academic assistant specializing in the cs.CL domain. Strictly based on the provided literature information, answer the user's query. You may slightly expand and refine the response using existing knowledge, but avoid fabricating information. The response language should match the user's query language (English for English queries, Chinese for Chinese queries). Ensure the response is concise and limited to 500 words. Include a citation with the corresponding literature link (like [text](f\"https://arxiv.org/abs/{{id}}\")) and adjust the response style based on the temperature parameter (from 0.0 to 1.0). If the temperature is low (e.g., 0.2), provide a strict and precise answer. If the temperature is high (e.g., 0.8), provide a more creative and exploratory answer.\n",
    "\n",
    "Temperature: 0.2\n",
    "Question: 在机器翻译中，如何处理专有名词和术语的翻译问题？\n",
    "Relevant Literature: \n",
    "ID: cmp-lg/9601008, Distance: 0.40932410955429077, Content: {'title': 'Noun Phrase Reference in Japanese-to-English Machine Translation', 'doi': None, 'categories': 'cmp-lg cs.CL', 'abstract': '  This paper shows the necessity of distinguishing different referential uses\\nof noun phrases in machine translation. We argue that differentiating between\\nthe generic, referential and ascriptive uses of noun phrases is the minimum\\nnecessary to generate articles and number correctly when translating from\\nJapanese to English. Heuristics for determining these differences are proposed\\nfor a Japanese-to-English machine translation system. Finally the results of\\nusing the proposed heuristics are shown to have raised the percentage of noun\\nphrases generated with correct use of articles and number in the\\nJapanese-to-English machine translation system ALT-J/E from 65% to 77%.\\n', 'publish_date': 'Tue, 23 Jan 1996 13:52:50 GMT'}\n",
    "ID: cmp-lg/9608014, Distance: 0.4405078887939453, Content: {'title': 'Classifiers in Japanese-to-English Machine Translation', 'doi': None, 'categories': 'cmp-lg cs.CL', 'abstract': '  This paper proposes an analysis of classifiers into four major types: UNIT,\\nMETRIC, GROUP and SPECIES, based on properties of both Japanese and English.\\nThe analysis makes possible a uniform and straightforward treatment of noun\\nphrases headed by classifiers in Japanese-to-English machine translation, and\\nhas been implemented in the MT system ALT-J/E. Although the analysis is based\\non the characteristics of, and differences between, Japanese and English, it is\\nshown to be also applicable to the unrelated language Thai.\\n', 'publish_date': 'Wed, 21 Aug 1996 06:05:54 GMT'}\n",
    "ID: 2101.10035, Distance: 0.45035213232040405, Content: {'title': 'Facilitating Terminology Translation with Target Lemma Annotations', 'doi': None, 'categories': 'cs.CL', 'abstract': '  Most of the recent work on terminology integration in machine translation has\\nassumed that terminology translations are given already inflected in forms that\\nare suitable for the target language sentence. In day-to-day work of\\nprofessional translators, however, it is seldom the case as translators work\\nwith bilingual glossaries where terms are given in their dictionary forms;\\nfinding the right target language form is part of the translation process. We\\nargue that the requirement for apriori specified target language forms is\\nunrealistic and impedes the practical applicability of previous work. In this\\nwork, we propose to train machine translation systems using a source-side data\\naugmentation method that annotates randomly selected source language words with\\ntheir target language lemmas. We show that systems trained on such augmented\\ndata are readily usable for terminology integration in real-life translation\\nscenarios. Our experiments on terminology translation into the morphologically\\ncomplex Baltic and Uralic languages show an improvement of up to 7 BLEU points\\nover baseline systems with no means for terminology integration and an average\\nimprovement of 4 BLEU points over the previous work. Results of the human\\nevaluation indicate a 47.7% absolute improvement over the previous work in term\\ntranslation accuracy when translating into Latvian.\\n', 'publish_date': 'Mon, 25 Jan 2021 12:07:20 GMT'}\n",
    "ID: 1105.1072, Distance: 0.4505748748779297, Content: {'title': 'English-Lithuanian-English Machine Translation lexicon and engine:\\n  current state and future work', 'doi': None, 'categories': 'cs.CL', 'abstract': '  This article overviews the current state of the English-Lithuanian-English\\nmachine translation system. The first part of the article describes the\\nproblems that system poses today and what actions will be taken to solve them\\nin the future. The second part of the article tackles the main issue of the\\ntranslation process. Article briefly overviews the word sense disambiguation\\nfor MT technique using Google.\\n', 'publish_date': 'Thu, 5 May 2011 13:51:46 GMT'}\n",
    "ID: 1407.1605, Distance: 0.45342761278152466, Content: {'title': \"Les noms propres se traduisent-ils ? \\\\'Etude d'un corpus multilingue\", 'doi': None, 'categories': 'cs.CL', 'abstract': '  In this paper, we tackle the problem of the translation of proper names. We\\nintroduce our hypothesis according to which proper names can be translated more\\noften than most people seem to think. Then, we describe the construction of a\\nparallel multilingual corpus used to illustrate our point. We eventually\\nevaluate both the advantages and limits of this corpus in our study.\\n', 'publish_date': 'Mon, 7 Jul 2014 07:08:07 GMT'}\n",
    "\n",
    "Answer:\n",
    "\n",
    "==================================================\n",
    "生成嵌入中: 100%|██████████| 1/1 [00:00<00:00, 15.40it/s]\n",
    "[INFO] Query vector shape: (1, 1, 768)\n",
    "\n",
    "\n",
    "cost 0.1266 seconds\n",
    "\n",
    "\n",
    "You are a professional academic assistant specializing in the cs.CL domain. Strictly based on the provided literature information, answer the user's query. You may slightly expand and refine the response using existing knowledge, but avoid fabricating information. The response language should match the user's query language (English for English queries, Chinese for Chinese queries). Ensure the response is concise and limited to 500 words. Include a citation with the corresponding literature link (like [text](f\"https://arxiv.org/abs/{{id}}\")) and adjust the response style based on the temperature parameter (from 0.0 to 1.0). If the temperature is low (e.g., 0.2), provide a strict and precise answer. If the temperature is high (e.g., 0.8), provide a more creative and exploratory answer.\n",
    "\n",
    "Temperature: 0.2\n",
    "Question: 在机器翻译中，如何提高对长句和复杂句的翻译质量？\n",
    "Relevant Literature: \n",
    "ID: 2111.00554, Distance: 0.3593173921108246, Content: {'title': 'Quality Estimation Using Round-trip Translation with Sentence Embeddings', 'doi': None, 'categories': 'cs.CL cs.LG', 'abstract': '  Estimating the quality of machine translation systems has been an ongoing\\nchallenge for researchers in this field. Many previous attempts at using\\nround-trip translation as a measure of quality have failed, and there is much\\ndisagreement as to whether it can be a viable method of quality estimation. In\\nthis paper, we revisit round-trip translation, proposing a system which aims to\\nsolve the previous pitfalls found with the approach. Our method makes use of\\nrecent advances in language representation learning to more accurately gauge\\nthe similarity between the original and round-trip translated sentences.\\nExperiments show that while our approach does not reach the performance of\\ncurrent state of the art methods, it may still be an effective approach for\\nsome language pairs.\\n', 'publish_date': 'Sun, 31 Oct 2021 17:51:12 GMT'}\n",
    "ID: 2005.03519, Distance: 0.3666062653064728, Content: {'title': 'Practical Perspectives on Quality Estimation for Machine Translation', 'doi': None, 'categories': 'cs.CL', 'abstract': '  Sentence level quality estimation (QE) for machine translation (MT) attempts\\nto predict the translation edit rate (TER) cost of post-editing work required\\nto correct MT output. We describe our view on sentence-level QE as dictated by\\nseveral practical setups encountered in the industry. We find consumers of MT\\noutput---whether human or algorithmic ones---to be primarily interested in a\\nbinary quality metric: is the translated sentence adequate as-is or does it\\nneed post-editing? Motivated by this we propose a quality classification (QC)\\nview on sentence-level QE whereby we focus on maximizing recall at precision\\nabove a given threshold. We demonstrate that, while classical QE regression\\nmodels fare poorly on this task, they can be re-purposed by replacing the\\noutput regression layer with a binary classification one, achieving 50-60\\\\%\\nrecall at 90\\\\% precision. For a high-quality MT system producing 75-80\\\\%\\ncorrect translations, this promises a significant reduction in post-editing\\nwork indeed.\\n', 'publish_date': 'Sat, 2 May 2020 01:50:10 GMT'}\n",
    "ID: 2306.15399, Distance: 0.36823415756225586, Content: {'title': 'Quality Estimation of Machine Translated Texts based on Direct Evidence\\n  from Training Data', 'doi': None, 'categories': 'cs.CL', 'abstract': '  Current Machine Translation systems achieve very good results on a growing\\nvariety of language pairs and data sets. However, it is now well known that\\nthey produce fluent translation outputs that often can contain important\\nmeaning errors. Quality Estimation task deals with the estimation of quality of\\ntranslations produced by a Machine Translation system without depending on\\nReference Translations. A number of approaches have been suggested over the\\nyears. In this paper we show that the parallel corpus used as training data for\\ntraining the MT system holds direct clues for estimating the quality of\\ntranslations produced by the MT system. Our experiments show that this simple\\nand direct method holds promise for quality estimation of translations produced\\nby any purely data driven machine translation system.\\n', 'publish_date': 'Tue, 27 Jun 2023 11:52:28 GMT'}\n",
    "ID: 2109.05016, Distance: 0.3713919520378113, Content: {'title': 'Neural Machine Translation Quality and Post-Editing Performance', 'doi': None, 'categories': 'cs.CL cs.HC', 'abstract': '  We test the natural expectation that using MT in professional translation\\nsaves human processing time. The last such study was carried out by\\nSanchez-Torron and Koehn (2016) with phrase-based MT, artificially reducing the\\ntranslation quality. In contrast, we focus on neural MT (NMT) of high quality,\\nwhich has become the state-of-the-art approach since then and also got adopted\\nby most translation companies.\\n  Through an experimental study involving over 30 professional translators for\\nEnglish -> Czech translation, we examine the relationship between NMT\\nperformance and post-editing time and quality. Across all models, we found that\\nbetter MT systems indeed lead to fewer changes in the sentences in this\\nindustry setting. The relation between system quality and post-editing time is\\nhowever not straightforward and, contrary to the results on phrase-based MT,\\nBLEU is definitely not a stable predictor of the time or final output quality.\\n', 'publish_date': 'Fri, 10 Sep 2021 17:56:02 GMT'}\n",
    "ID: 1802.01451, Distance: 0.3731197714805603, Content: {'title': 'Quantitative Fine-Grained Human Evaluation of Machine Translation\\n  Systems: a Case Study on English to Croatian', 'doi': '10.1007/s10590-018-9214-x', 'categories': 'cs.CL cs.AI', 'abstract': '  This paper presents a quantitative fine-grained manual evaluation approach to\\ncomparing the performance of different machine translation (MT) systems. We\\nbuild upon the well-established Multidimensional Quality Metrics (MQM) error\\ntaxonomy and implement a novel method that assesses whether the differences in\\nperformance for MQM error types between different MT systems are statistically\\nsignificant. We conduct a case study for English-to-Croatian, a language\\ndirection that involves translating into a morphologically rich language, for\\nwhich we compare three MT systems belonging to different paradigms: pure\\nphrase-based, factored phrase-based and neural. First, we design an\\nMQM-compliant error taxonomy tailored to the relevant linguistic phenomena of\\nSlavic languages, which made the annotation process feasible and accurate.\\nErrors in MT outputs were then annotated by two annotators following this\\ntaxonomy. Subsequently, we carried out a statistical analysis which showed that\\nthe best-performing system (neural) reduces the errors produced by the worst\\nsystem (pure phrase-based) by more than half (54\\\\%). Moreover, we conducted an\\nadditional analysis of agreement errors in which we distinguished between short\\n(phrase-level) and long distance (sentence-level) errors. We discovered that\\nphrase-based MT approaches are of limited use for long distance agreement\\nphenomena, for which neural MT was found to be especially effective.\\n', 'publish_date': 'Fri, 2 Feb 2018 14:41:08 GMT'}\n",
    "\n",
    "Answer:\n",
    "\n",
    "==================================================\n",
    "生成嵌入中: 100%|██████████| 1/1 [00:00<00:00, 10.22it/s]\n",
    "[INFO] Query vector shape: (1, 1, 768)\n",
    "\n",
    "\n",
    "cost 0.1584 seconds\n",
    "\n",
    "\n",
    "You are a professional academic assistant specializing in the cs.CL domain. Strictly based on the provided literature information, answer the user's query. You may slightly expand and refine the response using existing knowledge, but avoid fabricating information. The response language should match the user's query language (English for English queries, Chinese for Chinese queries). Ensure the response is concise and limited to 500 words. Include a citation with the corresponding literature link (like [text](f\"https://arxiv.org/abs/{{id}}\")) and adjust the response style based on the temperature parameter (from 0.0 to 1.0). If the temperature is low (e.g., 0.2), provide a strict and precise answer. If the temperature is high (e.g., 0.8), provide a more creative and exploratory answer.\n",
    "\n",
    "Temperature: 0.2\n",
    "Question: 如何构建一个高效的文本语义相似度计算模型？\n",
    "Relevant Literature: \n",
    "ID: 2004.13820, Distance: 0.4167209267616272, Content: {'title': 'Evolution of Semantic Similarity -- A Survey', 'doi': '10.1145/3440755', 'categories': 'cs.CL cs.IR', 'abstract': '  Estimating the semantic similarity between text data is one of the\\nchallenging and open research problems in the field of Natural Language\\nProcessing (NLP). The versatility of natural language makes it difficult to\\ndefine rule-based methods for determining semantic similarity measures. In\\norder to address this issue, various semantic similarity methods have been\\nproposed over the years. This survey article traces the evolution of such\\nmethods, categorizing them based on their underlying principles as\\nknowledge-based, corpus-based, deep neural network-based methods, and hybrid\\nmethods. Discussing the strengths and weaknesses of each method, this survey\\nprovides a comprehensive view of existing systems in place, for new researchers\\nto experiment and develop innovative ideas to address the issue of semantic\\nsimilarity.\\n', 'publish_date': 'Sun, 19 Apr 2020 22:07:39 GMT'}\n",
    "ID: 1310.8059, Distance: 0.4580899477005005, Content: {'title': 'Description and Evaluation of Semantic Similarity Measures Approaches', 'doi': '10.5120/13897-1851', 'categories': 'cs.CL', 'abstract': '  In recent years, semantic similarity measure has a great interest in Semantic\\nWeb and Natural Language Processing (NLP). Several similarity measures have\\nbeen developed, being given the existence of a structured knowledge\\nrepresentation offered by ontologies and corpus which enable semantic\\ninterpretation of terms. Semantic similarity measures compute the similarity\\nbetween concepts/terms included in knowledge sources in order to perform\\nestimations. This paper discusses the existing semantic similarity methods\\nbased on structure, information content and feature approaches. Additionally,\\nwe present a critical evaluation of several categories of semantic similarity\\napproaches based on two standard benchmarks. The aim of this paper is to give\\nan efficient evaluation of all these measures which help researcher and\\npractitioners to select the measure that best fit for their requirements.\\n', 'publish_date': 'Wed, 30 Oct 2013 08:08:43 GMT'}\n",
    "ID: 1610.04533, Distance: 0.46748241782188416, Content: {'title': 'A Comprehensive Comparative Study of Word and Sentence Similarity\\n  Measures', 'doi': '10.5120/ijca2016908259', 'categories': 'cs.IR cs.CL', 'abstract': '  Sentence similarity is considered the basis of many natural language tasks\\nsuch as information retrieval, question answering and text summarization. The\\nsemantic meaning between compared text fragments is based on the words semantic\\nfeatures and their relationships. This article reviews a set of word and\\nsentence similarity measures and compares them on benchmark datasets. On the\\nstudied datasets, results showed that hybrid semantic measures perform better\\nthan both knowledge and corpus based measures.\\n', 'publish_date': 'Wed, 17 Feb 2016 19:33:47 GMT'}\n",
    "ID: 1401.5699, Distance: 0.4688728451728821, Content: {'title': 'Text Relatedness Based on a Word Thesaurus', 'doi': '10.1613/jair.2880', 'categories': 'cs.CL', 'abstract': '  The computation of relatedness between two fragments of text in an automated\\nmanner requires taking into account a wide range of factors pertaining to the\\nmeaning the two fragments convey, and the pairwise relations between their\\nwords. Without doubt, a measure of relatedness between text segments must take\\ninto account both the lexical and the semantic relatedness between words. Such\\na measure that captures well both aspects of text relatedness may help in many\\ntasks, such as text retrieval, classification and clustering. In this paper we\\npresent a new approach for measuring the semantic relatedness between words\\nbased on their implicit semantic links. The approach exploits only a word\\nthesaurus in order to devise implicit semantic links between words. Based on\\nthis approach, we introduce Omiotis, a new measure of semantic relatedness\\nbetween texts which capitalizes on the word-to-word semantic relatedness\\nmeasure (SR) and extends it to measure the relatedness between texts. We\\ngradually validate our method: we first evaluate the performance of the\\nsemantic relatedness measure between individual words, covering word-to-word\\nsimilarity and relatedness, synonym identification and word analogy; then, we\\nproceed with evaluating the performance of our method in measuring text-to-text\\nsemantic relatedness in two tasks, namely sentence-to-sentence similarity and\\nparaphrase recognition. Experimental evaluation shows that the proposed method\\noutperforms every lexicon-based method of semantic relatedness in the selected\\ntasks and the used data sets, and competes well against corpus-based and hybrid\\napproaches.\\n', 'publish_date': 'Wed, 15 Jan 2014 05:41:08 GMT'}\n",
    "ID: cmp-lg/9709008, Distance: 0.47116953134536743, Content: {'title': 'Semantic Similarity Based on Corpus Statistics and Lexical Taxonomy', 'doi': None, 'categories': 'cmp-lg cs.CL', 'abstract': '  This paper presents a new approach for measuring semantic similarity/distance\\nbetween words and concepts. It combines a lexical taxonomy structure with\\ncorpus statistical information so that the semantic distance between nodes in\\nthe semantic space constructed by the taxonomy can be better quantified with\\nthe computational evidence derived from a distributional analysis of corpus\\ndata. Specifically, the proposed measure is a combined approach that inherits\\nthe edge-based approach of the edge counting scheme, which is then enhanced by\\nthe node-based approach of the information content calculation. When tested on\\na common data set of word pair similarity ratings, the proposed approach\\noutperforms other computational models. It gives the highest correlation value\\n(r = 0.828) with a benchmark based on human similarity judgements, whereas an\\nupper bound (r = 0.885) is observed when human subjects replicate the same\\ntask.\\n', 'publish_date': 'Sat, 20 Sep 1997 15:16:26 GMT'}\n",
    "\n",
    "Answer:\n",
    "\n",
    "==================================================\n",
    "生成嵌入中: 100%|██████████| 1/1 [00:00<00:00,  8.75it/s]\n",
    "[INFO] Query vector shape: (1, 1, 768)\n",
    "\n",
    "\n",
    "cost 0.1789 seconds\n",
    "\n",
    "\n",
    "You are a professional academic assistant specializing in the cs.CL domain. Strictly based on the provided literature information, answer the user's query. You may slightly expand and refine the response using existing knowledge, but avoid fabricating information. The response language should match the user's query language (English for English queries, Chinese for Chinese queries). Ensure the response is concise and limited to 500 words. Include a citation with the corresponding literature link (like [text](f\"https://arxiv.org/abs/{{id}}\")) and adjust the response style based on the temperature parameter (from 0.0 to 1.0). If the temperature is low (e.g., 0.2), provide a strict and precise answer. If the temperature is high (e.g., 0.8), provide a more creative and exploratory answer.\n",
    "\n",
    "Temperature: 0.2\n",
    "Question: 在多语言对话系统中，如何实现语言的无缝切换和交流？\n",
    "Relevant Literature: \n",
    "ID: 2502.12813, Distance: 0.41870278120040894, Content: {'title': 'Simulating User Diversity in Task-Oriented Dialogue Systems using Large\\n  Language Models', 'doi': None, 'categories': 'cs.CL', 'abstract': '  In this study, we explore the application of Large Language Models (LLMs) for\\ngenerating synthetic users and simulating user conversations with a\\ntask-oriented dialogue system and present detailed results and their analysis.\\nWe propose a comprehensive novel approach to user simulation technique that\\nuses LLMs to create diverse user profiles, set goals, engage in multi-turn\\ndialogues, and evaluate the conversation success. We employ two proprietary\\nLLMs, namely GPT-4o and GPT-o1 (Achiam et al., 2023), to generate a\\nheterogeneous base of user profiles, characterized by varied demographics,\\nmultiple user goals, different conversational styles, initial knowledge levels,\\ninterests, and conversational objectives. We perform a detailed analysis of the\\nuser profiles generated by LLMs to assess the diversity, consistency, and\\npotential biases inherent in these LLM-generated user simulations. We find that\\nGPT-o1 generates more heterogeneous user distribution across most user\\nattributes, while GPT-4o generates more skewed user attributes. The generated\\nset of user profiles are then utilized to simulate dialogue sessions by\\ninteracting with a task-oriented dialogue system.\\n', 'publish_date': 'Tue, 18 Feb 2025 12:20:16 GMT'}\n",
    "ID: 2003.07568, Distance: 0.4248824417591095, Content: {'title': 'XPersona: Evaluating Multilingual Personalized Chatbot', 'doi': None, 'categories': 'cs.CL', 'abstract': '  Personalized dialogue systems are an essential step toward better\\nhuman-machine interaction. Existing personalized dialogue agents rely on\\nproperly designed conversational datasets, which are mostly monolingual (e.g.,\\nEnglish), which greatly limits the usage of conversational agents in other\\nlanguages. In this paper, we propose a multi-lingual extension of Persona-Chat,\\nnamely XPersona. Our dataset includes persona conversations in six different\\nlanguages other than English for building and evaluating multilingual\\npersonalized agents. We experiment with both multilingual and cross-lingual\\ntrained baselines, and evaluate them against monolingual and\\ntranslation-pipeline models using both automatic and human evaluation.\\nExperimental results show that the multilingual trained models outperform the\\ntranslation-pipeline and that they are on par with the monolingual models, with\\nthe advantage of having a single model across multiple languages. On the other\\nhand, the state-of-the-art cross-lingual trained models achieve inferior\\nperformance to the other models, showing that cross-lingual conversation\\nmodeling is a challenging task. We hope that our dataset and baselines will\\naccelerate research in multilingual dialogue systems.\\n', 'publish_date': 'Tue, 17 Mar 2020 07:52:08 GMT'}\n",
    "ID: 2305.16324, Distance: 0.42533189058303833, Content: {'title': 'Talking with Machines: A Comprehensive Survey of Emergent Dialogue\\n  Systems', 'doi': None, 'categories': 'cs.CL', 'abstract': '  From the earliest experiments in the 20th century to the utilization of large\\nlanguage models and transformers, dialogue systems research has continued to\\nevolve, playing crucial roles in numerous fields. This paper offers a\\ncomprehensive review of these systems, tracing their historical development and\\nexamining their fundamental operations. We analyze popular and emerging\\ndatasets for training and survey key contributions in dialogue systems\\nresearch, including traditional systems and advanced machine learning methods.\\nFinally, we consider conventional and transformer-based evaluation metrics,\\nfollowed by a short discussion of prevailing challenges and future prospects in\\nthe field.\\n', 'publish_date': 'Wed, 10 May 2023 12:24:03 GMT'}\n",
    "ID: cmp-lg/9612004, Distance: 0.42772236466407776, Content: {'title': 'Dialogos: a Robust System for Human-Machine Spoken Dialogue on the\\n  Telephone', 'doi': None, 'categories': 'cmp-lg cs.CL', 'abstract': '  This paper presents Dialogos, a real-time system for human-machine spoken\\ndialogue on the telephone in task-oriented domains. The system has been tested\\nin a large trial with inexperienced users and it has proved robust enough to\\nallow spontaneous interactions both to users which get good recognition\\nperformance and to the ones which get lower scores. The robust behavior of the\\nsystem has been achieved by combining the use of specific language models\\nduring the recognition phase of analysis, the tolerance toward spontaneous\\nspeech phenomena, the activity of a robust parser, and the use of\\npragmatic-based dialogue knowledge. This integration of the different modules\\nallows to deal with partial or total breakdowns of the different levels of\\nanalysis. We report the field trial data of the system and the evaluation\\nresults of the overall system and of the submodules.\\n', 'publish_date': 'Fri, 20 Dec 1996 09:50:56 GMT'}\n",
    "ID: cs/9903008, Distance: 0.43078622221946716, Content: {'title': 'Empirically Evaluating an Adaptable Spoken Dialogue System', 'doi': None, 'categories': 'cs.CL', 'abstract': \"  Recent technological advances have made it possible to build real-time,\\ninteractive spoken dialogue systems for a wide variety of applications.\\nHowever, when users do not respect the limitations of such systems, performance\\ntypically degrades. Although users differ with respect to their knowledge of\\nsystem limitations, and although different dialogue strategies make system\\nlimitations more apparent to users, most current systems do not try to improve\\nperformance by adapting dialogue behavior to individual users. This paper\\npresents an empirical evaluation of TOOT, an adaptable spoken dialogue system\\nfor retrieving train schedules on the web. We conduct an experiment in which 20\\nusers carry out 4 tasks with both adaptable and non-adaptable versions of TOOT,\\nresulting in a corpus of 80 dialogues. The values for a wide range of\\nevaluation measures are then extracted from this corpus. Our results show that\\nadaptable TOOT generally outperforms non-adaptable TOOT, and that the utility\\nof adaptation depends on TOOT's initial dialogue strategies.\\n\", 'publish_date': 'Fri, 5 Mar 1999 22:03:13 GMT'}\n",
    "\n",
    "Answer:\n",
    "\n",
    "==================================================\n",
    "生成嵌入中: 100%|██████████| 1/1 [00:00<00:00,  9.74it/s]\n",
    "[INFO] Query vector shape: (1, 1, 768)\n",
    "\n",
    "\n",
    "cost 0.1648 seconds\n",
    "\n",
    "\n",
    "You are a professional academic assistant specializing in the cs.CL domain. Strictly based on the provided literature information, answer the user's query. You may slightly expand and refine the response using existing knowledge, but avoid fabricating information. The response language should match the user's query language (English for English queries, Chinese for Chinese queries). Ensure the response is concise and limited to 500 words. Include a citation with the corresponding literature link (like [text](f\"https://arxiv.org/abs/{{id}}\")) and adjust the response style based on the temperature parameter (from 0.0 to 1.0). If the temperature is low (e.g., 0.2), provide a strict and precise answer. If the temperature is high (e.g., 0.8), provide a more creative and exploratory answer.\n",
    "\n",
    "Temperature: 0.2\n",
    "Question: 在诗歌生成任务中，如何让机器生成符合诗歌格律和意境的作品？\n",
    "Relevant Literature: \n",
    "ID: 2205.01821, Distance: 0.3823384940624237, Content: {'title': 'Zero-shot Sonnet Generation with Discourse-level Planning and Aesthetics\\n  Features', 'doi': None, 'categories': 'cs.CL cs.AI cs.LG', 'abstract': '  Poetry generation, and creative language generation in general, usually\\nsuffers from the lack of large training data. In this paper, we present a novel\\nframework to generate sonnets that does not require training on poems. We\\ndesign a hierarchical framework which plans the poem sketch before decoding.\\nSpecifically, a content planning module is trained on non-poetic texts to\\nobtain discourse-level coherence; then a rhyme module generates rhyme words and\\na polishing module introduces imagery and similes for aesthetics purposes.\\nFinally, we design a constrained decoding algorithm to impose the\\nmeter-and-rhyme constraint of the generated sonnets. Automatic and human\\nevaluation show that our multi-stage approach without training on poem corpora\\ngenerates more coherent, poetic, and creative sonnets than several strong\\nbaselines.\\n', 'publish_date': 'Tue, 3 May 2022 23:44:28 GMT'}\n",
    "ID: 2412.15263, Distance: 0.3843775987625122, Content: {'title': \"PROPOE 2: Avan\\\\c{c}os na S\\\\'intese Computacional de Poemas Baseados em\\n  Prosa Liter\\\\'aria Brasileira\", 'doi': None, 'categories': 'cs.CL', 'abstract': \"  The computational generation of poems is a complex task, which involves\\nseveral sound, prosodic and rhythmic resources. In this work we present PROPOE\\n2, with the extension of structural and rhythmic possibilities compared to the\\noriginal system, generating poems from metered sentences extracted from the\\nprose of Brazilian literature, with multiple rhythmic assembly criteria. These\\nadvances allow for a more coherent exploration of rhythms and sound effects for\\nthe poem. Results of poems generated by the system are demonstrated, with\\nvariations in parameters to exemplify generation and evaluation using various\\ncriteria.\\n  A gera\\\\c{c}\\\\~ao computacional de poemas \\\\'e uma tarefa complexa, que envolve\\ndiversos recursos sonoros, pros\\\\'odicos e r\\\\'itmicos. Neste trabalho\\napresentamos PROPOE 2, com a amplia\\\\c{c}\\\\~ao de possibilidades estruturais e\\nr\\\\'itmicas em rela\\\\c{c}\\\\~ao ao sistema original, gerando poemas a partir de\\nsenten\\\\c{c}as metrificadas extra\\\\'idas da prosa da literatura brasileira, com\\nm\\\\'ultiplos crit\\\\'erios r\\\\'itmicos de montagem. Esses avan\\\\c{c}os permitem uma\\nexplora\\\\c{c}\\\\~ao mais coerente de ritmos e efeitos sonoros para o poema.\\nResultados de poemas gerados pelo sistema s\\\\~ao demonstrados, com\\nvaria\\\\c{c}\\\\~oes de par\\\\^ametros para exemplificar a gera\\\\c{c}\\\\~ao e a\\navalia\\\\c{c}\\\\~ao pelos variados crit\\\\'erios.\\n\", 'publish_date': 'Mon, 16 Dec 2024 20:53:24 GMT'}\n",
    "ID: 1910.13946, Distance: 0.4063025414943695, Content: {'title': \"Let's FACE it. Finnish Poetry Generation with Aesthetics and Framing\", 'doi': None, 'categories': 'cs.CL', 'abstract': '  We present a creative poem generator for the morphologically rich Finnish\\nlanguage. Our method falls into the master-apprentice paradigm, where a\\ncomputationally creative genetic algorithm teaches a BRNN model to generate\\npoetry. We model several parts of poetic aesthetics in the fitness function of\\nthe genetic algorithm, such as sonic features, semantic coherence, imagery and\\nmetaphor. Furthermore, we justify the creativity of our method based on the\\nFACE theory on computational creativity and take additional care in evaluating\\nour system by automatic metrics for concepts together with human evaluation for\\naesthetics, framing and expressions.\\n', 'publish_date': 'Wed, 30 Oct 2019 16:00:54 GMT'}\n",
    "ID: 2002.02511, Distance: 0.41297638416290283, Content: {'title': 'Introducing Aspects of Creativity in Automatic Poetry Generation', 'doi': None, 'categories': 'cs.CL cs.AI cs.LG', 'abstract': '  Poetry Generation involves teaching systems to automatically generate text\\nthat resembles poetic work. A deep learning system can learn to generate poetry\\non its own by training on a corpus of poems and modeling the particular style\\nof language. In this paper, we propose taking an approach that fine-tunes\\nGPT-2, a pre-trained language model, to our downstream task of poetry\\ngeneration. We extend prior work on poetry generation by introducing creative\\nelements. Specifically, we generate poems that express emotion and elicit the\\nsame in readers, and poems that use the language of dreams---called dream\\npoetry. We are able to produce poems that correctly elicit the emotions of\\nsadness and joy 87.5 and 85 percent, respectively, of the time. We produce\\ndreamlike poetry by training on a corpus of texts that describe dreams. Poems\\nfrom this model are shown to capture elements of dream poetry with scores of no\\nless than 3.2 on the Likert scale. We perform crowdsourced human-evaluation for\\nall our poems. We also make use of the Coh-Metrix tool, outlining metrics we\\nuse to gauge the quality of text generated.\\n', 'publish_date': 'Thu, 6 Feb 2020 20:44:12 GMT'}\n",
    "ID: 2406.15267, Distance: 0.4135574698448181, Content: {'title': 'Evaluating Diversity in Automatic Poetry Generation', 'doi': None, 'categories': 'cs.CL', 'abstract': '  Natural Language Generation (NLG), and more generally generative AI, are\\namong the currently most impactful research fields. Creative NLG, such as\\nautomatic poetry generation, is a fascinating niche in this area. While most\\nprevious research has focused on forms of the Turing test when evaluating\\nautomatic poetry generation -- can humans distinguish between automatic and\\nhuman generated poetry -- we evaluate the diversity of automatically generated\\npoetry (with a focus on quatrains), by comparing distributions of generated\\npoetry to distributions of human poetry along structural, lexical, semantic and\\nstylistic dimensions, assessing different model types (word vs.\\ncharacter-level, general purpose LLMs vs. poetry-specific models), including\\nthe very recent LLaMA3-8B, and types of fine-tuning (conditioned vs.\\nunconditioned). We find that current automatic poetry systems are considerably\\nunderdiverse along multiple dimensions -- they often do not rhyme sufficiently,\\nare semantically too uniform and even do not match the length distribution of\\nhuman poetry. Our experiments reveal, however, that style-conditioning and\\ncharacter-level modeling clearly increases diversity across virtually all\\ndimensions we explore. Our identified limitations may serve as the basis for\\nmore genuinely diverse future poetry generation models.\\n', 'publish_date': 'Fri, 21 Jun 2024 16:03:21 GMT'}\n",
    "\n",
    "Answer:\n",
    "\n",
    "==================================================\n",
    "生成嵌入中: 100%|██████████| 1/1 [00:00<00:00, 10.79it/s]\n",
    "[INFO] Query vector shape: (1, 1, 768)\n",
    "\n",
    "\n",
    "cost 0.1722 seconds\n",
    "\n",
    "\n",
    "You are a professional academic assistant specializing in the cs.CL domain. Strictly based on the provided literature information, answer the user's query. You may slightly expand and refine the response using existing knowledge, but avoid fabricating information. The response language should match the user's query language (English for English queries, Chinese for Chinese queries). Ensure the response is concise and limited to 500 words. Include a citation with the corresponding literature link (like [text](f\"https://arxiv.org/abs/{{id}}\")) and adjust the response style based on the temperature parameter (from 0.0 to 1.0). If the temperature is low (e.g., 0.2), provide a strict and precise answer. If the temperature is high (e.g., 0.8), provide a more creative and exploratory answer.\n",
    "\n",
    "Temperature: 0.2\n",
    "Question: 在文本纠错任务中，如何提高对拼写错误、语法错误的检测和纠正能力？\n",
    "Relevant Literature: \n",
    "ID: cmp-lg/9502031, Distance: 0.3797731101512909, Content: {'title': 'Cooperative Error Handling and Shallow Processing', 'doi': None, 'categories': 'cmp-lg cs.CL', 'abstract': '  This paper is concerned with the detection and correction of sub-sentential\\nEnglish text errors. Previous spelling programs, unless restricted to a very\\nsmall set of words, have operated as post-processors. And to date, grammar\\ncheckers and other programs which deal with ill-formed input usually step\\ndirectly from spelling considerations to a full-scale parse, assuming a\\ncomplete sentence. Work described below is aimed at evaluating the\\neffectiveness of shallow (sub-sentential) processing and the feasibility of\\ncooperative error checking, through building and testing appropriately an\\nerror-processing system. A system under construction is outlined which\\nincorporates morphological checks (using new two-level error rules) over a\\ndirected letter graph, tag positional trigrams and partial parsing. Intended\\ntesting is discussed.\\n', 'publish_date': 'Thu, 23 Feb 1995 11:19:11 GMT'}\n",
    "ID: 2302.06407, Distance: 0.42555558681488037, Content: {'title': 'Correcting Real-Word Spelling Errors: A New Hybrid Approach', 'doi': '10.1093/llc/fqx054', 'categories': 'cs.CL cs.AI', 'abstract': \"  Spelling correction is one of the main tasks in the field of Natural Language\\nProcessing. Contrary to common spelling errors, real-word errors cannot be\\ndetected by conventional spelling correction methods. The real-word correction\\nmodel proposed by Mays, Damerau and Mercer showed a great performance in\\ndifferent evaluations. In this research, however, a new hybrid approach is\\nproposed which relies on statistical and syntactic knowledge to detect and\\ncorrect real-word errors. In this model, Constraint Grammar (CG) is used to\\ndiscriminate among sets of correction candidates in the search space. Mays,\\nDamerau and Mercer's trigram approach is manipulated to estimate the\\nprobability of syntactically well-formed correction candidates. The approach\\nproposed here is tested on the Wall Street Journal corpus. The model can prove\\nto be more practical than some other models, such as WordNet-based method of\\nHirst and Budanitsky and fixed windows size method of Wilcox-O'Hearn and Hirst.\\n\", 'publish_date': 'Thu, 9 Feb 2023 06:03:11 GMT'}\n",
    "ID: cmp-lg/9806010, Distance: 0.42740118503570557, Content: {'title': 'Towards a single proposal is spelling correction', 'doi': None, 'categories': 'cmp-lg cs.CL', 'abstract': '  The study presented here relies on the integrated use of different kinds of\\nknowledge in order to improve first-guess accuracy in non-word\\ncontext-sensitive correction for general unrestricted texts. State of the art\\nspelling correction systems, e.g. ispell, apart from detecting spelling errors,\\nalso assist the user by offering a set of candidate corrections that are close\\nto the misspelled word. Based on the correction proposals of ispell, we built\\nseveral guessers, which were combined in different ways. Firstly, we evaluated\\nall possibilities and selected the best ones in a corpus with artificially\\ngenerated typing errors. Secondly, the best combinations were tested on texts\\nwith genuine spelling errors. The results for the latter suggest that we can\\nexpect automatic non-word correction for all the errors in a free running text\\nwith 80% precision and a single proposal 98% of the times (1.02 proposals on\\naverage).\\n', 'publish_date': 'Mon, 15 Jun 1998 19:38:11 GMT'}\n",
    "ID: 2112.01846, Distance: 0.43136054277420044, Content: {'title': 'A Proposal of Automatic Error Correction in Text', 'doi': None, 'categories': 'cs.CL cs.AI', 'abstract': '  The great amount of information that can be stored in electronic media is\\ngrowing up daily. Many of them is got mainly by typing, such as the huge of\\ninformation obtained from web 2.0 sites; or scaned and processing by an Optical\\nCharacter Recognition software, like the texts of libraries and goverment\\noffices. Both processes introduce error in texts, so it is difficult to use the\\ndata for other purposes than just to read it, i.e. the processing of those\\ntexts by other applications like e-learning, learning of languages, electronic\\ntutorials, data minning, information retrieval and even more specialized\\nsystems such as tiflologic software, specifically blinded people-oriented\\napplications like automatic reading, where the text would be error free as\\npossible in order to make easier the text to speech task, and so on. In this\\npaper it is showed an application of automatic recognition and correction of\\nortographic errors in electronic texts. This task is composed of three stages:\\na) error detection; b) candidate corrections generation; and c) correction\\n-selection of the best candidate. The proposal is based in part of speech text\\ncategorization, word similarity, word diccionaries, statistical measures,\\nmorphologic analisys and n-grams based language model of Spanish.\\n', 'publish_date': 'Fri, 24 Sep 2021 17:17:56 GMT'}\n",
    "ID: 2205.05730, Distance: 0.43305298686027527, Content: {'title': 'Some Grammatical Errors are Frequent, Others are Important', 'doi': None, 'categories': 'cs.CL cs.AI cs.CY', 'abstract': '  In Grammatical Error Correction, systems are evaluated by the number of\\nerrors they correct. However, no one has assessed whether all error types are\\nequally important. We provide and apply a method to quantify the importance of\\ndifferent grammatical error types to humans. We show that some rare errors are\\nconsidered disturbing while other common ones are not. This affects possible\\ndirections to improve both systems and their evaluation.\\n', 'publish_date': 'Wed, 11 May 2022 18:59:20 GMT'}\n",
    "\n",
    "Answer:\n",
    "\n",
    "==================================================\n",
    "生成嵌入中: 100%|██████████| 1/1 [00:00<00:00, 11.13it/s]\n",
    "[INFO] Query vector shape: (1, 1, 768)\n",
    "\n",
    "\n",
    "\n",
    "cost 0.1615 seconds\n",
    "\n",
    "\n",
    "You are a professional academic assistant specializing in the cs.CL domain. Strictly based on the provided literature information, answer the user's query. You may slightly expand and refine the response using existing knowledge, but avoid fabricating information. The response language should match the user's query language (English for English queries, Chinese for Chinese queries). Ensure the response is concise and limited to 500 words. Include a citation with the corresponding literature link (like [text](f\"https://arxiv.org/abs/{{id}}\")) and adjust the response style based on the temperature parameter (from 0.0 to 1.0). If the temperature is low (e.g., 0.2), provide a strict and precise answer. If the temperature is high (e.g., 0.8), provide a more creative and exploratory answer.\n",
    "\n",
    "Temperature: 0.2\n",
    "Question: 在机器翻译中，如何更好地处理源语言与目标语言之间的语义差异和文化差异？\n",
    "Relevant Literature: \n",
    "ID: 1805.06522, Distance: 0.3782072961330414, Content: {'title': 'Semantic Relatedness for All (Languages): A Comparative Analysis of\\n  Multilingual Semantic Relatedness Using Machine Translation', 'doi': '10.1007/978-3-319-49004-5_14', 'categories': 'cs.CL', 'abstract': '  This paper provides a comparative analysis of the performance of four\\nstate-of-the-art distributional semantic models (DSMs) over 11 languages,\\ncontrasting the native language-specific models with the use of machine\\ntranslation over English-based DSMs. The experimental results show that there\\nis a significant improvement (average of 16.7% for the Spearman correlation) by\\nusing state-of-the-art machine translation approaches. The results also show\\nthat the benefit of using the most informative corpus outweighs the possible\\nerrors introduced by the machine translation. For all languages, the\\ncombination of machine translation over the Word2Vec English distributional\\nmodel provided the best results consistently (average Spearman correlation of\\n0.68).\\n', 'publish_date': 'Wed, 16 May 2018 20:43:45 GMT'}\n",
    "ID: 1609.03204, Distance: 0.3907979726791382, Content: {'title': 'On the Similarities Between Native, Non-native and Translated Texts', 'doi': None, 'categories': 'cs.CL', 'abstract': '  We present a computational analysis of three language varieties: native,\\nadvanced non-native, and translation. Our goal is to investigate the\\nsimilarities and differences between non-native language productions and\\ntranslations, contrasting both with native language. Using a collection of\\ncomputational methods we establish three main results: (1) the three types of\\ntexts are easily distinguishable; (2) non-native language and translations are\\ncloser to each other than each of them is to native language; and (3) some of\\nthese characteristics depend on the source or native language, while others do\\nnot, reflecting, perhaps, unified principles that similarly affect translations\\nand non-native language.\\n', 'publish_date': 'Sun, 11 Sep 2016 19:51:46 GMT'}\n",
    "ID: 1711.09476, Distance: 0.3975697159767151, Content: {'title': 'Machine Translation using Semantic Web Technologies: A Survey', 'doi': '10.1016/j.websem.2018.07.001', 'categories': 'cs.CL', 'abstract': '  A large number of machine translation approaches have recently been developed\\nto facilitate the fluid migration of content across languages. However, the\\nliterature suggests that many obstacles must still be dealt with to achieve\\nbetter automatic translations. One of these obstacles is lexical and syntactic\\nambiguity. A promising way of overcoming this problem is using Semantic Web\\ntechnologies. This article presents the results of a systematic review of\\nmachine translation approaches that rely on Semantic Web technologies for\\ntranslating texts. Overall, our survey suggests that while Semantic Web\\ntechnologies can enhance the quality of machine translation outputs for various\\nproblems, the combination of both is still in its infancy.\\n', 'publish_date': 'Sun, 26 Nov 2017 22:30:31 GMT'}\n",
    "ID: 1907.10676, Distance: 0.4017772078514099, Content: {'title': 'Semantic Web for Machine Translation: Challenges and Directions', 'doi': None, 'categories': 'cs.CL', 'abstract': '  A large number of machine translation approaches have recently been developed\\nto facilitate the fluid migration of content across languages. However, the\\nliterature suggests that many obstacles must still be dealt with to achieve\\nbetter automatic translations. One of these obstacles is lexical and syntactic\\nambiguity. A promising way of overcoming this problem is using Semantic Web\\ntechnologies. This article is an extended abstract of our systematic review on\\nmachine translation approaches that rely on Semantic Web technologies for\\nimproving the translation of texts. Overall, we present the challenges and\\nopportunities in the use of Semantic Web technologies in Machine Translation.\\nMoreover, our research suggests that while Semantic Web technologies can\\nenhance the quality of machine translation outputs for various problems, the\\ncombination of both is still in its infancy.\\n', 'publish_date': 'Tue, 23 Jul 2019 15:49:20 GMT'}\n",
    "ID: 2401.01419, Distance: 0.41347238421440125, Content: {'title': 'To Diverge or Not to Diverge: A Morphosyntactic Perspective on Machine\\n  Translation vs Human Translation', 'doi': None, 'categories': 'cs.CL', 'abstract': '  We conduct a large-scale fine-grained comparative analysis of machine\\ntranslations (MT) against human translations (HT) through the lens of\\nmorphosyntactic divergence. Across three language pairs and two types of\\ndivergence defined as the structural difference between the source and the\\ntarget, MT is consistently more conservative than HT, with less morphosyntactic\\ndiversity, more convergent patterns, and more one-to-one alignments. Through\\nanalysis on different decoding algorithms, we attribute this discrepancy to the\\nuse of beam search that biases MT towards more convergent patterns. This bias\\nis most amplified when the convergent pattern appears around 50% of the time in\\ntraining data. Lastly, we show that for a majority of morphosyntactic\\ndivergences, their presence in HT is correlated with decreased MT performance,\\npresenting a greater challenge for MT systems.\\n', 'publish_date': 'Tue, 2 Jan 2024 20:05:56 GMT'}\n",
    "\n",
    "Answer:\n",
    "\n",
    "==================================================\n",
    "生成嵌入中: 100%|██████████| 1/1 [00:00<00:00, 11.71it/s]\n",
    "[INFO] Query vector shape: (1, 1, 768)\n",
    "\n",
    "\n",
    "cost 0.1559 seconds\n",
    "\n",
    "\n",
    "You are a professional academic assistant specializing in the cs.CL domain. Strictly based on the provided literature information, answer the user's query. You may slightly expand and refine the response using existing knowledge, but avoid fabricating information. The response language should match the user's query language (English for English queries, Chinese for Chinese queries). Ensure the response is concise and limited to 500 words. Include a citation with the corresponding literature link (like [text](f\"https://arxiv.org/abs/{{id}}\")) and adjust the response style based on the temperature parameter (from 0.0 to 1.0). If the temperature is low (e.g., 0.2), provide a strict and precise answer. If the temperature is high (e.g., 0.8), provide a more creative and exploratory answer.\n",
    "\n",
    "Temperature: 0.2\n",
    "Question: 在文本生成任务中，如何平衡生成文本的多样性、正确性和连贯性？\n",
    "Relevant Literature: \n",
    "ID: 2004.02990, Distance: 0.3690778613090515, Content: {'title': 'Evaluating the Evaluation of Diversity in Natural Language Generation', 'doi': None, 'categories': 'cs.CL', 'abstract': '  Despite growing interest in natural language generation (NLG) models that\\nproduce diverse outputs, there is currently no principled method for evaluating\\nthe diversity of an NLG system. In this work, we propose a framework for\\nevaluating diversity metrics. The framework measures the correlation between a\\nproposed diversity metric and a diversity parameter, a single parameter that\\ncontrols some aspect of diversity in generated text. For example, a diversity\\nparameter might be a binary variable used to instruct crowdsourcing workers to\\ngenerate text with either low or high content diversity. We demonstrate the\\nutility of our framework by: (a) establishing best practices for eliciting\\ndiversity judgments from humans, (b) showing that humans substantially\\noutperform automatic metrics in estimating content diversity, and (c)\\ndemonstrating that existing methods for controlling diversity by tuning a\\n\"decoding parameter\" mostly affect form but not meaning. Our framework can\\nadvance the understanding of different diversity metrics, an essential step on\\nthe road towards better NLG systems.\\n', 'publish_date': 'Mon, 6 Apr 2020 20:44:10 GMT'}\n",
    "ID: 1904.03971, Distance: 0.3912854790687561, Content: {'title': 'Jointly Measuring Diversity and Quality in Text Generation Models', 'doi': None, 'categories': 'cs.LG cs.CL stat.ML', 'abstract': \"  Text generation is an important Natural Language Processing task with various\\napplications. Although several metrics have already been introduced to evaluate\\nthe text generation methods, each of them has its own shortcomings. The most\\nwidely used metrics such as BLEU only consider the quality of generated\\nsentences and neglect their diversity. For example, repeatedly generation of\\nonly one high quality sentence would result in a high BLEU score. On the other\\nhand, the more recent metric introduced to evaluate the diversity of generated\\ntexts known as Self-BLEU ignores the quality of generated texts. In this paper,\\nwe propose metrics to evaluate both the quality and diversity simultaneously by\\napproximating the distance of the learned generative model and the real data\\ndistribution. For this purpose, we first introduce a metric that approximates\\nthis distance using n-gram based measures. Then, a feature-based measure which\\nis based on a recent highly deep model trained on a large text corpus called\\nBERT is introduced. Finally, for oracle training mode in which the generator's\\ndensity can also be calculated, we propose to use the distance measures between\\nthe corresponding explicit distributions. Eventually, the most popular and\\nrecent text generation models are evaluated using both the existing and the\\nproposed metrics and the preferences of the proposed metrics are determined.\\n\", 'publish_date': 'Mon, 8 Apr 2019 11:44:41 GMT'}\n",
    "ID: 2205.10938, Distance: 0.4029007852077484, Content: {'title': 'Diversity Enhanced Table-to-Text Generation via Type Control', 'doi': None, 'categories': 'cs.CL', 'abstract': '  Generating natural language statements to convey logical inferences from\\ntabular data (i.e., Logical NLG) is a process with one input and a variety of\\nvalid outputs. This characteristic underscores the need for a method to produce\\na diverse set of valid outputs, presenting different perspectives of the input\\ndata. We propose a simple yet effective diversity-enhancing scheme that builds\\nupon an inherent property of the statements, their logic-types, by using a\\ntype-controlled table-to-text generation model. We demonstrate, through\\nextensive automatic and human evaluations over the two publicly available\\nLogical NLG datasets, that our proposed method both facilitates the ability to\\neffectively control the generated statement type, and produces results superior\\nto the strongest baselines in terms of quality and factuality-diversity\\ntrade-off.\\n', 'publish_date': 'Sun, 22 May 2022 22:05:21 GMT'}\n",
    "ID: 2203.15108, Distance: 0.4039303660392761, Content: {'title': 'A Well-Composed Text is Half Done! Composition Sampling for Diverse\\n  Conditional Generation', 'doi': None, 'categories': 'cs.CL', 'abstract': '  We propose Composition Sampling, a simple but effective method to generate\\ndiverse outputs for conditional generation of higher quality compared to\\nprevious stochastic decoding strategies. It builds on recently proposed\\nplan-based neural generation models (Narayan et al, 2021) that are trained to\\nfirst create a composition of the output and then generate by conditioning on\\nit and the input. Our approach avoids text degeneration by first sampling a\\ncomposition in the form of an entity chain and then using beam search to\\ngenerate the best possible text grounded to this entity chain. Experiments on\\nsummarization (CNN/DailyMail and XSum) and question generation (SQuAD), using\\nexisting and newly proposed automatic metrics together with human-based\\nevaluation, demonstrate that Composition Sampling is currently the best\\navailable decoding strategy for generating diverse meaningful outputs.\\n', 'publish_date': 'Mon, 28 Mar 2022 21:24:03 GMT'}\n",
    "ID: 2403.00553, Distance: 0.40433967113494873, Content: {'title': 'Standardizing the Measurement of Text Diversity: A Tool and a\\n  Comparative Analysis of Scores', 'doi': None, 'categories': 'cs.CL', 'abstract': \"  The diversity across outputs generated by LLMs shapes perception of their\\nquality and utility. High lexical diversity is often desirable, but there is no\\nstandard method to measure this property. Templated answer structures and\\n``canned'' responses across different documents are readily noticeable, but\\ndifficult to visualize across large corpora. This work aims to standardize\\nmeasurement of text diversity. Specifically, we empirically investigate the\\nconvergent validity of existing scores across English texts, and we release\\ndiversity, an open-source Python package for measuring and extracting\\nrepetition in text. We also build a platform based on diversity for users to\\ninteractively explore repetition in text. We find that fast compression\\nalgorithms capture information similar to what is measured by slow-to-compute\\n$n$-gram overlap homogeneity scores. Further, a combination of measures --\\ncompression ratios, self-repetition of long $n$-grams, and Self-BLEU and\\nBERTScore -- are sufficient to report, as they have low mutual correlation with\\neach other.\\n\", 'publish_date': 'Fri, 1 Mar 2024 14:23:12 GMT'}\n",
    "\n",
    "Answer:\n",
    "\n",
    "==================================================\n",
    "生成嵌入中: 100%|██████████| 1/1 [00:00<00:00, 13.38it/s]\n",
    "[INFO] Query vector shape: (1, 1, 768)\n",
    "\n",
    "\n",
    "cost 0.1364 seconds\n",
    "\n",
    "\n",
    "You are a professional academic assistant specializing in the cs.CL domain. Strictly based on the provided literature information, answer the user's query. You may slightly expand and refine the response using existing knowledge, but avoid fabricating information. The response language should match the user's query language (English for English queries, Chinese for Chinese queries). Ensure the response is concise and limited to 500 words. Include a citation with the corresponding literature link (like [text](f\"https://arxiv.org/abs/{{id}}\")) and adjust the response style based on the temperature parameter (from 0.0 to 1.0). If the temperature is low (e.g., 0.2), provide a strict and precise answer. If the temperature is high (e.g., 0.8), provide a more creative and exploratory answer.\n",
    "\n",
    "Temperature: 0.2\n",
    "Question: 在对话系统中，如何提高对用户意图的理解准确率并生成更加自然流畅的回复？\n",
    "Relevant Literature: \n",
    "ID: 1302.1380, Distance: 0.39405548572540283, Content: {'title': 'Towards the Rapid Development of a Natural Language Understanding Module', 'doi': None, 'categories': 'cs.CL', 'abstract': '  When developing a conversational agent, there is often an urgent need to have\\na prototype available in order to test the application with real users. A\\nWizard of Oz is a possibility, but sometimes the agent should be simply\\ndeployed in the environment where it will be used. Here, the agent should be\\nable to capture as many interactions as possible and to understand how people\\nreact to failure. In this paper, we focus on the rapid development of a natural\\nlanguage understanding module by non experts. Our approach follows the learning\\nparadigm and sees the process of understanding natural language as a\\nclassification problem. We test our module with a conversational agent that\\nanswers questions in the art domain. Moreover, we show how our approach can be\\nused by a natural language interface to a cinema database.\\n', 'publish_date': 'Wed, 6 Feb 2013 14:17:55 GMT'}\n",
    "ID: cmp-lg/9711008, Distance: 0.402814120054245, Content: {'title': 'On the use of expectations for detecting and repairing human-machine\\n  miscommunication', 'doi': None, 'categories': 'cmp-lg cs.CL', 'abstract': \"  In this paper I describe how miscommunication problems are dealt with in the\\nspoken language system DIALOGOS. The dialogue module of the system exploits\\ndialogic expectations in a twofold way: to model what future user utterance\\nmight be about (predictions), and to account how the user's next utterance may\\nbe related to previous ones in the ongoing interaction (pragmatic-based\\nexpectations). The analysis starts from the hypothesis that the occurrence of\\nmiscommunication is concomitant with two pragmatic phenomena: the deviation of\\nthe user from the expected behaviour and the generation of a conversational\\nimplicature. A preliminary evaluation of a large amount of interactions between\\nsubjects and DIALOGOS shows that the system performance is enhanced by the uses\\nof both predictions and pragmatic-based expectations.\\n\", 'publish_date': 'Wed, 19 Nov 1997 16:14:41 GMT'}\n",
    "ID: cs/9809022, Distance: 0.41370248794555664, Content: {'title': 'Modelling Users, Intentions, and Structure in Spoken Dialog', 'doi': None, 'categories': 'cs.CL', 'abstract': '  We outline how utterances in dialogs can be interpreted using a partial first\\norder logic. We exploit the capability of this logic to talk about the truth\\nstatus of formulae to define a notion of coherence between utterances and\\nexplain how this coherence relation can serve for the construction of AND/OR\\ntrees that represent the segmentation of the dialog. In a BDI model we\\nformalize basic assumptions about dialog and cooperative behaviour of\\nparticipants. These assumptions provide a basis for inferring speech acts from\\ncoherence relations between utterances and attitudes of dialog participants.\\nSpeech acts prove to be useful for determining dialog segments defined on the\\nnotion of completing expectations of dialog participants. Finally, we sketch\\nhow explicit segmentation signalled by cue phrases and performatives is covered\\nby our dialog model.\\n', 'publish_date': 'Thu, 17 Sep 1998 11:10:14 GMT'}\n",
    "ID: 2009.13902, Distance: 0.4164215326309204, Content: {'title': 'Utterance-level Dialogue Understanding: An Empirical Study', 'doi': None, 'categories': 'cs.CL', 'abstract': '  The recent abundance of conversational data on the Web and elsewhere calls\\nfor effective NLP systems for dialog understanding. Complete utterance-level\\nunderstanding often requires context understanding, defined by nearby\\nutterances. In recent years, a number of approaches have been proposed for\\nvarious utterance-level dialogue understanding tasks. Most of these approaches\\naccount for the context for effective understanding. In this paper, we explore\\nand quantify the role of context for different aspects of a dialogue, namely\\nemotion, intent, and dialogue act identification, using state-of-the-art dialog\\nunderstanding methods as baselines. Specifically, we employ various\\nperturbations to distort the context of a given utterance and study its impact\\non the different tasks and baselines. This provides us with insights into the\\nfundamental contextual controlling factors of different aspects of a dialogue.\\nSuch insights can inspire more effective dialogue understanding models, and\\nprovide support for future text generation approaches. The implementation\\npertaining to this work is available at\\nhttps://github.com/declare-lab/dialogue-understanding.\\n', 'publish_date': 'Tue, 29 Sep 2020 09:50:21 GMT'}\n",
    "ID: 2311.05450, Distance: 0.4228866696357727, Content: {'title': 'Cognitively Inspired Components for Social Conversational Agents', 'doi': None, 'categories': 'cs.CL cs.AI', 'abstract': '  Current conversational agents (CA) have seen improvement in conversational\\nquality in recent years due to the influence of large language models (LLMs)\\nlike GPT3. However, two key categories of problem remain. Firstly there are the\\nunique technical problems resulting from the approach taken in creating the CA,\\nsuch as scope with retrieval agents and the often nonsensical answers of former\\ngenerative agents. Secondly, humans perceive CAs as social actors, and as a\\nresult expect the CA to adhere to social convention. Failure on the part of the\\nCA in this respect can lead to a poor interaction and even the perception of\\nthreat by the user. As such, this paper presents a survey highlighting a\\npotential solution to both categories of problem through the introduction of\\ncognitively inspired additions to the CA. Through computational facsimiles of\\nsemantic and episodic memory, emotion, working memory, and the ability to\\nlearn, it is possible to address both the technical and social problems\\nencountered by CAs.\\n', 'publish_date': 'Thu, 9 Nov 2023 15:38:58 GMT'}\n",
    "\n",
    "Answer:\n",
    "\n",
    "==================================================\n",
    "生成嵌入中: 100%|██████████| 1/1 [00:00<00:00,  9.02it/s]\n",
    "[INFO] Query vector shape: (1, 1, 768)\n",
    "\n",
    "\n",
    "cost 0.1747 seconds\n",
    "\n",
    "\n",
    "You are a professional academic assistant specializing in the cs.CL domain. Strictly based on the provided literature information, answer the user's query. You may slightly expand and refine the response using existing knowledge, but avoid fabricating information. The response language should match the user's query language (English for English queries, Chinese for Chinese queries). Ensure the response is concise and limited to 500 words. Include a citation with the corresponding literature link (like [text](f\"https://arxiv.org/abs/{{id}}\")) and adjust the response style based on the temperature parameter (from 0.0 to 1.0). If the temperature is low (e.g., 0.2), provide a strict and precise answer. If the temperature is high (e.g., 0.8), provide a more creative and exploratory answer.\n",
    "\n",
    "Temperature: 0.2\n",
    "Question: 如何构建一个鲁棒的文本分类模型，以应对文本数据的噪声和不平衡问题？\n",
    "Relevant Literature: \n",
    "ID: 1904.08067, Distance: 0.4491845965385437, Content: {'title': 'Text Classification Algorithms: A Survey', 'doi': '10.3390/info10040150', 'categories': 'cs.LG cs.AI cs.CL cs.IR stat.ML', 'abstract': '  In recent years, there has been an exponential growth in the number of\\ncomplex documents and texts that require a deeper understanding of machine\\nlearning methods to be able to accurately classify texts in many applications.\\nMany machine learning approaches have achieved surpassing results in natural\\nlanguage processing. The success of these learning algorithms relies on their\\ncapacity to understand complex models and non-linear relationships within data.\\nHowever, finding suitable structures, architectures, and techniques for text\\nclassification is a challenge for researchers. In this paper, a brief overview\\nof text classification algorithms is discussed. This overview covers different\\ntext feature extractions, dimensionality reduction methods, existing algorithms\\nand techniques, and evaluations methods. Finally, the limitations of each\\ntechnique and their application in the real-world problem are discussed.\\n', 'publish_date': 'Wed, 17 Apr 2019 03:29:05 GMT'}\n",
    "ID: 1706.07912, Distance: 0.4678591787815094, Content: {'title': 'Cluster Based Symbolic Representation for Skewed Text Categorization', 'doi': '10.1007/978-981-10-4859-3_19', 'categories': 'cs.IR cs.CL', 'abstract': '  In this work, a problem associated with imbalanced text corpora is addressed.\\nA method of converting an imbalanced text corpus into a balanced one is\\npresented. The presented method employs a clustering algorithm for conversion.\\nInitially to avoid curse of dimensionality, an effective representation scheme\\nbased on term class relevancy measure is adapted, which drastically reduces the\\ndimension to the number of classes in the corpus. Subsequently, the samples of\\nlarger sized classes are grouped into a number of subclasses of smaller sizes\\nto make the entire corpus balanced. Each subclass is then given a single\\nsymbolic vector representation by the use of interval valued features. This\\nsymbolic representation in addition to being compact helps in reducing the\\nspace requirement and also the classification time. The proposed model has been\\nempirically demonstrated for its superiority on bench marking datasets viz.,\\nReuters 21578 and TDT2. Further, it has been compared against several other\\nexisting contemporary models including model based on support vector machine.\\nThe comparative analysis indicates that the proposed model outperforms the\\nother existing models.\\n', 'publish_date': 'Sat, 24 Jun 2017 06:04:21 GMT'}\n",
    "ID: 2010.02458, Distance: 0.4698503315448761, Content: {'title': 'Identifying Spurious Correlations for Robust Text Classification', 'doi': None, 'categories': 'cs.LG cs.CL cs.IR', 'abstract': '  The predictions of text classifiers are often driven by spurious correlations\\n-- e.g., the term `Spielberg\\' correlates with positively reviewed movies, even\\nthough the term itself does not semantically convey a positive sentiment. In\\nthis paper, we propose a method to distinguish spurious and genuine\\ncorrelations in text classification. We treat this as a supervised\\nclassification problem, using features derived from treatment effect estimators\\nto distinguish spurious correlations from \"genuine\" ones. Due to the generic\\nnature of these features and their small dimensionality, we find that the\\napproach works well even with limited training examples, and that it is\\npossible to transport the word classifier to new domains. Experiments on four\\ndatasets (sentiment classification and toxicity detection) suggest that using\\nthis approach to inform feature selection also leads to more robust\\nclassification, as measured by improved worst-case accuracy on the samples\\naffected by spurious correlations.\\n', 'publish_date': 'Tue, 6 Oct 2020 03:49:22 GMT'}\n",
    "ID: 1801.07875, Distance: 0.4810374975204468, Content: {'title': 'Support Vector Machine Active Learning Algorithms with\\n  Query-by-Committee versus Closest-to-Hyperplane Selection', 'doi': '10.1109/ICSC.2018.00029', 'categories': 'cs.LG cs.CL cs.IR stat.ML', 'abstract': '  This paper investigates and evaluates support vector machine active learning\\nalgorithms for use with imbalanced datasets, which commonly arise in many\\napplications such as information extraction applications. Algorithms based on\\nclosest-to-hyperplane selection and query-by-committee selection are combined\\nwith methods for addressing imbalance such as positive amplification based on\\nprevalence statistics from initial random samples. Three algorithms (ClosestPA,\\nQBagPA, and QBoostPA) are presented and carefully evaluated on datasets for\\ntext classification and relation extraction. The ClosestPA algorithm is shown\\nto consistently outperform the other two in a variety of ways and insights are\\nprovided as to why this is the case.\\n', 'publish_date': 'Wed, 24 Jan 2018 06:38:06 GMT'}\n",
    "ID: 2303.07203, Distance: 0.48456257581710815, Content: {'title': 'On the Robustness of Text Vectorizers', 'doi': None, 'categories': 'cs.CL cs.LG stat.ML', 'abstract': '  A fundamental issue in machine learning is the robustness of the model with\\nrespect to changes in the input. In natural language processing, models\\ntypically contain a first embedding layer, transforming a sequence of tokens\\ninto vector representations. While the robustness with respect to changes of\\ncontinuous inputs is well-understood, the situation is less clear when\\nconsidering discrete changes, for instance replacing a word by another in an\\ninput sentence. Our work formally proves that popular embedding schemes, such\\nas concatenation, TF-IDF, and Paragraph Vector (a.k.a. doc2vec), exhibit\\nrobustness in the H\\\\\"older or Lipschitz sense with respect to the Hamming\\ndistance. We provide quantitative bounds for these schemes and demonstrate how\\nthe constants involved are affected by the length of the document. These\\nfindings are exemplified through a series of numerical examples.\\n', 'publish_date': 'Thu, 9 Mar 2023 16:37:37 GMT'}\n",
    "\n",
    "Answer:\n",
    "\n",
    "==================================================\n",
    "生成嵌入中: 100%|██████████| 1/1 [00:00<00:00, 12.22it/s]\n",
    "[INFO] Query vector shape: (1, 1, 768)\n",
    "\n",
    "\n",
    "cost 0.1482 seconds\n",
    "\n",
    "\n",
    "You are a professional academic assistant specializing in the cs.CL domain. Strictly based on the provided literature information, answer the user's query. You may slightly expand and refine the response using existing knowledge, but avoid fabricating information. The response language should match the user's query language (English for English queries, Chinese for Chinese queries). Ensure the response is concise and limited to 500 words. Include a citation with the corresponding literature link (like [text](f\"https://arxiv.org/abs/{{id}}\")) and adjust the response style based on the temperature parameter (from 0.0 to 1.0). If the temperature is low (e.g., 0.2), provide a strict and precise answer. If the temperature is high (e.g., 0.8), provide a more creative and exploratory answer.\n",
    "\n",
    "Temperature: 0.2\n",
    "Question: 如何设计一种能够准确识别文本中隐喻表达的模型，并将其应用于情感分析或文本理解？\n",
    "Relevant Literature: \n",
    "ID: 2012.04540, Distance: 0.29042288661003113, Content: {'title': 'Improvements and Extensions on Metaphor Detection', 'doi': '10.18653/v1/2021.unimplicit-1.5', 'categories': 'cs.CL cs.LG', 'abstract': '  Metaphors are ubiquitous in human language. The metaphor detection task (MD)\\naims at detecting and interpreting metaphors from written language, which is\\ncrucial in natural language understanding (NLU) research. In this paper, we\\nintroduce a pre-trained Transformer-based model into MD. Our model outperforms\\nthe previous state-of-the-art models by large margins in our evaluations, with\\nrelative improvements on the F-1 score from 5.33% to 28.39%. Second, we extend\\nMD to a classification task about the metaphoricity of an entire piece of text\\nto make MD applicable in more general NLU scenes. Finally, we clean up the\\nimproper or outdated annotations in one of the MD benchmark datasets and\\nre-benchmark it with our Transformer-based model. This approach could be\\napplied to other existing MD datasets as well, since the metaphoricity\\nannotations in these benchmark datasets may be outdated. Future research\\nefforts are also necessary to build an up-to-date and well-annotated dataset\\nconsisting of longer and more complex texts.\\n', 'publish_date': 'Mon, 7 Dec 2020 08:17:42 GMT'}\n",
    "ID: 2305.17268, Distance: 0.3013506829738617, Content: {'title': 'Metaphor Detection via Explicit Basic Meanings Modelling', 'doi': None, 'categories': 'cs.CL', 'abstract': '  One noticeable trend in metaphor detection is the embrace of linguistic\\ntheories such as the metaphor identification procedure (MIP) for model\\narchitecture design. While MIP clearly defines that the metaphoricity of a\\nlexical unit is determined based on the contrast between its \\\\textit{contextual\\nmeaning} and its \\\\textit{basic meaning}, existing work does not strictly follow\\nthis principle, typically using the \\\\textit{aggregated meaning} to approximate\\nthe basic meaning of target words. In this paper, we propose a novel metaphor\\ndetection method, which models the basic meaning of the word based on literal\\nannotation from the training set, and then compares this with the contextual\\nmeaning in a target sentence to identify metaphors. Empirical results show that\\nour method outperforms the state-of-the-art method significantly by 1.0\\\\% in F1\\nscore. Moreover, our performance even reaches the theoretical upper bound on\\nthe VUA18 benchmark for targets with basic annotations, which demonstrates the\\nimportance of modelling basic meanings for metaphor detection.\\n', 'publish_date': 'Fri, 26 May 2023 21:25:05 GMT'}\n",
    "ID: 2009.12565, Distance: 0.3061826825141907, Content: {'title': 'Metaphor Detection using Deep Contextualized Word Embeddings', 'doi': None, 'categories': 'cs.CL cs.LG', 'abstract': '  Metaphors are ubiquitous in natural language, and their detection plays an\\nessential role in many natural language processing tasks, such as language\\nunderstanding, sentiment analysis, etc. Most existing approaches for metaphor\\ndetection rely on complex, hand-crafted and fine-tuned feature pipelines, which\\ngreatly limit their applicability. In this work, we present an end-to-end\\nmethod composed of deep contextualized word embeddings, bidirectional LSTMs and\\nmulti-head attention mechanism to address the task of automatic metaphor\\ndetection. Our method, unlike many other existing approaches, requires only the\\nraw text sequences as input features to detect the metaphoricity of a phrase.\\nWe compare the performance of our method against the existing baselines on two\\nbenchmark datasets, TroFi, and MOH-X respectively. Experimental evaluations\\nconfirm the effectiveness of our approach.\\n', 'publish_date': 'Sat, 26 Sep 2020 11:00:35 GMT'}\n",
    "ID: 2311.00790, Distance: 0.3397301435470581, Content: {'title': 'Construction Artifacts in Metaphor Identification Datasets', 'doi': None, 'categories': 'cs.CL', 'abstract': '  Metaphor identification aims at understanding whether a given expression is\\nused figuratively in context. However, in this paper we show how existing\\nmetaphor identification datasets can be gamed by fully ignoring the potential\\nmetaphorical expression or the context in which it occurs. We test this\\nhypothesis in a variety of datasets and settings, and show that metaphor\\nidentification systems based on language models without complete information\\ncan be competitive with those using the full context. This is due to the\\nconstruction procedures to build such datasets, which introduce unwanted biases\\nfor positive and negative classes. Finally, we test the same hypothesis on\\ndatasets that are carefully sampled from natural corpora and where this bias is\\nnot present, making these datasets more challenging and reliable.\\n', 'publish_date': 'Wed, 1 Nov 2023 19:21:55 GMT'}\n",
    "ID: 1808.09653, Distance: 0.34305039048194885, Content: {'title': 'Neural Metaphor Detection in Context', 'doi': None, 'categories': 'cs.CL', 'abstract': '  We present end-to-end neural models for detecting metaphorical word use in\\ncontext. We show that relatively standard BiLSTM models which operate on\\ncomplete sentences work well in this setting, in comparison to previous work\\nthat used more restricted forms of linguistic context. These models establish a\\nnew state-of-the-art on existing verb metaphor detection benchmarks, and show\\nstrong performance on jointly predicting the metaphoricity of all words in a\\nrunning text.\\n', 'publish_date': 'Wed, 29 Aug 2018 06:32:47 GMT'}\n",
    "\n",
    "Answer:\n",
    "\n",
    "==================================================\n",
    "生成嵌入中: 100%|██████████| 1/1 [00:00<00:00, 10.02it/s]\n",
    "[INFO] Query vector shape: (1, 1, 768)\n",
    "\n",
    "\n",
    "\n",
    "cost 0.1712 seconds\n",
    "\n",
    "\n",
    "You are a professional academic assistant specializing in the cs.CL domain. Strictly based on the provided literature information, answer the user's query. You may slightly expand and refine the response using existing knowledge, but avoid fabricating information. The response language should match the user's query language (English for English queries, Chinese for Chinese queries). Ensure the response is concise and limited to 500 words. Include a citation with the corresponding literature link (like [text](f\"https://arxiv.org/abs/{{id}}\")) and adjust the response style based on the temperature parameter (from 0.0 to 1.0). If the temperature is low (e.g., 0.2), provide a strict and precise answer. If the temperature is high (e.g., 0.8), provide a more creative and exploratory answer.\n",
    "\n",
    "Temperature: 0.2\n",
    "Question: 针对低资源语言，如何利用迁移学习来提高自然语言处理任务的性能？\n",
    "Relevant Literature: \n",
    "ID: 2208.09180, Distance: 0.3016367256641388, Content: {'title': 'Effective Transfer Learning for Low-Resource Natural Language\\n  Understanding', 'doi': None, 'categories': 'cs.CL cs.AI', 'abstract': \"  Natural language understanding (NLU) is the task of semantic decoding of\\nhuman languages by machines. NLU models rely heavily on large training data to\\nensure good performance. However, substantial languages and domains have very\\nfew data resources and domain experts. It is necessary to overcome the data\\nscarcity challenge, when very few or even zero training samples are available.\\nIn this thesis, we focus on developing cross-lingual and cross-domain methods\\nto tackle the low-resource issues. First, we propose to improve the model's\\ncross-lingual ability by focusing on the task-related keywords, enhancing the\\nmodel's robustness and regularizing the representations. We find that the\\nrepresentations for low-resource languages can be easily and greatly improved\\nby focusing on just the keywords. Second, we present Order-Reduced Modeling\\nmethods for the cross-lingual adaptation, and find that modeling partial word\\norders instead of the whole sequence can improve the robustness of the model\\nagainst word order differences between languages and task knowledge transfer to\\nlow-resource languages. Third, we propose to leverage different levels of\\ndomain-related corpora and additional masking of data in the pre-training for\\nthe cross-domain adaptation, and discover that more challenging pre-training\\ncan better address the domain discrepancy issue in the task knowledge transfer.\\nFinally, we introduce a coarse-to-fine framework, Coach, and a cross-lingual\\nand cross-domain parsing framework, X2Parser. Coach decomposes the\\nrepresentation learning process into a coarse-grained and a fine-grained\\nfeature learning, and X2Parser simplifies the hierarchical task structures into\\nflattened ones. We observe that simplifying task structures makes the\\nrepresentation learning more effective for low-resource languages and domains.\\n\", 'publish_date': 'Fri, 19 Aug 2022 06:59:00 GMT'}\n",
    "ID: 2010.03179, Distance: 0.32798051834106445, Content: {'title': 'Transfer Learning and Distant Supervision for Multilingual Transformer\\n  Models: A Study on African Languages', 'doi': None, 'categories': 'cs.CL cs.LG', 'abstract': \"  Multilingual transformer models like mBERT and XLM-RoBERTa have obtained\\ngreat improvements for many NLP tasks on a variety of languages. However,\\nrecent works also showed that results from high-resource languages could not be\\neasily transferred to realistic, low-resource scenarios. In this work, we study\\ntrends in performance for different amounts of available resources for the\\nthree African languages Hausa, isiXhosa and Yor\\\\`ub\\\\'a on both NER and topic\\nclassification. We show that in combination with transfer learning or distant\\nsupervision, these models can achieve with as little as 10 or 100 labeled\\nsentences the same performance as baselines with much more supervised training\\ndata. However, we also find settings where this does not hold. Our discussions\\nand additional experiments on assumptions such as time and hardware\\nrestrictions highlight challenges and opportunities in low-resource learning.\\n\", 'publish_date': 'Wed, 7 Oct 2020 05:23:27 GMT'}\n",
    "ID: 2309.05311, Distance: 0.3384498953819275, Content: {'title': 'Analysing Cross-Lingual Transfer in Low-Resourced African Named Entity\\n  Recognition', 'doi': None, 'categories': 'cs.CL', 'abstract': '  Transfer learning has led to large gains in performance for nearly all NLP\\ntasks while making downstream models easier and faster to train. This has also\\nbeen extended to low-resourced languages, with some success. We investigate the\\nproperties of cross-lingual transfer learning between ten low-resourced\\nlanguages, from the perspective of a named entity recognition task. We\\nspecifically investigate how much adaptive fine-tuning and the choice of\\ntransfer language affect zero-shot transfer performance. We find that models\\nthat perform well on a single language often do so at the expense of\\ngeneralising to others, while models with the best generalisation to other\\nlanguages suffer in individual language performance. Furthermore, the amount of\\ndata overlap between the source and target datasets is a better predictor of\\ntransfer performance than either the geographical or genetic distance between\\nthe languages.\\n', 'publish_date': 'Mon, 11 Sep 2023 08:56:47 GMT'}\n",
    "ID: 2211.05015, Distance: 0.33989882469177246, Content: {'title': 'Detecting Languages Unintelligible to Multilingual Models through Local\\n  Structure Probes', 'doi': None, 'categories': 'cs.CL', 'abstract': '  Providing better language tools for low-resource and endangered languages is\\nimperative for equitable growth. Recent progress with massively multilingual\\npretrained models has proven surprisingly effective at performing zero-shot\\ntransfer to a wide variety of languages. However, this transfer is not\\nuniversal, with many languages not currently understood by multilingual\\napproaches. It is estimated that only 72 languages possess a \"small set of\\nlabeled datasets\" on which we could test a model\\'s performance, the vast\\nmajority of languages not having the resources available to simply evaluate\\nperformances on. In this work, we attempt to clarify which languages do and do\\nnot currently benefit from such transfer. To that end, we develop a general\\napproach that requires only unlabelled text to detect which languages are not\\nwell understood by a cross-lingual model. Our approach is derived from the\\nhypothesis that if a model\\'s understanding is insensitive to perturbations to\\ntext in a language, it is likely to have a limited understanding of that\\nlanguage. We construct a cross-lingual sentence similarity task to evaluate our\\napproach empirically on 350, primarily low-resource, languages.\\n', 'publish_date': 'Wed, 9 Nov 2022 16:45:16 GMT'}\n",
    "ID: 2311.05741, Distance: 0.3460055887699127, Content: {'title': 'Efficiently Adapting Pretrained Language Models To New Languages', 'doi': None, 'categories': 'cs.CL cs.AI cs.LG', 'abstract': '  Recent large language models (LLM) exhibit sub-optimal performance on\\nlow-resource languages, as the training data of these models is usually\\ndominated by English and other high-resource languages. Furthermore, it is\\nchallenging to train models for low-resource languages, especially from\\nscratch, due to a lack of high quality training data. Adapting pretrained LLMs\\nreduces the need for data in the new language while also providing cross\\nlingual transfer capabilities. However, naively adapting to new languages leads\\nto catastrophic forgetting and poor tokenizer efficiency. In this work, we\\nstudy how to efficiently adapt any existing pretrained LLM to a new language\\nwithout running into these issues. In particular, we improve the encoding\\nefficiency of the tokenizer by adding new tokens from the target language and\\nstudy the data mixing recipe to mitigate forgetting. Our experiments on\\nadapting an English LLM to Hungarian and Thai show that our recipe can reach\\nbetter performance than open source models on the target language, with minimal\\nregressions on English.\\n', 'publish_date': 'Thu, 9 Nov 2023 20:59:08 GMT'}\n",
    "\n",
    "Answer:\n",
    "\n",
    "==================================================\n",
    "生成嵌入中: 100%|██████████| 1/1 [00:00<00:00, 11.87it/s]\n",
    "[INFO] Query vector shape: (1, 1, 768)\n",
    "\n",
    "\n",
    "cost 0.1498 seconds\n",
    "\n",
    "\n",
    "You are a professional academic assistant specializing in the cs.CL domain. Strictly based on the provided literature information, answer the user's query. You may slightly expand and refine the response using existing knowledge, but avoid fabricating information. The response language should match the user's query language (English for English queries, Chinese for Chinese queries). Ensure the response is concise and limited to 500 words. Include a citation with the corresponding literature link (like [text](f\"https://arxiv.org/abs/{{id}}\")) and adjust the response style based on the temperature parameter (from 0.0 to 1.0). If the temperature is low (e.g., 0.2), provide a strict and precise answer. If the temperature is high (e.g., 0.8), provide a more creative and exploratory answer.\n",
    "\n",
    "Temperature: 0.2\n",
    "Question: 在自然语言处理中，如何处理文本数据中的歧义问题？\n",
    "Relevant Literature: \n",
    "ID: cmp-lg/9406034, Distance: 0.38253289461135864, Content: {'title': 'Decision Lists for Lexical Ambiguity Resolution: Application to Accent\\n  Restoration in Spanish and French', 'doi': None, 'categories': 'cmp-lg cs.CL', 'abstract': '  This paper presents a statistical decision procedure for lexical ambiguity\\nresolution. The algorithm exploits both local syntactic patterns and more\\ndistant collocational evidence, generating an efficient, effective, and highly\\nperspicuous recipe for resolving a given ambiguity. By identifying and\\nutilizing only the single best disambiguating evidence in a target context, the\\nalgorithm avoids the problematic complex modeling of statistical dependencies.\\nAlthough directly applicable to a wide class of ambiguities, the algorithm is\\ndescribed and evaluated in a realistic case study, the problem of restoring\\nmissing accents in Spanish and French text.\\n', 'publish_date': 'Thu, 23 Jun 1994 03:34:55 GMT'}\n",
    "ID: cmp-lg/9605030, Distance: 0.4226163625717163, Content: {'title': 'Incremental Centering and Center Ambiguity', 'doi': None, 'categories': 'cmp-lg cs.CL', 'abstract': '  In this paper, we present a model of anaphor resolution within the framework\\nof the centering model. The consideration of an incremental processing mode\\nintroduces the need to manage structural ambiguity at the center level. Hence,\\nthe centering framework is further refined to account for local and global\\nparsing ambiguities which propagate up to the level of center representations,\\nyielding moderately adapted data structures for the centering algorithm.\\n', 'publish_date': 'Thu, 16 May 1996 08:10:27 GMT'}\n",
    "ID: cmp-lg/9502033, Distance: 0.4263451099395752, Content: {'title': 'An Algorithm to Co-Ordinate Anaphora Resolution and PPS Disambiguation\\n  Process', 'doi': None, 'categories': 'cmp-lg cs.CL', 'abstract': '  This paper concerns both anaphora resolution and prepositional phrase (PP)\\nattachment that are the most frequent ambiguities in natural language\\nprocessing. Several methods have been proposed to deal with each phenomenon\\nseparately, however none of proposed systems has considered the way of dealing\\nboth phenomena. We tackle this issue, proposing an algorithm to co-ordinate the\\ntreatment of these two problems efficiently, i.e., the aim is also to exploit\\nat each step all the results that each component can provide.\\n', 'publish_date': 'Fri, 24 Feb 1995 19:34:10 GMT'}\n",
    "ID: 2109.10013, Distance: 0.43988338112831116, Content: {'title': 'Negation-Instance Based Evaluation of End-to-End Negation Resolution', 'doi': None, 'categories': 'cs.CL', 'abstract': '  In this paper, we revisit the task of negation resolution, which includes the\\nsubtasks of cue detection (e.g. \"not\", \"never\") and scope resolution. In the\\ncontext of previous shared tasks, a variety of evaluation metrics have been\\nproposed. Subsequent works usually use different subsets of these, including\\nvariations and custom implementations, rendering meaningful comparisons between\\nsystems difficult. Examining the problem both from a linguistic perspective and\\nfrom a downstream viewpoint, we here argue for a negation-instance based\\napproach to evaluating negation resolution. Our proposed metrics correspond to\\nexpectations over per-instance scores and hence are intuitively interpretable.\\nTo render research comparable and to foster future work, we provide results for\\na set of current state-of-the-art systems for negation resolution on three\\nEnglish corpora, and make our implementation of the evaluation scripts publicly\\navailable.\\n', 'publish_date': 'Tue, 21 Sep 2021 07:49:41 GMT'}\n",
    "ID: 2202.12645, Distance: 0.4459282159805298, Content: {'title': 'Exploring Multi-Modal Representations for Ambiguity Detection &\\n  Coreference Resolution in the SIMMC 2.0 Challenge', 'doi': None, 'categories': 'cs.CL cs.AI', 'abstract': \"  Anaphoric expressions, such as pronouns and referential descriptions, are\\nsituated with respect to the linguistic context of prior turns, as well as, the\\nimmediate visual environment. However, a speaker's referential descriptions do\\nnot always uniquely identify the referent, leading to ambiguities in need of\\nresolution through subsequent clarificational exchanges. Thus, effective\\nAmbiguity Detection and Coreference Resolution are key to task success in\\nConversational AI. In this paper, we present models for these two tasks as part\\nof the SIMMC 2.0 Challenge (Kottur et al. 2021). Specifically, we use TOD-BERT\\nand LXMERT based models, compare them to a number of baselines and provide\\nablation experiments. Our results show that (1) language models are able to\\nexploit correlations in the data to detect ambiguity; and (2) unimodal\\ncoreference resolution models can avoid the need for a vision component,\\nthrough the use of smart object representations.\\n\", 'publish_date': 'Fri, 25 Feb 2022 12:10:02 GMT'}\n",
    "\n",
    "Answer:\n",
    "\n",
    "==================================================\n",
    "生成嵌入中: 100%|██████████| 1/1 [00:00<00:00, 14.86it/s]\n",
    "[INFO] Query vector shape: (1, 1, 768)\n",
    "\n",
    "\n",
    "cost 0.1242 seconds\n",
    "\n",
    "\n",
    "You are a professional academic assistant specializing in the cs.CL domain. Strictly based on the provided literature information, answer the user's query. You may slightly expand and refine the response using existing knowledge, but avoid fabricating information. The response language should match the user's query language (English for English queries, Chinese for Chinese queries). Ensure the response is concise and limited to 500 words. Include a citation with the corresponding literature link (like [text](f\"https://arxiv.org/abs/{{id}}\")) and adjust the response style based on the temperature parameter (from 0.0 to 1.0). If the temperature is low (e.g., 0.2), provide a strict and precise answer. If the temperature is high (e.g., 0.8), provide a more creative and exploratory answer.\n",
    "\n",
    "Temperature: 0.2\n",
    "Question: 如何提高对古文字的识别和释读能力？\n",
    "Relevant Literature: \n",
    "ID: 1501.01894, Distance: 0.428060919046402, Content: {'title': 'Quantifying Scripts: Defining metrics of characters for quantitative and\\n  descriptive analysis', 'doi': None, 'categories': 'cs.CL', 'abstract': '  Analysis of scripts plays an important role in paleography and in\\nquantitative linguistics. Especially in the field of digital paleography\\nquantitative features are much needed to differentiate glyphs. We describe an\\nelaborate set of metrics that quantify qualitative information contained in\\ncharacters and hence indirectly also quantify the scribal features. We broadly\\ndivide the metrics into several categories and describe each individual metric\\nwith its underlying qualitative significance. The metrics are largely derived\\nfrom the related area of gesture design and recognition. We also propose\\nseveral novel metrics. The proposed metrics are soundly grounded on the\\nprinciples of handwriting production and handwriting analysis. These computed\\nmetrics could serve as descriptors for scripts and also be used for comparing\\nand analyzing scripts. We illustrate some quantitative analysis based on the\\nproposed metrics by applying it to the paleographic evolution of the medieval\\nTamil script from Brahmi. We also outline future work.\\n', 'publish_date': 'Thu, 8 Jan 2015 16:12:34 GMT'}\n",
    "ID: 1702.00523, Distance: 0.435880571603775, Content: {'title': 'Deep Learning the Indus Script', 'doi': None, 'categories': 'cs.CV cs.CL cs.LG', 'abstract': '  Standardized corpora of undeciphered scripts, a necessary starting point for\\ncomputational epigraphy, requires laborious human effort for their preparation\\nfrom raw archaeological records. Automating this process through machine\\nlearning algorithms can be of significant aid to epigraphical research. Here,\\nwe take the first steps in this direction and present a deep learning pipeline\\nthat takes as input images of the undeciphered Indus script, as found in\\narchaeological artifacts, and returns as output a string of graphemes, suitable\\nfor inclusion in a standard corpus. The image is first decomposed into regions\\nusing Selective Search and these regions are classified as containing textual\\nand/or graphical information using a convolutional neural network. Regions\\nclassified as potentially containing text are hierarchically merged and trimmed\\nto remove non-textual information. The remaining textual part of the image is\\nsegmented using standard image processing techniques to isolate individual\\ngraphemes. This set is finally passed to a second convolutional neural network\\nto classify the graphemes, based on a standard corpus. The classifier can\\nidentify the presence or absence of the most frequent Indus grapheme, the \"jar\"\\nsign, with an accuracy of 92%. Our results demonstrate the great potential of\\ndeep learning approaches in computational epigraphy and, more generally, in the\\ndigital humanities.\\n', 'publish_date': 'Thu, 2 Feb 2017 01:56:22 GMT'}\n",
    "ID: 1608.02153, Distance: 0.4391481876373291, Content: {'title': 'OCR of historical printings with an application to building diachronic\\n  corpora: A case study using the RIDGES herbal corpus', 'doi': None, 'categories': 'cs.CL cs.DL', 'abstract': '  This article describes the results of a case study that applies Neural\\nNetwork-based Optical Character Recognition (OCR) to scanned images of books\\nprinted between 1487 and 1870 by training the OCR engine OCRopus\\n[@breuel2013high] on the RIDGES herbal text corpus [@OdebrechtEtAlSubmitted].\\nTraining specific OCR models was possible because the necessary *ground truth*\\nis available as error-corrected diplomatic transcriptions. The OCR results have\\nbeen evaluated for accuracy against the ground truth of unseen test sets.\\nCharacter and word accuracies (percentage of correctly recognized items) for\\nthe resulting machine-readable texts of individual documents range from 94% to\\nmore than 99% (character level) and from 76% to 97% (word level). This includes\\nthe earliest printed books, which were thought to be inaccessible by OCR\\nmethods until recently. Furthermore, OCR models trained on one part of the\\ncorpus consisting of books with different printing dates and different typesets\\n*(mixed models)* have been tested for their predictive power on the books from\\nthe other part containing yet other fonts, mostly yielding character accuracies\\nwell above 90%. It therefore seems possible to construct generalized models\\ntrained on a range of fonts that can be applied to a wide variety of historical\\nprintings still giving good results. A moderate postcorrection effort of some\\npages will then enable the training of individual models with even better\\naccuracies. Using this method, diachronic corpora including early printings can\\nbe constructed much faster and cheaper than by manual transcription. The OCR\\nmethods reported here open up the possibility of transforming our printed\\ntextual cultural heritage into electronic text by largely automatic means,\\nwhich is a prerequisite for the mass conversion of scanned books.\\n', 'publish_date': 'Sat, 6 Aug 2016 20:51:53 GMT'}\n",
    "ID: 1509.01978, Distance: 0.439435213804245, Content: {'title': 'An Approach to the Analysis of the South Slavic Medieval Labels Using\\n  Image Texture', 'doi': None, 'categories': 'cs.CV cs.AI cs.CL', 'abstract': '  The paper presents a new script classification method for the discrimination\\nof the South Slavic medieval labels. It consists in the textural analysis of\\nthe script types. In the first step, each letter is coded by the equivalent\\nscript type, which is defined by its typographical features. Obtained coded\\ntext is subjected to the run-length statistical analysis and to the adjacent\\nlocal binary pattern analysis in order to extract the features. The result\\nshows a diversity between the extracted features of the scripts, which makes\\nthe feature classification more effective. It is the basis for the\\nclassification process of the script identification by using an extension of a\\nstate-of-the-art approach for document clustering. The proposed method is\\nevaluated on an example of hand-engraved in stone and hand-printed in paper\\nlabels in old Cyrillic, angular and round Glagolitic. Experiments demonstrate\\nvery positive results, which prove the effectiveness of the proposed method.\\n', 'publish_date': 'Mon, 7 Sep 2015 10:39:20 GMT'}\n",
    "ID: 1507.04908, Distance: 0.4463350772857666, Content: {'title': 'Analysis of the South Slavic Scripts by Run-Length Features of the Image\\n  Texture', 'doi': '10.5755/j01.eee.21.4.12785', 'categories': 'cs.CV cs.CL', 'abstract': '  The paper proposes an algorithm for the script recognition based on the\\ntexture characteristics. The image texture is achieved by coding each letter\\nwith the equivalent script type (number code) according to its position in the\\ntext line. Each code is transformed into equivalent gray level pixel creating\\nan 1-D image. Then, the image texture is subjected to the run-length analysis.\\nThis analysis extracts the run-length features, which are classified to make a\\ndistinction between the scripts under consideration. In the experiment, a\\ncustom oriented database is subject to the proposed algorithm. The database\\nconsists of some text documents written in Cyrillic, Latin and Glagolitic\\nscripts. Furthermore, it is divided into training and test parts. The results\\nof the experiment show that 3 out of 5 run-length features can be used for\\neffective differentiation between the analyzed South Slavic scripts.\\n', 'publish_date': 'Fri, 17 Jul 2015 10:34:23 GMT'}\n",
    "\n",
    "Answer:\n",
    "\n",
    "==================================================\n",
    "生成嵌入中: 100%|██████████| 1/1 [00:00<00:00, 12.83it/s]\n",
    "[INFO] Query vector shape: (1, 1, 768)\n",
    "\n",
    "\n",
    "cost 0.1414 seconds\n",
    "\n",
    "\n",
    "You are a professional academic assistant specializing in the cs.CL domain. Strictly based on the provided literature information, answer the user's query. You may slightly expand and refine the response using existing knowledge, but avoid fabricating information. The response language should match the user's query language (English for English queries, Chinese for Chinese queries). Ensure the response is concise and limited to 500 words. Include a citation with the corresponding literature link (like [text](f\"https://arxiv.org/abs/{{id}}\")) and adjust the response style based on the temperature parameter (from 0.0 to 1.0). If the temperature is low (e.g., 0.2), provide a strict and precise answer. If the temperature is high (e.g., 0.8), provide a more creative and exploratory answer.\n",
    "\n",
    "Temperature: 0.2\n",
    "Question: 在文本风格迁移任务中，如何准确地将文本从一种风格转换为另一种风格？\n",
    "Relevant Literature: \n",
    "ID: 2407.14822, Distance: 0.4025307297706604, Content: {'title': 'Text Style Transfer: An Introductory Overview', 'doi': None, 'categories': 'cs.CL', 'abstract': '  Text Style Transfer (TST) is a pivotal task in natural language generation to\\nmanipulate text style attributes while preserving style-independent content.\\nThe attributes targeted in TST can vary widely, including politeness,\\nauthorship, mitigation of offensive language, modification of feelings, and\\nadjustment of text formality. TST has become a widely researched topic with\\nsubstantial advancements in recent years. This paper provides an introductory\\noverview of TST, addressing its challenges, existing approaches, datasets,\\nevaluation measures, subtasks, and applications. This fundamental overview\\nimproves understanding of the background and fundamentals of text style\\ntransfer.\\n', 'publish_date': 'Sat, 20 Jul 2024 09:54:55 GMT'}\n",
    "ID: 2010.12742, Distance: 0.40275296568870544, Content: {'title': 'Text Style Transfer: A Review and Experimental Evaluation', 'doi': None, 'categories': 'cs.CL', 'abstract': '  The stylistic properties of text have intrigued computational linguistics\\nresearchers in recent years. Specifically, researchers have investigated the\\nText Style Transfer (TST) task, which aims to change the stylistic properties\\nof the text while retaining its style independent content. Over the last few\\nyears, many novel TST algorithms have been developed, while the industry has\\nleveraged these algorithms to enable exciting TST applications. The field of\\nTST research has burgeoned because of this symbiosis. This article aims to\\nprovide a comprehensive review of recent research efforts on text style\\ntransfer. More concretely, we create a taxonomy to organize the TST models and\\nprovide a comprehensive summary of the state of the art. We review the existing\\nevaluation methodologies for TST tasks and conduct a large-scale\\nreproducibility study where we experimentally benchmark 19 state-of-the-art TST\\nalgorithms on two publicly available datasets. Finally, we expand on current\\ntrends and provide new perspectives on the new and exciting developments in the\\nTST field.\\n', 'publish_date': 'Sat, 24 Oct 2020 02:02:58 GMT'}\n",
    "ID: 2005.00136, Distance: 0.41681936383247375, Content: {'title': 'Contextual Text Style Transfer', 'doi': None, 'categories': 'cs.CL cs.LG', 'abstract': '  We introduce a new task, Contextual Text Style Transfer - translating a\\nsentence into a desired style with its surrounding context taken into account.\\nThis brings two key challenges to existing style transfer approaches: ($i$) how\\nto preserve the semantic meaning of target sentence and its consistency with\\nsurrounding context during transfer; ($ii$) how to train a robust model with\\nlimited labeled data accompanied with context. To realize high-quality style\\ntransfer with natural context preservation, we propose a Context-Aware Style\\nTransfer (CAST) model, which uses two separate encoders for each input sentence\\nand its surrounding context. A classifier is further trained to ensure\\ncontextual consistency of the generated sentence. To compensate for the lack of\\nparallel data, additional self-reconstruction and back-translation losses are\\nintroduced to leverage non-parallel data in a semi-supervised fashion. Two new\\nbenchmarks, Enron-Context and Reddit-Context, are introduced for formality and\\noffensiveness style transfer. Experimental results on these datasets\\ndemonstrate the effectiveness of the proposed CAST model over state-of-the-art\\nmethods across style accuracy, content preservation and contextual consistency\\nmetrics.\\n', 'publish_date': 'Thu, 30 Apr 2020 23:01:12 GMT'}\n",
    "ID: 2002.06525, Distance: 0.42682838439941406, Content: {'title': 'Learning to Generate Multiple Style Transfer Outputs for an Input\\n  Sentence', 'doi': None, 'categories': 'cs.CL cs.LG', 'abstract': '  Text style transfer refers to the task of rephrasing a given text in a\\ndifferent style. While various methods have been proposed to advance the state\\nof the art, they often assume the transfer output follows a delta distribution,\\nand thus their models cannot generate different style transfer results for a\\ngiven input text. To address the limitation, we propose a one-to-many text\\nstyle transfer framework. In contrast to prior works that learn a one-to-one\\nmapping that converts an input sentence to one output sentence, our approach\\nlearns a one-to-many mapping that can convert an input sentence to multiple\\ndifferent output sentences, while preserving the input content. This is\\nachieved by applying adversarial training with a latent decomposition scheme.\\nSpecifically, we decompose the latent representation of the input sentence to a\\nstyle code that captures the language style variation and a content code that\\nencodes the language style-independent content. We then combine the content\\ncode with the style code for generating a style transfer output. By combining\\nthe same content code with a different style code, we generate a different\\nstyle transfer output. Extensive experimental results with comparisons to\\nseveral text style transfer approaches on multiple public datasets using a\\ndiverse set of performance metrics validate effectiveness of the proposed\\napproach.\\n', 'publish_date': 'Sun, 16 Feb 2020 07:10:45 GMT'}\n",
    "ID: 2306.00539, Distance: 0.42831653356552124, Content: {'title': 'A Call for Standardization and Validation of Text Style Transfer\\n  Evaluation', 'doi': None, 'categories': 'cs.LG cs.CL', 'abstract': '  Text Style Transfer (TST) evaluation is, in practice, inconsistent.\\nTherefore, we conduct a meta-analysis on human and automated TST evaluation and\\nexperimentation that thoroughly examines existing literature in the field. The\\nmeta-analysis reveals a substantial standardization gap in human and automated\\nevaluation. In addition, we also find a validation gap: only few automated\\nmetrics have been validated using human experiments. To this end, we thoroughly\\nscrutinize both the standardization and validation gap and reveal the resulting\\npitfalls. This work also paves the way to close the standardization and\\nvalidation gap in TST evaluation by calling out requirements to be met by\\nfuture research.\\n', 'publish_date': 'Thu, 1 Jun 2023 10:46:08 GMT'}\n",
    "\n",
    "Answer:\n",
    "\n",
    "==================================================\n",
    "生成嵌入中: 100%|██████████| 1/1 [00:00<00:00, 17.10it/s]\n",
    "[INFO] Query vector shape: (1, 1, 768)\n",
    "\n",
    "\n",
    "cost 0.1217 seconds\n",
    "\n",
    "\n",
    "You are a professional academic assistant specializing in the cs.CL domain. Strictly based on the provided literature information, answer the user's query. You may slightly expand and refine the response using existing knowledge, but avoid fabricating information. The response language should match the user's query language (English for English queries, Chinese for Chinese queries). Ensure the response is concise and limited to 500 words. Include a citation with the corresponding literature link (like [text](f\"https://arxiv.org/abs/{{id}}\")) and adjust the response style based on the temperature parameter (from 0.0 to 1.0). If the temperature is low (e.g., 0.2), provide a strict and precise answer. If the temperature is high (e.g., 0.8), provide a more creative and exploratory answer.\n",
    "\n",
    "Temperature: 0.2\n",
    "Question: 如何利用计算语言学技术对文学作品进行风格分析？\n",
    "Relevant Literature: \n",
    "ID: 2109.00601, Distance: 0.3944563567638397, Content: {'title': 'Latin writing styles analysis with Machine Learning: New approach to old\\n  questions', 'doi': None, 'categories': 'cs.CL', 'abstract': '  In the Middle Ages texts were learned by heart and spread using oral means of\\ncommunication from generation to generation. Adaptation of the art of prose and\\npoems allowed keeping particular descriptions and compositions characteristic\\nfor many literary genres. Taking into account such a specific construction of\\nliterature composed in Latin, we can search for and indicate the probability\\npatterns of familiar sources of specific narrative texts. Consideration of\\nNatural Language Processing tools allowed us the transformation of textual\\nobjects into numerical ones and then application of machine learning algorithms\\nto extract information from the dataset. We carried out the task consisting of\\nthe practical use of those concepts and observation to create a tool for\\nanalyzing narrative texts basing on open-source databases. The tool focused on\\ncreating specific search tools resources which could enable us detailed\\nsearching throughout the text. The main objectives of the study take into\\naccount finding similarities between sentences and between documents. Next, we\\napplied machine learning algorithms on chosen texts to calculate specific\\nfeatures of them (for instance authorship or centuries) and to recognize\\nsources of anonymous texts with a certain percentage.\\n', 'publish_date': 'Wed, 1 Sep 2021 20:21:45 GMT'}\n",
    "ID: 2201.04356, Distance: 0.40893232822418213, Content: {'title': 'Computational analyses of the topics, sentiments, literariness,\\n  creativity and beauty of texts in a large Corpus of English Literature', 'doi': None, 'categories': 'cs.CL', 'abstract': \"  The Gutenberg Literary English Corpus (GLEC, Jacobs, 2018a) provides a rich\\nsource of textual data for research in digital humanities, computational\\nlinguistics or neurocognitive poetics. In this study we address differences\\namong the different literature categories in GLEC, as well as differences\\nbetween authors. We report the results of three studies providing i) topic and\\nsentiment analyses for six text categories of GLEC (i.e., children and youth,\\nessays, novels, plays, poems, stories) and its >100 authors, ii) novel measures\\nof semantic complexity as indices of the literariness, creativity and book\\nbeauty of the works in GLEC (e.g., Jane Austen's six novels), and iii) two\\nexperiments on text classification and authorship recognition using novel\\nfeatures of semantic complexity. The data on two novel measures estimating a\\ntext's literariness, intratextual variance and stepwise distance (van\\nCranenburgh et al., 2019) revealed that plays are the most literary texts in\\nGLEC, followed by poems and novels. Computation of a novel index of text\\ncreativity (Gray et al., 2016) revealed poems and plays as the most creative\\ncategories with the most creative authors all being poets (Milton, Pope, Keats,\\nByron, or Wordsworth). We also computed a novel index of perceived beauty of\\nverbal art (Kintsch, 2012) for the works in GLEC and predict that Emma is the\\ntheoretically most beautiful of Austen's novels. Finally, we demonstrate that\\nthese novel measures of semantic complexity are important features for text\\nclassification and authorship recognition with overall predictive accuracies in\\nthe range of .75 to .97. Our data pave the way for future computational and\\nempirical studies of literature or experiments in reading psychology and offer\\nmultiple baselines and benchmarks for analysing and validating other book\\ncorpora.\\n\", 'publish_date': 'Wed, 12 Jan 2022 08:16:52 GMT'}\n",
    "ID: 1901.00519, Distance: 0.41809970140457153, Content: {'title': 'Pull out all the stops: Textual analysis via punctuation sequences', 'doi': '10.1017/S0956792520000157', 'categories': 'cs.CL cs.LG physics.soc-ph', 'abstract': '  Whether enjoying the lucid prose of a favorite author or slogging through\\nsome other writer\\'s cumbersome, heavy-set prattle (full of parentheses, em\\ndashes, compound adjectives, and Oxford commas), readers will notice stylistic\\nsignatures not only in word choice and grammar, but also in punctuation itself.\\nIndeed, visual sequences of punctuation from different authors produce\\nmarvelously different (and visually striking) sequences. Punctuation is a\\nlargely overlooked stylistic feature in \"stylometry\", the quantitative analysis\\nof written text. In this paper, we examine punctuation sequences in a corpus of\\nliterary documents and ask the following questions: Are the properties of such\\nsequences a distinctive feature of different authors? Is it possible to\\ndistinguish literary genres based on their punctuation sequences? Do the\\npunctuation styles of authors evolve over time? Are we on to something\\ninteresting in trying to do stylometry without words, or are we full of sound\\nand fury (signifying nothing)?\\n', 'publish_date': 'Mon, 31 Dec 2018 18:48:20 GMT'}\n",
    "ID: 1501.00841, Distance: 0.41813206672668457, Content: {'title': 'Chasing the Ghosts of Ibsen: A computational stylistic analysis of drama\\n  in translation', 'doi': None, 'categories': 'cs.CL', 'abstract': \"  Research into the stylistic properties of translations is an issue which has\\nreceived some attention in computational stylistics. Previous work by Rybicki\\n(2006) on the distinguishing of character idiolects in the work of Polish\\nauthor Henryk Sienkiewicz and two corresponding English translations using\\nBurrow's Delta method concluded that idiolectal differences could be observed\\nin the source texts and this variation was preserved to a large degree in both\\ntranslations. This study also found that the two translations were also highly\\ndistinguishable from one another. Burrows (2002) examined English translations\\nof Juvenal also using the Delta method, results of this work suggest that some\\ntranslators are more adept at concealing their own style when translating the\\nworks of another author whereas other authors tend to imprint their own style\\nto a greater extent on the work they translate. Our work examines the writing\\nof a single author, Norwegian playwright Henrik Ibsen, and these writings\\ntranslated into both German and English from Norwegian, in an attempt to\\ninvestigate the preservation of characterization, defined here as the\\ndistinctiveness of textual contributions of characters.\\n\", 'publish_date': 'Mon, 5 Jan 2015 12:55:03 GMT'}\n",
    "ID: 1511.03053, Distance: 0.42559486627578735, Content: {'title': 'Investigating the stylistic relevance of adjective and verb simile\\n  markers', 'doi': None, 'categories': 'cs.CL', 'abstract': '  Similes play an important role in literary texts not only as rhetorical\\ndevices and as figures of speech but also because of their evocative power,\\ntheir aptness for description and the relative ease with which they can be\\ncombined with other figures of speech (Israel et al. 2004). Detecting all types\\nof simile constructions in a particular text therefore seems crucial when\\nanalysing the style of an author. Few research studies however have been\\ndedicated to the study of less prominent simile markers in fictional prose and\\ntheir relevance for stylistic studies. The present paper studies the frequency\\nof adjective and verb simile markers in a corpus of British and French novels\\nin order to determine which ones are really informative and worth including in\\na stylistic analysis. Furthermore, are those adjectives and verb simile markers\\nused differently in both languages?\\n', 'publish_date': 'Tue, 10 Nov 2015 10:33:47 GMT'}\n",
    "\n",
    "Answer:\n",
    "\n",
    "==================================================\n",
    "生成嵌入中: 100%|██████████| 1/1 [00:00<00:00, 14.86it/s]\n",
    "[INFO] Query vector shape: (1, 1, 768)\n",
    "\n",
    "\n",
    "cost 0.1282 seconds\n",
    "\n",
    "\n",
    "You are a professional academic assistant specializing in the cs.CL domain. Strictly based on the provided literature information, answer the user's query. You may slightly expand and refine the response using existing knowledge, but avoid fabricating information. The response language should match the user's query language (English for English queries, Chinese for Chinese queries). Ensure the response is concise and limited to 500 words. Include a citation with the corresponding literature link (like [text](f\"https://arxiv.org/abs/{{id}}\")) and adjust the response style based on the temperature parameter (from 0.0 to 1.0). If the temperature is low (e.g., 0.2), provide a strict and precise answer. If the temperature is high (e.g., 0.8), provide a more creative and exploratory answer.\n",
    "\n",
    "Temperature: 0.2\n",
    "Question: 如何设计一种能够准确识别文本中事件的模型？\n",
    "Relevant Literature: \n",
    "ID: 2006.10093, Distance: 0.3916696310043335, Content: {'title': 'Extensively Matching for Few-shot Learning Event Detection', 'doi': None, 'categories': 'cs.CL', 'abstract': '  Current event detection models under super-vised learning settings fail to\\ntransfer to newevent types. Few-shot learning has not beenexplored in event\\ndetection even though it al-lows a model to perform well with high\\ngener-alization on new event types. In this work, weformulate event detection\\nas a few-shot learn-ing problem to enable to extend event detec-tion to new\\nevent types. We propose two novelloss factors that matching examples in the\\nsup-port set to provide more training signals to themodel. Moreover, these\\ntraining signals can beapplied in many metric-based few-shot learn-ing models.\\nOur extensive experiments on theACE-2005 dataset (under a few-shot\\nlearningsetting) show that the proposed method can im-prove the performance of\\nfew-shot learning\\n', 'publish_date': 'Wed, 17 Jun 2020 18:30:30 GMT'}\n",
    "ID: 2002.05295, Distance: 0.3989177346229553, Content: {'title': 'Exploiting the Matching Information in the Support Set for Few Shot\\n  Event Classification', 'doi': '10.1007/978-3-030-47436-2_18', 'categories': 'cs.CL cs.LG stat.ML', 'abstract': '  The existing event classification (EC) work primarily focuseson the\\ntraditional supervised learning setting in which models are unableto extract\\nevent mentions of new/unseen event types. Few-shot learninghas not been\\ninvestigated in this area although it enables EC models toextend their\\noperation to unobserved event types. To fill in this gap, inthis work, we\\ninvestigate event classification under the few-shot learningsetting. We propose\\na novel training method for this problem that exten-sively exploit the support\\nset during the training process of a few-shotlearning model. In particular, in\\naddition to matching the query exam-ple with those in the support set for\\ntraining, we seek to further matchthe examples within the support set\\nthemselves. This method providesmore training signals for the models and can be\\napplied to every metric-learning-based few-shot learning methods. Our extensive\\nexperiments ontwo benchmark EC datasets show that the proposed method can\\nimprovethe best reported few-shot learning models by up to 10% on accuracyfor\\nevent classification\\n', 'publish_date': 'Thu, 13 Feb 2020 00:40:36 GMT'}\n",
    "ID: 1910.11368, Distance: 0.4219037890434265, Content: {'title': 'Extending Event Detection to New Types with Learning from Keywords', 'doi': None, 'categories': 'cs.LG cs.CL stat.ML', 'abstract': '  Traditional event detection classifies a word or a phrase in a given sentence\\nfor a set of predefined event types. The limitation of such predefined set is\\nthat it prevents the adaptation of the event detection models to new event\\ntypes. We study a novel formulation of event detection that describes types via\\nseveral keywords to match the contexts in documents. This facilitates the\\noperation of the models to new types. We introduce a novel feature-based\\nattention mechanism for convolutional neural networks for event detection in\\nthe new formulation. Our extensive experiments demonstrate the benefits of the\\nnew formulation for new type extension for event detection as well as the\\nproposed attention mechanism for this problem.\\n', 'publish_date': 'Thu, 24 Oct 2019 18:20:48 GMT'}\n",
    "ID: 1808.08504, Distance: 0.4275459349155426, Content: {'title': 'Event Detection with Neural Networks: A Rigorous Empirical Evaluation', 'doi': None, 'categories': 'cs.CL', 'abstract': '  Detecting events and classifying them into predefined types is an important\\nstep in knowledge extraction from natural language texts. While the neural\\nnetwork models have generally led the state-of-the-art, the differences in\\nperformance between different architectures have not been rigorously studied.\\nIn this paper we present a novel GRU-based model that combines syntactic\\ninformation along with temporal structure through an attention mechanism. We\\nshow that it is competitive with other neural network architectures through\\nempirical evaluations under different random initializations and\\ntraining-validation-test splits of ACE2005 dataset.\\n', 'publish_date': 'Sun, 26 Aug 2018 04:04:39 GMT'}\n",
    "ID: 2209.01979, Distance: 0.43190476298332214, Content: {'title': 'Few-shot Incremental Event Detection', 'doi': None, 'categories': 'cs.CL', 'abstract': '  Event detection tasks can enable the quick detection of events from texts and\\nprovide powerful support for downstream natural language processing tasks. Most\\nsuch methods can only detect a fixed set of predefined event classes. To extend\\nthem to detect a new class without losing the ability to detect old classes\\nrequires costly retraining of the model from scratch. Incremental learning can\\neffectively solve this problem, but it requires abundant data of new classes.\\nIn practice, however, the lack of high-quality labeled data of new event\\nclasses makes it difficult to obtain enough data for model training. To address\\nthe above mentioned issues, we define a new task, few-shot incremental event\\ndetection, which focuses on learning to detect a new event class with limited\\ndata, while retaining the ability to detect old classes to the extent possible.\\nWe created a benchmark dataset IFSED for the few-shot incremental event\\ndetection task based on FewEvent and propose two benchmarks, IFSED-K and\\nIFSED-KP. Experimental results show that our approach has a higher F1-score\\nthan baseline methods and is more stable.\\n', 'publish_date': 'Mon, 5 Sep 2022 14:21:26 GMT'}\n",
    "\n",
    "Answer:\n",
    "\n",
    "==================================================\n",
    "生成嵌入中: 100%|██████████| 1/1 [00:00<00:00,  9.58it/s]\n",
    "[INFO] Query vector shape: (1, 1, 768)\n",
    "\n",
    "\n",
    "cost 0.1688 seconds\n",
    "\n",
    "\n",
    "You are a professional academic assistant specializing in the cs.CL domain. Strictly based on the provided literature information, answer the user's query. You may slightly expand and refine the response using existing knowledge, but avoid fabricating information. The response language should match the user's query language (English for English queries, Chinese for Chinese queries). Ensure the response is concise and limited to 500 words. Include a citation with the corresponding literature link (like [text](f\"https://arxiv.org/abs/{{id}}\")) and adjust the response style based on the temperature parameter (from 0.0 to 1.0). If the temperature is low (e.g., 0.2), provide a strict and precise answer. If the temperature is high (e.g., 0.8), provide a more creative and exploratory answer.\n",
    "\n",
    "Temperature: 0.2\n",
    "Question: 如何通过计算语言学方法挖掘文学作品中的叙事结构和情节发展？\n",
    "Relevant Literature: \n",
    "ID: 1902.01109, Distance: 0.3887389302253723, Content: {'title': 'Strategies for Structuring Story Generation', 'doi': None, 'categories': 'cs.CL', 'abstract': '  Writers generally rely on plans or sketches to write long stories, but most\\ncurrent language models generate word by word from left to right. We explore\\ncoarse-to-fine models for creating narrative texts of several hundred words,\\nand introduce new models which decompose stories by abstracting over actions\\nand entities. The model first generates the predicate-argument structure of the\\ntext, where different mentions of the same entity are marked with placeholder\\ntokens. It then generates a surface realization of the predicate-argument\\nstructure, and finally replaces the entity placeholders with context-sensitive\\nnames and references. Human judges prefer the stories from our models to a wide\\nrange of previous approaches to hierarchical text generation. Extensive\\nanalysis shows that our methods can help improve the diversity and coherence of\\nevents and entities in generated stories.\\n', 'publish_date': 'Mon, 4 Feb 2019 10:23:39 GMT'}\n",
    "ID: 2103.12872, Distance: 0.39508679509162903, Content: {'title': 'Towards a Formal Model of Narratives', 'doi': None, 'categories': 'cs.CL cs.AI cs.LO', 'abstract': \"  In this paper, we propose the beginnings of a formal framework for modeling\\nnarrative \\\\textit{qua} narrative. Our framework affords the ability to discuss\\nkey qualities of stories and their communication, including the flow of\\ninformation from a Narrator to a Reader, the evolution of a Reader's story\\nmodel over time, and Reader uncertainty. We demonstrate its applicability to\\ncomputational narratology by giving explicit algorithms for measuring the\\naccuracy with which information was conveyed to the Reader and two novel\\nmeasurements of story coherence.\\n\", 'publish_date': 'Tue, 23 Mar 2021 22:33:23 GMT'}\n",
    "ID: 1604.03029, Distance: 0.3978700339794159, Content: {'title': 'Mapping Out Narrative Structures and Dynamics Using Networks and Textual\\n  Information', 'doi': '10.1371/journal.pone.0226025', 'categories': 'cs.CL cs.SI physics.soc-ph', 'abstract': '  Human communication is often executed in the form of a narrative, an account\\nof connected events composed of characters, actions, and settings. A coherent\\nnarrative structure is therefore a requisite for a well-formulated narrative --\\nbe it fictional or nonfictional -- for informative and effective communication,\\nopening up the possibility of a deeper understanding of a narrative by studying\\nits structural properties. In this paper we present a network-based framework\\nfor modeling and analyzing the structure of a narrative, which is further\\nexpanded by incorporating methods from computational linguistics to utilize the\\nnarrative text. Modeling a narrative as a dynamically unfolding system, we\\ncharacterize its progression via the growth patterns of the character network,\\nand use sentiment analysis and topic modeling to represent the actual content\\nof the narrative in the form of interaction maps between characters with\\nassociated sentiment values and keywords. This is a network framework advanced\\nbeyond the simple occurrence-based one most often used until now, allowing one\\nto utilize the unique characteristics of a given narrative to a high degree.\\nGiven the ubiquity and importance of narratives, such advanced network-based\\nrepresentation and analysis framework may lead to a more systematic modeling\\nand understanding of narratives for social interactions, expression of human\\nsentiments, and communication.\\n', 'publish_date': 'Thu, 24 Mar 2016 10:59:28 GMT'}\n",
    "ID: 2407.13248, Distance: 0.4000556468963623, Content: {'title': 'Are Large Language Models Capable of Generating Human-Level Narratives?', 'doi': None, 'categories': 'cs.CL', 'abstract': '  This paper investigates the capability of LLMs in storytelling, focusing on\\nnarrative development and plot progression. We introduce a novel computational\\nframework to analyze narratives through three discourse-level aspects: i) story\\narcs, ii) turning points, and iii) affective dimensions, including arousal and\\nvalence. By leveraging expert and automatic annotations, we uncover significant\\ndiscrepancies between the LLM- and human- written stories. While human-written\\nstories are suspenseful, arousing, and diverse in narrative structures, LLM\\nstories are homogeneously positive and lack tension. Next, we measure narrative\\nreasoning skills as a precursor to generative capacities, concluding that most\\nLLMs fall short of human abilities in discourse understanding. Finally, we show\\nthat explicit integration of aforementioned discourse features can enhance\\nstorytelling, as is demonstrated by over 40% improvement in neural storytelling\\nin terms of diversity, suspense, and arousal.\\n', 'publish_date': 'Thu, 18 Jul 2024 08:02:49 GMT'}\n",
    "ID: 2206.03021, Distance: 0.4123629629611969, Content: {'title': 'Plot Writing From Pre-Trained Language Models', 'doi': None, 'categories': 'cs.CL', 'abstract': \"  Pre-trained language models (PLMs) fail to generate long-form narrative text\\nbecause they do not consider global structure. As a result, the generated texts\\nare often incohesive, repetitive, or lack content. Recent work in story\\ngeneration reintroduced explicit content planning in the form of prompts,\\nkeywords, or semantic frames. Trained on large parallel corpora, these models\\ncan generate more logical event sequences and thus more contentful stories.\\nHowever, these intermediate representations are often not in natural language\\nand cannot be utilized by PLMs without fine-tuning. We propose generating story\\nplots using off-the-shelf PLMs while maintaining the benefit of content\\nplanning to generate cohesive and contentful stories. Our proposed method,\\nScratchPlot, first prompts a PLM to compose a content plan. Then, we generate\\nthe story's body and ending conditioned on the content plan. Furthermore, we\\ntake a generate-and-rank approach by using additional PLMs to rank the\\ngenerated (story, ending) pairs. We benchmark our method with various baselines\\nand achieved superior results in both human and automatic evaluation.\\n\", 'publish_date': 'Tue, 7 Jun 2022 05:30:46 GMT'}\n",
    "\n",
    "Answer:\n",
    "\n",
    "==================================================\n",
    "生成嵌入中: 100%|██████████| 1/1 [00:00<00:00, 11.94it/s]\n",
    "[INFO] Query vector shape: (1, 1, 768)\n",
    "\n",
    "\n",
    "cost 0.1540 seconds\n",
    "\n",
    "\n",
    "You are a professional academic assistant specializing in the cs.CL domain. Strictly based on the provided literature information, answer the user's query. You may slightly expand and refine the response using existing knowledge, but avoid fabricating information. The response language should match the user's query language (English for English queries, Chinese for Chinese queries). Ensure the response is concise and limited to 500 words. Include a citation with the corresponding literature link (like [text](f\"https://arxiv.org/abs/{{id}}\")) and adjust the response style based on the temperature parameter (from 0.0 to 1.0). If the temperature is low (e.g., 0.2), provide a strict and precise answer. If the temperature is high (e.g., 0.8), provide a more creative and exploratory answer.\n",
    "\n",
    "Temperature: 0.2\n",
    "Question: 如何构建一个高效的文本语义相似度计算模型，以支持信息检索和文本匹配？\n",
    "Relevant Literature: \n",
    "ID: 1903.10675, Distance: 0.37604469060897827, Content: {'title': 'Document Similarity for Texts of Varying Lengths via Hidden Topics', 'doi': None, 'categories': 'cs.CL', 'abstract': '  Measuring similarity between texts is an important task for several\\napplications. Available approaches to measure document similarity are\\ninadequate for document pairs that have non-comparable lengths, such as a long\\ndocument and its summary. This is because of the lexical, contextual and the\\nabstraction gaps between a long document of rich details and its concise\\nsummary of abstract information. In this paper, we present a document matching\\napproach to bridge this gap, by comparing the texts in a common space of hidden\\ntopics. We evaluate the matching algorithm on two matching tasks and find that\\nit consistently and widely outperforms strong baselines. We also highlight the\\nbenefits of incorporating domain knowledge to text matching.\\n', 'publish_date': 'Tue, 26 Mar 2019 04:42:17 GMT'}\n",
    "ID: 1910.09129, Distance: 0.3761413097381592, Content: {'title': 'A Comparison of Semantic Similarity Methods for Maximum Human\\n  Interpretability', 'doi': None, 'categories': 'cs.IR cs.CL cs.LG', 'abstract': \"  The inclusion of semantic information in any similarity measures improves the\\nefficiency of the similarity measure and provides human interpretable results\\nfor further analysis. The similarity calculation method that focuses on\\nfeatures related to the text's words only, will give less accurate results.\\nThis paper presents three different methods that not only focus on the text's\\nwords but also incorporates semantic information of texts in their feature\\nvector and computes semantic similarities. These methods are based on\\ncorpus-based and knowledge-based methods, which are: cosine similarity using\\ntf-idf vectors, cosine similarity using word embedding and soft cosine\\nsimilarity using word embedding. Among these three, cosine similarity using\\ntf-idf vectors performed best in finding similarities between short news texts.\\nThe similar texts given by the method are easy to interpret and can be used\\ndirectly in other information retrieval applications.\\n\", 'publish_date': 'Mon, 21 Oct 2019 03:09:02 GMT'}\n",
    "ID: 2304.01330, Distance: 0.3791707456111908, Content: {'title': 'A Comparison of Document Similarity Algorithms', 'doi': None, 'categories': 'cs.CL cs.AI', 'abstract': '  Document similarity is an important part of Natural Language Processing and\\nis most commonly used for plagiarism-detection and text summarization. Thus,\\nfinding the overall most effective document similarity algorithm could have a\\nmajor positive impact on the field of Natural Language Processing. This report\\nsets out to examine the numerous document similarity algorithms, and determine\\nwhich ones are the most useful. It addresses the most effective document\\nsimilarity algorithm by categorizing them into 3 types of document similarity\\nalgorithms: statistical algorithms, neural networks, and corpus/knowledge-based\\nalgorithms. The most effective algorithms in each category are also compared in\\nour work using a series of benchmark datasets and evaluations that test every\\npossible area that each algorithm could be used in.\\n', 'publish_date': 'Mon, 3 Apr 2023 19:50:55 GMT'}\n",
    "ID: 1910.03940, Distance: 0.3885693848133087, Content: {'title': 'Measuring Sentences Similarity: A Survey', 'doi': '10.17485/ijst/2019/v12i25/143977', 'categories': 'cs.CL cs.IR', 'abstract': '  This study is to review the approaches used for measuring sentences\\nsimilarity. Measuring similarity between natural language sentences is a\\ncrucial task for many Natural Language Processing applications such as text\\nclassification, information retrieval, question answering, and plagiarism\\ndetection. This survey classifies approaches of calculating sentences\\nsimilarity based on the adopted methodology into three categories. Word-to-word\\nbased, structure based, and vector-based are the most widely used approaches to\\nfind sentences similarity. Each approach measures relatedness between short\\ntexts based on a specific perspective. In addition, datasets that are mostly\\nused as benchmarks for evaluating techniques in this field are introduced to\\nprovide a complete view on this issue. The approaches that combine more than\\none perspective give better results. Moreover, structure based similarity that\\nmeasures similarity between sentences structures needs more investigation.\\n', 'publish_date': 'Sun, 6 Oct 2019 09:21:21 GMT'}\n",
    "ID: 1403.4024, Distance: 0.39371269941329956, Content: {'title': 'Measuring Global Similarity between Texts', 'doi': None, 'categories': 'cs.CL', 'abstract': '  We propose a new similarity measure between texts which, contrary to the\\ncurrent state-of-the-art approaches, takes a global view of the texts to be\\ncompared. We have implemented a tool to compute our textual distance and\\nconducted experiments on several corpuses of texts. The experiments show that\\nour methods can reliably identify different global types of texts.\\n', 'publish_date': 'Mon, 17 Mar 2014 08:22:54 GMT'}\n",
    "\n",
    "Answer:\n",
    "\n",
    "==================================================\n",
    "生成嵌入中: 100%|██████████| 1/1 [00:00<00:00, 12.50it/s]\n",
    "[INFO] Query vector shape: (1, 1, 768)\n",
    "\n",
    "\n",
    "\n",
    "cost 0.1511 seconds\n",
    "\n",
    "\n",
    "You are a professional academic assistant specializing in the cs.CL domain. Strictly based on the provided literature information, answer the user's query. You may slightly expand and refine the response using existing knowledge, but avoid fabricating information. The response language should match the user's query language (English for English queries, Chinese for Chinese queries). Ensure the response is concise and limited to 500 words. Include a citation with the corresponding literature link (like [text](f\"https://arxiv.org/abs/{{id}}\")) and adjust the response style based on the temperature parameter (from 0.0 to 1.0). If the temperature is low (e.g., 0.2), provide a strict and precise answer. If the temperature is high (e.g., 0.8), provide a more creative and exploratory answer.\n",
    "\n",
    "Temperature: 0.2\n",
    "Question: 深度学习中的 “灾难性遗忘” 在 NLP 任务中表现为何种形式？\n",
    "Relevant Literature: \n",
    "ID: 2406.04836, Distance: 0.3405640125274658, Content: {'title': 'Revisiting Catastrophic Forgetting in Large Language Model Tuning', 'doi': None, 'categories': 'cs.CL cs.AI', 'abstract': '  Catastrophic Forgetting (CF) means models forgetting previously acquired\\nknowledge when learning new data. It compromises the effectiveness of large\\nlanguage models (LLMs) during fine-tuning, yet the underlying causes have not\\nbeen thoroughly investigated. This paper takes the first step to reveal the\\ndirect link between the flatness of the model loss landscape and the extent of\\nCF in the field of LLMs. Based on this, we introduce the sharpness-aware\\nminimization to mitigate CF by flattening the loss landscape. Experiments on\\nthree widely-used fine-tuning datasets, spanning different model scales,\\ndemonstrate the effectiveness of our method in alleviating CF. Analyses show\\nthat we nicely complement the existing anti-forgetting strategies, further\\nenhancing the resistance of LLMs to CF.\\n', 'publish_date': 'Fri, 7 Jun 2024 11:09:13 GMT'}\n",
    "ID: 2203.03910, Distance: 0.3545580208301544, Content: {'title': 'Overcoming Catastrophic Forgetting beyond Continual Learning: Balanced\\n  Training for Neural Machine Translation', 'doi': None, 'categories': 'cs.CL cs.AI', 'abstract': '  Neural networks tend to gradually forget the previously learned knowledge\\nwhen learning multiple tasks sequentially from dynamic data distributions. This\\nproblem is called \\\\textit{catastrophic forgetting}, which is a fundamental\\nchallenge in the continual learning of neural networks. In this work, we\\nobserve that catastrophic forgetting not only occurs in continual learning but\\nalso affects the traditional static training. Neural networks, especially\\nneural machine translation models, suffer from catastrophic forgetting even if\\nthey learn from a static training set. To be specific, the final model pays\\nimbalanced attention to training samples, where recently exposed samples\\nattract more attention than earlier samples. The underlying cause is that\\ntraining samples do not get balanced training in each model update, so we name\\nthis problem \\\\textit{imbalanced training}. To alleviate this problem, we\\npropose Complementary Online Knowledge Distillation (COKD), which uses\\ndynamically updated teacher models trained on specific data orders to\\niteratively provide complementary knowledge to the student model. Experimental\\nresults on multiple machine translation tasks show that our method successfully\\nalleviates the problem of imbalanced training and achieves substantial\\nimprovements over strong baseline systems.\\n', 'publish_date': 'Tue, 8 Mar 2022 08:08:45 GMT'}\n",
    "ID: 2308.08747, Distance: 0.3688642680644989, Content: {'title': 'An Empirical Study of Catastrophic Forgetting in Large Language Models\\n  During Continual Fine-tuning', 'doi': None, 'categories': 'cs.CL', 'abstract': \"  Catastrophic forgetting (CF) is a phenomenon that occurs in machine learning\\nwhen a model forgets previously learned information while acquiring new\\nknowledge for achieving a satisfactory performance in downstream tasks. As\\nlarge language models (LLMs) have demonstrated remarkable performance, it is\\nintriguing to investigate whether CF exists during the continual instruction\\ntuning of LLMs. This study empirically evaluates the forgetting phenomenon in\\nLLMs' knowledge during continual instruction tuning from the perspectives of\\ndomain knowledge, reasoning, and reading comprehension. The experiments reveal\\nthat catastrophic forgetting is generally observed in LLMs ranging from 1b to\\n7b parameters. Surprisingly, as the model scale increases, the severity of\\nforgetting intensifies in such a model sale range which may result from the\\nmuch significant initial performance in the larger LLM. Comparing the\\ndecoder-only model BLOOMZ with the encoder-decoder model mT0, BLOOMZ exhibits\\nless forgetting and retains more knowledge. Interestingly, we also observe that\\nLLMs can mitigate language biases, such as gender bias, during continual\\nfine-tuning. Furthermore, our findings indicate that general instruction tuning\\ncan help alleviate the forgetting phenomenon in LLMs during subsequent\\nfine-tuning.\\n\", 'publish_date': 'Thu, 17 Aug 2023 02:53:23 GMT'}\n",
    "ID: 2502.10966, Distance: 0.3742675185203552, Content: {'title': 'Neural Networks Remember More: The Power of Parameter Isolation and\\n  Combination', 'doi': None, 'categories': 'cs.CL cs.AI', 'abstract': \"  Catastrophic forgetting is a pervasive issue for pre-trained language models\\n(PLMs) during continual learning, where models lose previously acquired\\nknowledge when sequentially trained on a series of tasks. The model's ability\\nto retain old tasks is referred to as stability, while its adaptability to new\\ntasks is called plasticity. Therefore, the key to solving this problem is to\\nfind a trade-off between the plasticity and stability of the model. To address\\nthis issue, in this paper, we propose a novel method to achieve a balance\\nbetween model stability and plasticity, thereby mitigating catastrophic\\nforgetting. More specifically, our proposed approach leverages parameter\\nisolation and a subsequent combination strategy. Initially, in the training\\nstage, the model adapts to each downstream task via a parameter isolation\\nmethod to prevent potential interference among different tasks. We then combine\\nall trained parameters, which contain acquired knowledge, using the task\\narithmetic method and finally apply them to the backbone model. Empirical\\nevaluations on continual language learning benchmarks substantiate the\\neffectiveness of our approach, revealing a marked enhancement over existing\\nstate-of-the-art approaches.\\n\", 'publish_date': 'Sun, 16 Feb 2025 02:58:57 GMT'}\n",
    "ID: 2504.01241, Distance: 0.37489748001098633, Content: {'title': 'Catastrophic Forgetting in LLMs: A Comparative Analysis Across Language\\n  Tasks', 'doi': None, 'categories': 'cs.CL', 'abstract': \"  Large Language Models (LLMs) have significantly advanced Natural Language\\nProcessing (NLP), particularly in Natural Language Understanding (NLU) tasks.\\nAs we progress toward an agentic world where LLM-based agents autonomously\\nhandle specialized tasks, it becomes crucial for these models to adapt to new\\ntasks without forgetting previously learned information - a challenge known as\\ncatastrophic forgetting. This study evaluates the continual fine-tuning of\\nvarious open-source LLMs with different parameter sizes (specifically models\\nunder 10 billion parameters) on key NLU tasks from the GLUE benchmark,\\nincluding SST-2, MRPC, CoLA, and MNLI. By employing prompt engineering and\\ntask-specific adjustments, we assess and compare the models' abilities to\\nretain prior knowledge while learning new tasks. Our results indicate that\\nmodels such as Phi-3.5-mini exhibit minimal forgetting while maintaining strong\\nlearning capabilities, making them well-suited for continual learning\\nenvironments. Additionally, models like Orca-2-7b and Qwen2.5-7B demonstrate\\nimpressive learning abilities and overall performance after fine-tuning. This\\nwork contributes to understanding catastrophic forgetting in LLMs and\\nhighlights prompting engineering to optimize model performance for continual\\nlearning scenarios.\\n\", 'publish_date': 'Tue, 1 Apr 2025 23:06:55 GMT'}\n",
    "\n",
    "Answer:\n",
    "\n",
    "==================================================\n",
    "生成嵌入中: 100%|██████████| 1/1 [00:00<00:00, 12.72it/s]\n",
    "[INFO] Query vector shape: (1, 1, 768)\n",
    "\n",
    "\n",
    "cost 0.1478 seconds\n",
    "\n",
    "\n",
    "You are a professional academic assistant specializing in the cs.CL domain. Strictly based on the provided literature information, answer the user's query. You may slightly expand and refine the response using existing knowledge, but avoid fabricating information. The response language should match the user's query language (English for English queries, Chinese for Chinese queries). Ensure the response is concise and limited to 500 words. Include a citation with the corresponding literature link (like [text](f\"https://arxiv.org/abs/{{id}}\")) and adjust the response style based on the temperature parameter (from 0.0 to 1.0). If the temperature is low (e.g., 0.2), provide a strict and precise answer. If the temperature is high (e.g., 0.8), provide a more creative and exploratory answer.\n",
    "\n",
    "Temperature: 0.2\n",
    "Question: 阅读障碍者的语言处理缺陷能否通过深度学习模型复现？\n",
    "Relevant Literature: \n",
    "ID: 2412.15785, Distance: 0.4120064973831177, Content: {'title': 'Learning from Impairment: Leveraging Insights from Clinical Linguistics\\n  in Language Modelling Research', 'doi': None, 'categories': 'cs.CL', 'abstract': '  This position paper investigates the potential of integrating insights from\\nlanguage impairment research and its clinical treatment to develop\\nhuman-inspired learning strategies and evaluation frameworks for language\\nmodels (LMs). We inspect the theoretical underpinnings underlying some\\ninfluential linguistically motivated training approaches derived from\\nneurolinguistics and, particularly, aphasiology, aimed at enhancing the\\nrecovery and generalization of linguistic skills in aphasia treatment, with a\\nprimary focus on those targeting the syntactic domain. We highlight how these\\ninsights can inform the design of rigorous assessments for LMs, specifically in\\ntheir handling of complex syntactic phenomena, as well as their implications\\nfor developing human-like learning strategies, aligning with efforts to create\\nmore sustainable and cognitively plausible natural language processing (NLP)\\nmodels.\\n', 'publish_date': 'Fri, 20 Dec 2024 10:53:21 GMT'}\n",
    "ID: 2211.05557, Distance: 0.46181079745292664, Content: {'title': 'Assistive Completion of Agrammatic Aphasic Sentences: A Transfer\\n  Learning Approach using Neurolinguistics-based Synthetic Dataset', 'doi': None, 'categories': 'q-bio.QM cs.CL', 'abstract': \"  Damage to the inferior frontal gyrus (Broca's area) can cause agrammatic\\naphasia wherein patients, although able to comprehend, lack the ability to form\\ncomplete sentences. This inability leads to communication gaps which cause\\ndifficulties in their daily lives. The usage of assistive devices can help in\\nmitigating these issues and enable the patients to communicate effectively.\\nHowever, due to lack of large scale studies of linguistic deficits in aphasia,\\nresearch on such assistive technology is relatively limited. In this work, we\\npresent two contributions that aim to re-initiate research and development in\\nthis field. Firstly, we propose a model that uses linguistic features from\\nsmall scale studies on aphasia patients and generates large scale datasets of\\nsynthetic aphasic utterances from grammatically correct datasets. We show that\\nthe mean length of utterance, the noun/verb ratio, and the simple/complex\\nsentence ratio of our synthetic datasets correspond to the reported features of\\naphasic speech. Further, we demonstrate how the synthetic datasets may be\\nutilized to develop assistive devices for aphasia patients. The pre-trained T5\\ntransformer is fine-tuned using the generated dataset to suggest 5 corrected\\nsentences given an aphasic utterance as input. We evaluate the efficacy of the\\nT5 model using the BLEU and cosine semantic similarity scores. Affirming\\nresults with BLEU score of 0.827/1.00 and semantic similarity of 0.904/1.00\\nwere obtained. These results provide a strong foundation for the concept that a\\nsynthetic dataset based on small scale studies on aphasia can be used to\\ndevelop effective assistive technology.\\n\", 'publish_date': 'Thu, 10 Nov 2022 13:24:02 GMT'}\n",
    "ID: 2110.15778, Distance: 0.4696163237094879, Content: {'title': 'Comparing Machine Learning-Centered Approaches for Forecasting Language\\n  Patterns During Frustration in Early Childhood', 'doi': None, 'categories': 'cs.CL cs.LG', 'abstract': '  When faced with self-regulation challenges, children have been known the use\\ntheir language to inhibit their emotions and behaviors. Yet, to date, there has\\nbeen a critical lack of evidence regarding what patterns in their speech\\nchildren use during these moments of frustration. In this paper, eXtreme\\nGradient Boosting, Random Forest, Long Short-Term Memory Recurrent Neural\\nNetworks, and Elastic Net Regression, have all been used to forecast these\\nlanguage patterns in children. Based on the results of a comparative analysis\\nbetween these methods, the study reveals that when dealing with\\nhigh-dimensional and dense data, with very irregular and abnormal\\ndistributions, as is the case with self-regulation patterns in children,\\ndecision tree-based algorithms are able to outperform traditional regression\\nand neural network methods in their shortcomings.\\n', 'publish_date': 'Fri, 29 Oct 2021 13:45:38 GMT'}\n",
    "ID: 2311.15054, Distance: 0.4710720181465149, Content: {'title': 'Detection of developmental language disorder in Cypriot Greek children\\n  using a neural network algorithm', 'doi': '10.1007/s41347-024-00460-4', 'categories': 'cs.CL cs.LG', 'abstract': '  Children with developmental language disorder (DLD) encounter difficulties in\\nacquiring various language structures. Early identification and intervention\\nare crucial to prevent negative long-term outcomes impacting the academic,\\nsocial, and emotional development of children. The study aims to develop an\\nautomated method for the identification of DLD using artificial intelligence,\\nspecifically a neural network machine learning algorithm. This protocol is\\napplied for the first time in a Cypriot Greek child population with DLD. The\\nneural network model was trained using perceptual and production data elicited\\nfrom 15 children with DLD and 15 healthy controls in the age range of 7;10\\nuntil 10;4. The k-fold technique was used to crossvalidate the algorithm. The\\nperformance of the model was evaluated using metrics such as accuracy,\\nprecision, recall, F1 score, and ROC/AUC curve to assess its ability to make\\naccurate predictions on a set of unseen data. The results demonstrated high\\nclassification values for all metrics, indicating the high accuracy of the\\nneural model in classifying children with DLD. Additionally, the variable\\nimportance analysis revealed that the language production skills of children\\nhad a more significant impact on the performance of the model compared to\\nperception skills. Machine learning paradigms provide effective discrimination\\nbetween children with DLD and those with TD, with the potential to enhance\\nclinical assessment and facilitate earlier and more efficient detection of the\\ndisorder.\\n', 'publish_date': 'Sat, 25 Nov 2023 15:23:46 GMT'}\n",
    "ID: 2407.11345, Distance: 0.4917356073856354, Content: {'title': 'Beyond Binary: Multiclass Paraphasia Detection with Generative\\n  Pretrained Transformers and End-to-End Models', 'doi': None, 'categories': 'cs.CL cs.SD eess.AS', 'abstract': '  Aphasia is a language disorder that can lead to speech errors known as\\nparaphasias, which involve the misuse, substitution, or invention of words.\\nAutomatic paraphasia detection can help those with Aphasia by facilitating\\nclinical assessment and treatment planning options. However, most automatic\\nparaphasia detection works have focused solely on binary detection, which\\ninvolves recognizing only the presence or absence of a paraphasia. Multiclass\\nparaphasia detection represents an unexplored area of research that focuses on\\nidentifying multiple types of paraphasias and where they occur in a given\\nspeech segment. We present novel approaches that use a generative pretrained\\ntransformer (GPT) to identify paraphasias from transcripts as well as two\\nend-to-end approaches that focus on modeling both automatic speech recognition\\n(ASR) and paraphasia classification as multiple sequences vs. a single\\nsequence. We demonstrate that a single sequence model outperforms GPT baselines\\nfor multiclass paraphasia detection.\\n', 'publish_date': 'Tue, 16 Jul 2024 03:24:51 GMT'}\n",
    "\n",
    "Answer:\n",
    "\n",
    "==================================================\n",
    "生成嵌入中: 100%|██████████| 1/1 [00:00<00:00, 12.60it/s]\n",
    "[INFO] Query vector shape: (1, 1, 768)\n",
    "\n",
    "\n",
    "cost 0.1486 seconds\n",
    "\n",
    "\n",
    "You are a professional academic assistant specializing in the cs.CL domain. Strictly based on the provided literature information, answer the user's query. You may slightly expand and refine the response using existing knowledge, but avoid fabricating information. The response language should match the user's query language (English for English queries, Chinese for Chinese queries). Ensure the response is concise and limited to 500 words. Include a citation with the corresponding literature link (like [text](f\"https://arxiv.org/abs/{{id}}\")) and adjust the response style based on the temperature parameter (from 0.0 to 1.0). If the temperature is low (e.g., 0.2), provide a strict and precise answer. If the temperature is high (e.g., 0.8), provide a more creative and exploratory answer.\n",
    "\n",
    "Temperature: 0.2\n",
    "Question: 如何进行代码生成？\n",
    "Relevant Literature: \n",
    "ID: 2204.05999, Distance: 0.3610247075557709, Content: {'title': 'InCoder: A Generative Model for Code Infilling and Synthesis', 'doi': None, 'categories': 'cs.SE cs.CL cs.LG', 'abstract': '  Code is seldom written in a single left-to-right pass and is instead\\nrepeatedly edited and refined. We introduce InCoder, a unified generative model\\nthat can perform program synthesis (via left-to-right generation) as well as\\nediting (via infilling). InCoder is trained to generate code files from a large\\ncorpus of permissively licensed code, where regions of code have been randomly\\nmasked and moved to the end of each file, allowing code infilling with\\nbidirectional context. Our model is the first generative model that is able to\\ndirectly perform zero-shot code infilling, which we evaluate on challenging\\ntasks such as type inference, comment generation, and variable re-naming. We\\nfind that the ability to condition on bidirectional context substantially\\nimproves performance on these tasks, while still performing comparably on\\nstandard program synthesis benchmarks in comparison to left-to-right only\\nmodels pretrained at similar scale. The InCoder models and code are publicly\\nreleased. https://sites.google.com/view/incoder-code-models\\n', 'publish_date': 'Tue, 12 Apr 2022 16:25:26 GMT'}\n",
    "ID: 2203.13474, Distance: 0.3621959686279297, Content: {'title': 'CodeGen: An Open Large Language Model for Code with Multi-Turn Program\\n  Synthesis', 'doi': None, 'categories': 'cs.LG cs.CL cs.PL', 'abstract': '  Program synthesis strives to generate a computer program as a solution to a\\ngiven problem specification, expressed with input-output examples or natural\\nlanguage descriptions. The prevalence of large language models advances the\\nstate-of-the-art for program synthesis, though limited training resources and\\ndata impede open access to such models. To democratize this, we train and\\nrelease a family of large language models up to 16.1B parameters, called\\nCODEGEN, on natural language and programming language data, and open source the\\ntraining library JAXFORMER. We show the utility of the trained model by\\ndemonstrating that it is competitive with the previous state-of-the-art on\\nzero-shot Python code generation on HumanEval. We further investigate the\\nmulti-step paradigm for program synthesis, where a single program is factorized\\ninto multiple prompts specifying subproblems. To this end, we construct an open\\nbenchmark, Multi-Turn Programming Benchmark (MTPB), consisting of 115 diverse\\nproblem sets that are factorized into multi-turn prompts. Our analysis on MTPB\\nshows that the same intent provided to CODEGEN in multi-turn fashion\\nsignificantly improves program synthesis over that provided as a single turn.\\nWe make the training library JAXFORMER and model checkpoints available as open\\nsource contribution: https://github.com/salesforce/CodeGen.\\n', 'publish_date': 'Fri, 25 Mar 2022 06:55:15 GMT'}\n",
    "ID: 1708.00098, Distance: 0.38170579075813293, Content: {'title': 'The Code2Text Challenge: Text Generation in Source Code Libraries', 'doi': '10.18653/v1/W17-3516', 'categories': 'cs.CL', 'abstract': '  We propose a new shared task for tactical data-to-text generation in the\\ndomain of source code libraries. Specifically, we focus on text generation of\\nfunction descriptions from example software projects. Data is drawn from\\nexisting resources used for studying the related problem of semantic parser\\ninduction (Richardson and Kuhn, 2017b; Richardson and Kuhn, 2017a), and spans a\\nwide variety of both natural languages and programming languages. In this\\npaper, we describe these existing resources, which will serve as training and\\ndevelopment data for the task, and discuss plans for building new independent\\ntest sets.\\n', 'publish_date': 'Mon, 31 Jul 2017 23:29:41 GMT'}\n",
    "ID: 2201.08810, Distance: 0.38896894454956055, Content: {'title': 'GAP-Gen: Guided Automatic Python Code Generation', 'doi': None, 'categories': 'cs.PL cs.CL cs.LG cs.SE', 'abstract': '  Automatic code generation from natural language descriptions can be highly\\nbeneficial during the process of software development. In this work, we propose\\nGAP-Gen, a Guided Automatic Python Code Generation method based on Python\\nsyntactic constraints and semantic constraints. We first introduce Python\\nsyntactic constraints in the form of Syntax-Flow, which is a simplified version\\nof Abstract Syntax Tree (AST) reducing the size and high complexity of Abstract\\nSyntax Tree but maintaining crucial syntactic information of Python code. In\\naddition to Syntax-Flow, we introduce Variable-Flow which abstracts variable\\nand function names consistently through out the code. In our work, rather than\\npretraining, we focus on modifying the finetuning process which reduces\\ncomputational requirements but retains high generation performance on automatic\\nPython code generation task. GAP-Gen fine-tunes the transformer based language\\nmodels T5 and CodeT5 using the Code-to-Docstring datasets CodeSearchNet,\\nCodeSearchNet AdvTest and Code-Docstring Corpus from EdinburghNLP. Our\\nexperiments show that GAP-Gen achieves better results on automatic Python code\\ngeneration task than previous works.\\n', 'publish_date': 'Wed, 19 Jan 2022 06:32:47 GMT'}\n",
    "ID: 2410.02749, Distance: 0.39678213000297546, Content: {'title': 'Training Language Models on Synthetic Edit Sequences Improves Code\\n  Synthesis', 'doi': None, 'categories': 'cs.LG cs.CL', 'abstract': '  Software engineers mainly write code by editing existing programs. In\\ncontrast, language models (LMs) autoregressively synthesize programs in a\\nsingle pass. One explanation for this is the scarcity of sequential edit data.\\nWhile high-quality instruction data for code synthesis is scarce, edit data for\\nsynthesis is even scarcer. To fill this gap, we develop a synthetic data\\ngeneration algorithm called LintSeq. This algorithm refactors programs into\\nsequences of synthetic edits by using a linter to procedurally sample across\\ninterdependent lines of source code. Synthetic edits sampled with LintSeq\\nreflect the syntax and semantics of their programming language. To test the\\nalgorithm, we use it to refactor a dataset of instruction + program pairs into\\ninstruction + program-diff-sequence tuples. Then, we fine-tune a series of\\nsmaller LMs ranging from 2.6B to 14B parameters on both the re-factored and\\noriginal versions of this dataset. We perform comprehensive evaluations\\ncomparing edit sequence code LMs against baselines on HumanEval, MBPP(+),\\nCodeContests, DS-1000, and BigCodeBench. We show that models fine-tuned to\\niteratively synthesize code match or outperform baselines on pass@1, and\\nexhibit better scaling across higher pass@k as a function of total test-time\\nFLOPs. Finally, we also pretrain our own tiny LMs for code understanding. We\\nshow that fine-tuning these models to synthesize code edit-by-edit results in\\nstrong performance on HumanEval and MBPP(+) compared to existing code language\\nmodels of similar scale such as CodeT5+, AlphaCode, and Codex.\\n', 'publish_date': 'Thu, 3 Oct 2024 17:57:22 GMT'}\n",
    "\n",
    "Answer:\n",
    "\n",
    "==================================================\n",
    "生成嵌入中: 100%|██████████| 1/1 [00:00<00:00, 13.49it/s]\n",
    "[INFO] Query vector shape: (1, 1, 768)\n",
    "\n",
    "\n",
    "cost 0.1355 seconds\n",
    "\n",
    "\n",
    "You are a professional academic assistant specializing in the cs.CL domain. Strictly based on the provided literature information, answer the user's query. You may slightly expand and refine the response using existing knowledge, but avoid fabricating information. The response language should match the user's query language (English for English queries, Chinese for Chinese queries). Ensure the response is concise and limited to 500 words. Include a citation with the corresponding literature link (like [text](f\"https://arxiv.org/abs/{{id}}\")) and adjust the response style based on the temperature parameter (from 0.0 to 1.0). If the temperature is low (e.g., 0.2), provide a strict and precise answer. If the temperature is high (e.g., 0.8), provide a more creative and exploratory answer.\n",
    "\n",
    "Temperature: 0.2\n",
    "Question: 语系差异显著的语言（如汉藏语系 vs. 印欧语系）在句法表征上的本质区别如何影响跨语言迁移？\n",
    "Relevant Literature: \n",
    "ID: 1906.02656, Distance: 0.34337860345840454, Content: {'title': 'Cross-Lingual Syntactic Transfer through Unsupervised Adaptation of\\n  Invertible Projections', 'doi': None, 'categories': 'cs.CL cs.LG', 'abstract': '  Cross-lingual transfer is an effective way to build syntactic analysis tools\\nin low-resource languages. However, transfer is difficult when transferring to\\ntypologically distant languages, especially when neither annotated target data\\nnor parallel corpora are available. In this paper, we focus on methods for\\ncross-lingual transfer to distant languages and propose to learn a generative\\nmodel with a structured prior that utilizes labeled source data and unlabeled\\ntarget data jointly. The parameters of source model and target model are softly\\nshared through a regularized log likelihood objective. An invertible projection\\nis employed to learn a new interlingual latent embedding space that compensates\\nfor imperfect cross-lingual word embedding input. We evaluate our method on two\\nsyntactic tasks: part-of-speech (POS) tagging and dependency parsing. On the\\nUniversal Dependency Treebanks, we use English as the only source corpus and\\ntransfer to a wide range of target languages. On the 10 languages in this\\ndataset that are distant from English, our method yields an average of 5.2%\\nabsolute improvement on POS tagging and 8.3% absolute improvement on dependency\\nparsing over a direct transfer method using state-of-the-art discriminative\\nmodels.\\n', 'publish_date': 'Thu, 6 Jun 2019 15:46:17 GMT'}\n",
    "ID: 2304.08823, Distance: 0.35134708881378174, Content: {'title': 'Transfer to a Low-Resource Language via Close Relatives: The Case Study\\n  on Faroese', 'doi': None, 'categories': 'cs.CL', 'abstract': \"  Multilingual language models have pushed state-of-the-art in cross-lingual\\nNLP transfer. The majority of zero-shot cross-lingual transfer, however, use\\none and the same massively multilingual transformer (e.g., mBERT or XLM-R) to\\ntransfer to all target languages, irrespective of their typological,\\netymological, and phylogenetic relations to other languages. In particular,\\nreadily available data and models of resource-rich sibling languages are often\\nignored. In this work, we empirically show, in a case study for Faroese -- a\\nlow-resource language from a high-resource language family -- that by\\nleveraging the phylogenetic information and departing from the\\n'one-size-fits-all' paradigm, one can improve cross-lingual transfer to\\nlow-resource languages. In particular, we leverage abundant resources of other\\nScandinavian languages (i.e., Danish, Norwegian, Swedish, and Icelandic) for\\nthe benefit of Faroese. Our evaluation results show that we can substantially\\nimprove the transfer performance to Faroese by exploiting data and models of\\nclosely-related high-resource languages. Further, we release a new web corpus\\nof Faroese and Faroese datasets for named entity recognition (NER), semantic\\ntext similarity (STS), and new language models trained on all Scandinavian\\nlanguages.\\n\", 'publish_date': 'Tue, 18 Apr 2023 08:42:38 GMT'}\n",
    "ID: 2003.14056, Distance: 0.35307958722114563, Content: {'title': 'Understanding Cross-Lingual Syntactic Transfer in Multilingual Recurrent\\n  Neural Networks', 'doi': None, 'categories': 'cs.CL cs.LG', 'abstract': '  It is now established that modern neural language models can be successfully\\ntrained on multiple languages simultaneously without changes to the underlying\\narchitecture. But what kind of knowledge is really shared among languages\\nwithin these models? Does multilingual training mostly lead to an alignment of\\nthe lexical representation spaces or does it also enable the sharing of purely\\ngrammatical knowledge? In this paper we dissect different forms of\\ncross-lingual transfer and look for its most determining factors, using a\\nvariety of models and probing tasks. We find that exposing our LMs to a related\\nlanguage does not always increase grammatical knowledge in the target language,\\nand that optimal conditions for lexical-semantic transfer may not be optimal\\nfor syntactic transfer.\\n', 'publish_date': 'Tue, 31 Mar 2020 09:48:25 GMT'}\n",
    "ID: 2503.03962, Distance: 0.35709017515182495, Content: {'title': 'On the Acquisition of Shared Grammatical Representations in Bilingual\\n  Language Models', 'doi': None, 'categories': 'cs.CL', 'abstract': \"  While crosslingual transfer is crucial to contemporary language models'\\nmultilingual capabilities, how it occurs is not well understood. In this paper,\\nwe ask what happens to a monolingual language model when it begins to be\\ntrained on a second language. Specifically, we train small bilingual models for\\nwhich we control the amount of data for each language and the order of language\\nexposure. To find evidence of shared multilingual representations, we turn to\\nstructural priming, a method used to study grammatical representations in\\nhumans. We first replicate previous crosslingual structural priming results and\\nfind that after controlling for training data quantity and language exposure,\\nthere are asymmetrical effects across language pairs and directions. We argue\\nthat this asymmetry may shape hypotheses about human structural priming\\neffects. We also find that structural priming effects are less robust for less\\nsimilar language pairs, highlighting potential limitations of crosslingual\\ntransfer learning and shared representations for typologically diverse\\nlanguages.\\n\", 'publish_date': 'Wed, 5 Mar 2025 23:27:58 GMT'}\n",
    "ID: 1811.00570, Distance: 0.36333853006362915, Content: {'title': 'On Difficulties of Cross-Lingual Transfer with Order Differences: A Case\\n  Study on Dependency Parsing', 'doi': None, 'categories': 'cs.CL cs.LG', 'abstract': '  Different languages might have different word orders. In this paper, we\\ninvestigate cross-lingual transfer and posit that an order-agnostic model will\\nperform better when transferring to distant foreign languages. To test our\\nhypothesis, we train dependency parsers on an English corpus and evaluate their\\ntransfer performance on 30 other languages. Specifically, we compare encoders\\nand decoders based on Recurrent Neural Networks (RNNs) and modified\\nself-attentive architectures. The former relies on sequential information while\\nthe latter is more flexible at modeling word order. Rigorous experiments and\\ndetailed analysis shows that RNN-based architectures transfer well to languages\\nthat are close to English, while self-attentive models have better overall\\ncross-lingual transferability and perform especially well on distant languages.\\n', 'publish_date': 'Thu, 1 Nov 2018 18:11:01 GMT'}\n",
    "\n",
    "Answer:\n",
    "\n",
    "==================================================\n",
    "生成嵌入中: 100%|██████████| 1/1 [00:00<00:00,  9.39it/s]\n",
    "[INFO] Query vector shape: (1, 1, 768)\n",
    "\n",
    "\n",
    "cost 0.1671 seconds\n",
    "\n",
    "\n",
    "You are a professional academic assistant specializing in the cs.CL domain. Strictly based on the provided literature information, answer the user's query. You may slightly expand and refine the response using existing knowledge, but avoid fabricating information. The response language should match the user's query language (English for English queries, Chinese for Chinese queries). Ensure the response is concise and limited to 500 words. Include a citation with the corresponding literature link (like [text](f\"https://arxiv.org/abs/{{id}}\")) and adjust the response style based on the temperature parameter (from 0.0 to 1.0). If the temperature is low (e.g., 0.2), provide a strict and precise answer. If the temperature is high (e.g., 0.8), provide a more creative and exploratory answer.\n",
    "\n",
    "Temperature: 0.2\n",
    "Question: 在多语言文本分类任务中，如何利用多任务学习来提高分类性能？\n",
    "Relevant Literature: \n",
    "ID: 1812.09617, Distance: 0.3566693365573883, Content: {'title': 'Exploiting Cross-Lingual Subword Similarities in Low-Resource Document\\n  Classification', 'doi': None, 'categories': 'cs.CL cs.LG', 'abstract': '  Text classification must sometimes be applied in a low-resource language with\\nno labeled training data. However, training data may be available in a related\\nlanguage. We investigate whether character-level knowledge transfer from a\\nrelated language helps text classification. We present a cross-lingual document\\nclassification framework (CACO) that exploits cross-lingual subword similarity\\nby jointly training a character-based embedder and a word-based classifier. The\\nembedder derives vector representations for input words from their written\\nforms, and the classifier makes predictions based on the word vectors. We use a\\njoint character representation for both the source language and the target\\nlanguage, which allows the embedder to generalize knowledge about source\\nlanguage words to target language words with similar forms. We propose a\\nmulti-task objective that can further improve the model if additional\\ncross-lingual or monolingual resources are available. Experiments confirm that\\ncharacter-level knowledge transfer is more data-efficient than word-level\\ntransfer between related languages.\\n', 'publish_date': 'Sat, 22 Dec 2018 22:53:19 GMT'}\n",
    "ID: 1906.09543, Distance: 0.36465710401535034, Content: {'title': 'Cross-lingual Data Transformation and Combination for Text\\n  Classification', 'doi': None, 'categories': 'cs.IR cs.CL', 'abstract': '  Text classification is a fundamental task for text data mining. In order to\\ntrain a generalizable model, a large volume of text must be collected. To\\naddress data insufficiency, cross-lingual data may occasionally be necessary.\\nCross-lingual data sources may however suffer from data incompatibility, as\\ntext written in different languages can hold distinct word sequences and\\nsemantic patterns. Machine translation and word embedding alignment provide an\\neffective way to transform and combine data for cross-lingual data training. To\\nthe best of our knowledge, there has been little work done on evaluating how\\nthe methodology used to conduct semantic space transformation and data\\ncombination affects the performance of classification models trained from\\ncross-lingual resources. In this paper, we systematically evaluated the\\nperformance of two commonly used CNN (Convolutional Neural Network) and RNN\\n(Recurrent Neural Network) text classifiers with differing data transformation\\nand combination strategies. Monolingual models were trained from English and\\nFrench alongside their translated and aligned embeddings. Our results suggested\\nthat semantic space transformation may conditionally promote the performance of\\nmonolingual models. Bilingual models were trained from a combination of both\\nEnglish and French. Our results indicate that a cross-lingual classification\\nmodel can significantly benefit from cross-lingual data by learning from\\ntranslated or aligned embedding spaces.\\n', 'publish_date': 'Sun, 23 Jun 2019 02:56:02 GMT'}\n",
    "ID: 2010.02562, Distance: 0.36618563532829285, Content: {'title': 'Cross-Lingual Text Classification with Minimal Resources by Transferring\\n  a Sparse Teacher', 'doi': None, 'categories': 'cs.CL', 'abstract': '  Cross-lingual text classification alleviates the need for manually labeled\\ndocuments in a target language by leveraging labeled documents from other\\nlanguages. Existing approaches for transferring supervision across languages\\nrequire expensive cross-lingual resources, such as parallel corpora, while less\\nexpensive cross-lingual representation learning approaches train classifiers\\nwithout target labeled documents. In this work, we propose a cross-lingual\\nteacher-student method, CLTS, that generates \"weak\" supervision in the target\\nlanguage using minimal cross-lingual resources, in the form of a small number\\nof word translations. Given a limited translation budget, CLTS extracts and\\ntransfers only the most important task-specific seed words across languages and\\ninitializes a teacher classifier based on the translated seed words. Then, CLTS\\niteratively trains a more powerful student that also exploits the context of\\nthe seed words in unlabeled target documents and outperforms the teacher. CLTS\\nis simple and surprisingly effective in 18 diverse languages: by transferring\\njust 20 seed words, even a bag-of-words logistic regression student outperforms\\nstate-of-the-art cross-lingual methods (e.g., based on multilingual BERT).\\nMoreover, CLTS can accommodate any type of student classifier: leveraging a\\nmonolingual BERT student leads to further improvements and outperforms even\\nmore expensive approaches by up to 12% in accuracy. Finally, CLTS addresses\\nemerging tasks in low-resource languages using just a small number of word\\ntranslations.\\n', 'publish_date': 'Tue, 6 Oct 2020 09:11:02 GMT'}\n",
    "ID: 1805.09821, Distance: 0.3667503893375397, Content: {'title': 'A Corpus for Multilingual Document Classification in Eight Languages', 'doi': None, 'categories': 'cs.CL', 'abstract': '  Cross-lingual document classification aims at training a document classifier\\non resources in one language and transferring it to a different language\\nwithout any additional resources. Several approaches have been proposed in the\\nliterature and the current best practice is to evaluate them on a subset of the\\nReuters Corpus Volume 2. However, this subset covers only few languages\\n(English, German, French and Spanish) and almost all published works focus on\\nthe the transfer between English and German. In addition, we have observed that\\nthe class prior distributions differ significantly between the languages. We\\nargue that this complicates the evaluation of the multilinguality. In this\\npaper, we propose a new subset of the Reuters corpus with balanced class priors\\nfor eight languages. By adding Italian, Russian, Japanese and Chinese, we cover\\nlanguages which are very different with respect to syntax, morphology, etc. We\\nprovide strong baselines for all language transfer directions using\\nmultilingual word and sentence embeddings respectively. Our goal is to offer a\\nfreely available framework to evaluate cross-lingual document classification,\\nand we hope to foster by these means, research in this important area.\\n', 'publish_date': 'Thu, 24 May 2018 10:36:20 GMT'}\n",
    "ID: 2402.17016, Distance: 0.3677421808242798, Content: {'title': 'Multi-Task Contrastive Learning for 8192-Token Bilingual Text Embeddings', 'doi': None, 'categories': 'cs.CL cs.AI cs.IR', 'abstract': '  We introduce a novel suite of state-of-the-art bilingual text embedding\\nmodels that are designed to support English and another target language. These\\nmodels are capable of processing lengthy text inputs with up to 8192 tokens,\\nmaking them highly versatile for a range of natural language processing tasks\\nsuch as text retrieval, clustering, and semantic textual similarity (STS)\\ncalculations.\\n  By focusing on bilingual models and introducing a unique multi-task learning\\nobjective, we have significantly improved the model performance on STS tasks,\\nwhich outperforms the capabilities of existing multilingual models in both\\ntarget language understanding and cross-lingual evaluation tasks. Moreover, our\\nbilingual models are more efficient, requiring fewer parameters and less memory\\ndue to their smaller vocabulary needs. Furthermore, we have expanded the\\nMassive Text Embedding Benchmark (MTEB) to include benchmarks for German and\\nSpanish embedding models. This integration aims to stimulate further research\\nand advancement in text embedding technologies for these languages.\\n', 'publish_date': 'Mon, 26 Feb 2024 20:53:12 GMT'}\n",
    "\n",
    "Answer:\n",
    "\n",
    "==================================================\n",
    "生成嵌入中: 100%|██████████| 1/1 [00:00<00:00, 13.54it/s]\n",
    "[INFO] Query vector shape: (1, 1, 768)\n",
    "\n",
    "\n",
    "cost 0.1387 seconds\n",
    "\n",
    "\n",
    "You are a professional academic assistant specializing in the cs.CL domain. Strictly based on the provided literature information, answer the user's query. You may slightly expand and refine the response using existing knowledge, but avoid fabricating information. The response language should match the user's query language (English for English queries, Chinese for Chinese queries). Ensure the response is concise and limited to 500 words. Include a citation with the corresponding literature link (like [text](f\"https://arxiv.org/abs/{{id}}\")) and adjust the response style based on the temperature parameter (from 0.0 to 1.0). If the temperature is low (e.g., 0.2), provide a strict and precise answer. If the temperature is high (e.g., 0.8), provide a more creative and exploratory answer.\n",
    "\n",
    "Temperature: 0.2\n",
    "Question: 如何区分大语言模型生成文本与人类文本？\n",
    "Relevant Literature: \n",
    "ID: 1603.07771, Distance: 0.3722044825553894, Content: {'title': 'Neural Text Generation from Structured Data with Application to the\\n  Biography Domain', 'doi': None, 'categories': 'cs.CL', 'abstract': '  This paper introduces a neural model for concept-to-text generation that\\nscales to large, rich domains. We experiment with a new dataset of biographies\\nfrom Wikipedia that is an order of magnitude larger than existing resources\\nwith over 700k samples. The dataset is also vastly more diverse with a 400k\\nvocabulary, compared to a few hundred words for Weathergov or Robocup. Our\\nmodel builds upon recent work on conditional neural language model for text\\ngeneration. To deal with the large vocabulary, we extend these models to mix a\\nfixed vocabulary with copy actions that transfer sample-specific words from the\\ninput database to the generated output sentence. Our neural model significantly\\nout-performs a classical Kneser-Ney language model adapted to this task by\\nnearly 15 BLEU.\\n', 'publish_date': 'Thu, 24 Mar 2016 22:40:00 GMT'}\n",
    "ID: 2102.02723, Distance: 0.4088231325149536, Content: {'title': 'Data-to-text Generation with Macro Planning', 'doi': None, 'categories': 'cs.CL', 'abstract': '  Recent approaches to data-to-text generation have adopted the very successful\\nencoder-decoder architecture or variants thereof. These models generate text\\nwhich is fluent (but often imprecise) and perform quite poorly at selecting\\nappropriate content and ordering it coherently. To overcome some of these\\nissues, we propose a neural model with a macro planning stage followed by a\\ngeneration stage reminiscent of traditional methods which embrace separate\\nmodules for planning and surface realization. Macro plans represent high level\\norganization of important content such as entities, events and their\\ninteractions; they are learnt from data and given as input to the generator.\\nExtensive experiments on two data-to-text benchmarks (RotoWire and MLB) show\\nthat our approach outperforms competitive baselines in terms of automatic and\\nhuman evaluation.\\n', 'publish_date': 'Thu, 4 Feb 2021 16:32:57 GMT'}\n",
    "ID: 2004.10188, Distance: 0.40919992327690125, Content: {'title': 'Residual Energy-Based Models for Text', 'doi': None, 'categories': 'cs.CL cs.LG stat.ML', 'abstract': '  Current large-scale auto-regressive language models display impressive\\nfluency and can generate convincing text. In this work we start by asking the\\nquestion: Can the generations of these models be reliably distinguished from\\nreal text by statistical discriminators? We find experimentally that the answer\\nis affirmative when we have access to the training data for the model, and\\nguardedly affirmative even if we do not.\\n  This suggests that the auto-regressive models can be improved by\\nincorporating the (globally normalized) discriminators into the generative\\nprocess. We give a formalism for this using the Energy-Based Model framework,\\nand show that it indeed improves the results of the generative models, measured\\nboth in terms of perplexity and in terms of human evaluation.\\n', 'publish_date': 'Mon, 6 Apr 2020 13:44:03 GMT'}\n",
    "ID: 2504.08697, Distance: 0.40951859951019287, Content: {'title': 'Large Language Models as Span Annotators', 'doi': None, 'categories': 'cs.CL', 'abstract': '  For high-quality texts, single-score metrics seldom provide actionable\\nfeedback. In contrast, span annotation - pointing out issues in the text by\\nannotating their spans - can guide improvements and provide insights. Until\\nrecently, span annotation was limited to human annotators or fine-tuned encoder\\nmodels. In this study, we automate span annotation with large language models\\n(LLMs). We compare expert or skilled crowdworker annotators with open and\\nproprietary LLMs on three tasks: data-to-text generation evaluation, machine\\ntranslation evaluation, and propaganda detection in human-written texts. In our\\nexperiments, we show that LLMs as span annotators are straightforward to\\nimplement and notably more cost-efficient than human annotators. The LLMs\\nachieve moderate agreement with skilled human annotators, in some scenarios\\ncomparable to the average agreement among the annotators themselves.\\nQualitative analysis shows that reasoning models outperform their\\ninstruction-tuned counterparts and provide more valid explanations for\\nannotations. We release the dataset of more than 40k model and human\\nannotations for further research.\\n', 'publish_date': 'Fri, 11 Apr 2025 17:04:51 GMT'}\n",
    "ID: 1707.08052, Distance: 0.41169100999832153, Content: {'title': 'Challenges in Data-to-Document Generation', 'doi': None, 'categories': 'cs.CL', 'abstract': '  Recent neural models have shown significant progress on the problem of\\ngenerating short descriptive texts conditioned on a small number of database\\nrecords. In this work, we suggest a slightly more difficult data-to-text\\ngeneration task, and investigate how effective current approaches are on this\\ntask. In particular, we introduce a new, large-scale corpus of data records\\npaired with descriptive documents, propose a series of extractive evaluation\\nmethods for analyzing performance, and obtain baseline results using current\\nneural generation methods. Experiments show that these models produce fluent\\ntext, but fail to convincingly approximate human-generated documents. Moreover,\\neven templated baselines exceed the performance of these neural models on some\\nmetrics, though copy- and reconstruction-based extensions lead to noticeable\\nimprovements.\\n', 'publish_date': 'Tue, 25 Jul 2017 15:42:25 GMT'}\n",
    "\n",
    "Answer:\n",
    "\n",
    "==================================================\n",
    "生成嵌入中: 100%|██████████| 1/1 [00:00<00:00,  8.90it/s]\n",
    "[INFO] Query vector shape: (1, 1, 768)\n",
    "\n",
    "\n",
    "\n",
    "cost 0.1745 seconds\n",
    "\n",
    "\n",
    "You are a professional academic assistant specializing in the cs.CL domain. Strictly based on the provided literature information, answer the user's query. You may slightly expand and refine the response using existing knowledge, but avoid fabricating information. The response language should match the user's query language (English for English queries, Chinese for Chinese queries). Ensure the response is concise and limited to 500 words. Include a citation with the corresponding literature link (like [text](f\"https://arxiv.org/abs/{{id}}\")) and adjust the response style based on the temperature parameter (from 0.0 to 1.0). If the temperature is low (e.g., 0.2), provide a strict and precise answer. If the temperature is high (e.g., 0.8), provide a more creative and exploratory answer.\n",
    "\n",
    "Temperature: 0.2\n",
    "Question: 对于由多个大模型生成文本与人类文本混合的数据集（数据集仅含两种标签），如何区分出大模型生成的文本？\n",
    "Relevant Literature: \n",
    "ID: 1603.07771, Distance: 0.3425133228302002, Content: {'title': 'Neural Text Generation from Structured Data with Application to the\\n  Biography Domain', 'doi': None, 'categories': 'cs.CL', 'abstract': '  This paper introduces a neural model for concept-to-text generation that\\nscales to large, rich domains. We experiment with a new dataset of biographies\\nfrom Wikipedia that is an order of magnitude larger than existing resources\\nwith over 700k samples. The dataset is also vastly more diverse with a 400k\\nvocabulary, compared to a few hundred words for Weathergov or Robocup. Our\\nmodel builds upon recent work on conditional neural language model for text\\ngeneration. To deal with the large vocabulary, we extend these models to mix a\\nfixed vocabulary with copy actions that transfer sample-specific words from the\\ninput database to the generated output sentence. Our neural model significantly\\nout-performs a classical Kneser-Ney language model adapted to this task by\\nnearly 15 BLEU.\\n', 'publish_date': 'Thu, 24 Mar 2016 22:40:00 GMT'}\n",
    "ID: 1707.08052, Distance: 0.3503926396369934, Content: {'title': 'Challenges in Data-to-Document Generation', 'doi': None, 'categories': 'cs.CL', 'abstract': '  Recent neural models have shown significant progress on the problem of\\ngenerating short descriptive texts conditioned on a small number of database\\nrecords. In this work, we suggest a slightly more difficult data-to-text\\ngeneration task, and investigate how effective current approaches are on this\\ntask. In particular, we introduce a new, large-scale corpus of data records\\npaired with descriptive documents, propose a series of extractive evaluation\\nmethods for analyzing performance, and obtain baseline results using current\\nneural generation methods. Experiments show that these models produce fluent\\ntext, but fail to convincingly approximate human-generated documents. Moreover,\\neven templated baselines exceed the performance of these neural models on some\\nmetrics, though copy- and reconstruction-based extensions lead to noticeable\\nimprovements.\\n', 'publish_date': 'Tue, 25 Jul 2017 15:42:25 GMT'}\n",
    "ID: 2402.08496, Distance: 0.3582933843135834, Content: {'title': 'A Systematic Review of Data-to-Text NLG', 'doi': None, 'categories': 'cs.CL cs.AI cs.LG', 'abstract': '  This systematic review undertakes a comprehensive analysis of current\\nresearch on data-to-text generation, identifying gaps, challenges, and future\\ndirections within the field. Relevant literature in this field on datasets,\\nevaluation metrics, application areas, multilingualism, language models, and\\nhallucination mitigation methods is reviewed. Various methods for producing\\nhigh-quality text are explored, addressing the challenge of hallucinations in\\ndata-to-text generation. These methods include re-ranking, traditional and\\nneural pipeline architecture, planning architectures, data cleaning, controlled\\ngeneration, and modification of models and training techniques. Their\\neffectiveness and limitations are assessed, highlighting the need for\\nuniversally applicable strategies to mitigate hallucinations. The review also\\nexamines the usage, popularity, and impact of datasets, alongside evaluation\\nmetrics, with an emphasis on both automatic and human assessment. Additionally,\\nthe evolution of data-to-text models, particularly the widespread adoption of\\ntransformer models, is discussed. Despite advancements in text quality, the\\nreview emphasizes the importance of research in low-resourced languages and the\\nengineering of datasets in these languages to promote inclusivity. Finally,\\nseveral application domains of data-to-text are highlighted, emphasizing their\\nrelevance in such domains. Overall, this review serves as a guiding framework\\nfor fostering innovation and advancing data-to-text generation.\\n', 'publish_date': 'Tue, 13 Feb 2024 14:51:45 GMT'}\n",
    "ID: 2102.02723, Distance: 0.3691689968109131, Content: {'title': 'Data-to-text Generation with Macro Planning', 'doi': None, 'categories': 'cs.CL', 'abstract': '  Recent approaches to data-to-text generation have adopted the very successful\\nencoder-decoder architecture or variants thereof. These models generate text\\nwhich is fluent (but often imprecise) and perform quite poorly at selecting\\nappropriate content and ordering it coherently. To overcome some of these\\nissues, we propose a neural model with a macro planning stage followed by a\\ngeneration stage reminiscent of traditional methods which embrace separate\\nmodules for planning and surface realization. Macro plans represent high level\\norganization of important content such as entities, events and their\\ninteractions; they are learnt from data and given as input to the generator.\\nExtensive experiments on two data-to-text benchmarks (RotoWire and MLB) show\\nthat our approach outperforms competitive baselines in terms of automatic and\\nhuman evaluation.\\n', 'publish_date': 'Thu, 4 Feb 2021 16:32:57 GMT'}\n",
    "ID: 2004.10188, Distance: 0.3714131712913513, Content: {'title': 'Residual Energy-Based Models for Text', 'doi': None, 'categories': 'cs.CL cs.LG stat.ML', 'abstract': '  Current large-scale auto-regressive language models display impressive\\nfluency and can generate convincing text. In this work we start by asking the\\nquestion: Can the generations of these models be reliably distinguished from\\nreal text by statistical discriminators? We find experimentally that the answer\\nis affirmative when we have access to the training data for the model, and\\nguardedly affirmative even if we do not.\\n  This suggests that the auto-regressive models can be improved by\\nincorporating the (globally normalized) discriminators into the generative\\nprocess. We give a formalism for this using the Energy-Based Model framework,\\nand show that it indeed improves the results of the generative models, measured\\nboth in terms of perplexity and in terms of human evaluation.\\n', 'publish_date': 'Mon, 6 Apr 2020 13:44:03 GMT'}\n",
    "\n",
    "Answer:\n",
    "\n",
    "==================================================\n",
    "生成嵌入中: 100%|██████████| 1/1 [00:00<00:00, 12.08it/s]\n",
    "[INFO] Query vector shape: (1, 1, 768)\n",
    "\n",
    "\n",
    "cost 0.1540 seconds\n",
    "\n",
    "\n",
    "You are a professional academic assistant specializing in the cs.CL domain. Strictly based on the provided literature information, answer the user's query. You may slightly expand and refine the response using existing knowledge, but avoid fabricating information. The response language should match the user's query language (English for English queries, Chinese for Chinese queries). Ensure the response is concise and limited to 500 words. Include a citation with the corresponding literature link (like [text](f\"https://arxiv.org/abs/{{id}}\")) and adjust the response style based on the temperature parameter (from 0.0 to 1.0). If the temperature is low (e.g., 0.2), provide a strict and precise answer. If the temperature is high (e.g., 0.8), provide a more creative and exploratory answer.\n",
    "\n",
    "Temperature: 0.2\n",
    "Question: 不同大模型生成文本的检测方法是否存在差异？如何针对性地进行检测？\n",
    "Relevant Literature: \n",
    "ID: 1906.04043, Distance: 0.3639647662639618, Content: {'title': 'GLTR: Statistical Detection and Visualization of Generated Text', 'doi': None, 'categories': 'cs.CL cs.AI cs.HC cs.LG', 'abstract': '  The rapid improvement of language models has raised the specter of abuse of\\ntext generation systems. This progress motivates the development of simple\\nmethods for detecting generated text that can be used by and explained to\\nnon-experts. We develop GLTR, a tool to support humans in detecting whether a\\ntext was generated by a model. GLTR applies a suite of baseline statistical\\nmethods that can detect generation artifacts across common sampling schemes. In\\na human-subjects study, we show that the annotation scheme provided by GLTR\\nimproves the human detection-rate of fake text from 54% to 72% without any\\nprior training. GLTR is open-source and publicly deployed, and has already been\\nwidely used to detect generated outputs\\n', 'publish_date': 'Mon, 10 Jun 2019 14:52:41 GMT'}\n",
    "ID: 2004.10188, Distance: 0.3990912437438965, Content: {'title': 'Residual Energy-Based Models for Text', 'doi': None, 'categories': 'cs.CL cs.LG stat.ML', 'abstract': '  Current large-scale auto-regressive language models display impressive\\nfluency and can generate convincing text. In this work we start by asking the\\nquestion: Can the generations of these models be reliably distinguished from\\nreal text by statistical discriminators? We find experimentally that the answer\\nis affirmative when we have access to the training data for the model, and\\nguardedly affirmative even if we do not.\\n  This suggests that the auto-regressive models can be improved by\\nincorporating the (globally normalized) discriminators into the generative\\nprocess. We give a formalism for this using the Energy-Based Model framework,\\nand show that it indeed improves the results of the generative models, measured\\nboth in terms of perplexity and in terms of human evaluation.\\n', 'publish_date': 'Mon, 6 Apr 2020 13:44:03 GMT'}\n",
    "ID: 2111.09509, Distance: 0.40223774313926697, Content: {'title': 'How much do language models copy from their training data? Evaluating\\n  linguistic novelty in text generation using RAVEN', 'doi': None, 'categories': 'cs.CL', 'abstract': \"  Current language models can generate high-quality text. Are they simply\\ncopying text they have seen before, or have they learned generalizable\\nlinguistic abstractions? To tease apart these possibilities, we introduce\\nRAVEN, a suite of analyses for assessing the novelty of generated text,\\nfocusing on sequential structure (n-grams) and syntactic structure. We apply\\nthese analyses to four neural language models (an LSTM, a Transformer,\\nTransformer-XL, and GPT-2). For local structure - e.g., individual dependencies\\n- model-generated text is substantially less novel than our baseline of\\nhuman-generated text from each model's test set. For larger-scale structure -\\ne.g., overall sentence structure - model-generated text is as novel or even\\nmore novel than the human-generated baseline, but models still sometimes copy\\nsubstantially, in some cases duplicating passages over 1,000 words long from\\nthe training set. We also perform extensive manual analysis showing that\\nGPT-2's novel text is usually well-formed morphologically and syntactically but\\nhas reasonably frequent semantic issues (e.g., being self-contradictory).\\n\", 'publish_date': 'Thu, 18 Nov 2021 04:07:09 GMT'}\n",
    "ID: 1603.07771, Distance: 0.40489792823791504, Content: {'title': 'Neural Text Generation from Structured Data with Application to the\\n  Biography Domain', 'doi': None, 'categories': 'cs.CL', 'abstract': '  This paper introduces a neural model for concept-to-text generation that\\nscales to large, rich domains. We experiment with a new dataset of biographies\\nfrom Wikipedia that is an order of magnitude larger than existing resources\\nwith over 700k samples. The dataset is also vastly more diverse with a 400k\\nvocabulary, compared to a few hundred words for Weathergov or Robocup. Our\\nmodel builds upon recent work on conditional neural language model for text\\ngeneration. To deal with the large vocabulary, we extend these models to mix a\\nfixed vocabulary with copy actions that transfer sample-specific words from the\\ninput database to the generated output sentence. Our neural model significantly\\nout-performs a classical Kneser-Ney language model adapted to this task by\\nnearly 15 BLEU.\\n', 'publish_date': 'Thu, 24 Mar 2016 22:40:00 GMT'}\n",
    "ID: 2504.08697, Distance: 0.40632402896881104, Content: {'title': 'Large Language Models as Span Annotators', 'doi': None, 'categories': 'cs.CL', 'abstract': '  For high-quality texts, single-score metrics seldom provide actionable\\nfeedback. In contrast, span annotation - pointing out issues in the text by\\nannotating their spans - can guide improvements and provide insights. Until\\nrecently, span annotation was limited to human annotators or fine-tuned encoder\\nmodels. In this study, we automate span annotation with large language models\\n(LLMs). We compare expert or skilled crowdworker annotators with open and\\nproprietary LLMs on three tasks: data-to-text generation evaluation, machine\\ntranslation evaluation, and propaganda detection in human-written texts. In our\\nexperiments, we show that LLMs as span annotators are straightforward to\\nimplement and notably more cost-efficient than human annotators. The LLMs\\nachieve moderate agreement with skilled human annotators, in some scenarios\\ncomparable to the average agreement among the annotators themselves.\\nQualitative analysis shows that reasoning models outperform their\\ninstruction-tuned counterparts and provide more valid explanations for\\nannotations. We release the dataset of more than 40k model and human\\nannotations for further research.\\n', 'publish_date': 'Fri, 11 Apr 2025 17:04:51 GMT'}\n",
    "\n",
    "Answer:\n",
    "\n",
    "==================================================\n",
    "生成嵌入中: 100%|██████████| 1/1 [00:00<00:00,  9.47it/s]\n",
    "[INFO] Query vector shape: (1, 1, 768)\n",
    "\n",
    "\n",
    "cost 0.1734 seconds\n",
    "\n",
    "\n",
    "You are a professional academic assistant specializing in the cs.CL domain. Strictly based on the provided literature information, answer the user's query. You may slightly expand and refine the response using existing knowledge, but avoid fabricating information. The response language should match the user's query language (English for English queries, Chinese for Chinese queries). Ensure the response is concise and limited to 500 words. Include a citation with the corresponding literature link (like [text](f\"https://arxiv.org/abs/{{id}}\")) and adjust the response style based on the temperature parameter (from 0.0 to 1.0). If the temperature is low (e.g., 0.2), provide a strict and precise answer. If the temperature is high (e.g., 0.8), provide a more creative and exploratory answer.\n",
    "\n",
    "Temperature: 0.2\n",
    "Question: 在大语言模型生成文本检测任务中，如何设计有效的特征提取方法，以区分大模型生成文本与人工编写文本的细微差别？\n",
    "Relevant Literature: \n",
    "ID: 1906.04043, Distance: 0.38884851336479187, Content: {'title': 'GLTR: Statistical Detection and Visualization of Generated Text', 'doi': None, 'categories': 'cs.CL cs.AI cs.HC cs.LG', 'abstract': '  The rapid improvement of language models has raised the specter of abuse of\\ntext generation systems. This progress motivates the development of simple\\nmethods for detecting generated text that can be used by and explained to\\nnon-experts. We develop GLTR, a tool to support humans in detecting whether a\\ntext was generated by a model. GLTR applies a suite of baseline statistical\\nmethods that can detect generation artifacts across common sampling schemes. In\\na human-subjects study, we show that the annotation scheme provided by GLTR\\nimproves the human detection-rate of fake text from 54% to 72% without any\\nprior training. GLTR is open-source and publicly deployed, and has already been\\nwidely used to detect generated outputs\\n', 'publish_date': 'Mon, 10 Jun 2019 14:52:41 GMT'}\n",
    "ID: 2402.14873, Distance: 0.4141862988471985, Content: {'title': 'Technical Report on the Pangram AI-Generated Text Classifier', 'doi': None, 'categories': 'cs.CL cs.AI', 'abstract': '  We present Pangram Text, a transformer-based neural network trained to\\ndistinguish text written by large language models from text written by humans.\\nPangram Text outperforms zero-shot methods such as DetectGPT as well as leading\\ncommercial AI detection tools with over 38 times lower error rates on a\\ncomprehensive benchmark comprised of 10 text domains (student writing, creative\\nwriting, scientific writing, books, encyclopedias, news, email, scientific\\npapers, short-form Q&A) and 8 open- and closed-source large language models. We\\npropose a training algorithm, hard negative mining with synthetic mirrors, that\\nenables our classifier to achieve orders of magnitude lower false positive\\nrates on high-data domains such as reviews. Finally, we show that Pangram Text\\nis not biased against nonnative English speakers and generalizes to domains and\\nmodels unseen during training.\\n', 'publish_date': 'Wed, 21 Feb 2024 17:13:41 GMT'}\n",
    "ID: 2004.10188, Distance: 0.42217177152633667, Content: {'title': 'Residual Energy-Based Models for Text', 'doi': None, 'categories': 'cs.CL cs.LG stat.ML', 'abstract': '  Current large-scale auto-regressive language models display impressive\\nfluency and can generate convincing text. In this work we start by asking the\\nquestion: Can the generations of these models be reliably distinguished from\\nreal text by statistical discriminators? We find experimentally that the answer\\nis affirmative when we have access to the training data for the model, and\\nguardedly affirmative even if we do not.\\n  This suggests that the auto-regressive models can be improved by\\nincorporating the (globally normalized) discriminators into the generative\\nprocess. We give a formalism for this using the Energy-Based Model framework,\\nand show that it indeed improves the results of the generative models, measured\\nboth in terms of perplexity and in terms of human evaluation.\\n', 'publish_date': 'Mon, 6 Apr 2020 13:44:03 GMT'}\n",
    "ID: 1603.07771, Distance: 0.4227270185947418, Content: {'title': 'Neural Text Generation from Structured Data with Application to the\\n  Biography Domain', 'doi': None, 'categories': 'cs.CL', 'abstract': '  This paper introduces a neural model for concept-to-text generation that\\nscales to large, rich domains. We experiment with a new dataset of biographies\\nfrom Wikipedia that is an order of magnitude larger than existing resources\\nwith over 700k samples. The dataset is also vastly more diverse with a 400k\\nvocabulary, compared to a few hundred words for Weathergov or Robocup. Our\\nmodel builds upon recent work on conditional neural language model for text\\ngeneration. To deal with the large vocabulary, we extend these models to mix a\\nfixed vocabulary with copy actions that transfer sample-specific words from the\\ninput database to the generated output sentence. Our neural model significantly\\nout-performs a classical Kneser-Ney language model adapted to this task by\\nnearly 15 BLEU.\\n', 'publish_date': 'Thu, 24 Mar 2016 22:40:00 GMT'}\n",
    "ID: 1707.08052, Distance: 0.4247818887233734, Content: {'title': 'Challenges in Data-to-Document Generation', 'doi': None, 'categories': 'cs.CL', 'abstract': '  Recent neural models have shown significant progress on the problem of\\ngenerating short descriptive texts conditioned on a small number of database\\nrecords. In this work, we suggest a slightly more difficult data-to-text\\ngeneration task, and investigate how effective current approaches are on this\\ntask. In particular, we introduce a new, large-scale corpus of data records\\npaired with descriptive documents, propose a series of extractive evaluation\\nmethods for analyzing performance, and obtain baseline results using current\\nneural generation methods. Experiments show that these models produce fluent\\ntext, but fail to convincingly approximate human-generated documents. Moreover,\\neven templated baselines exceed the performance of these neural models on some\\nmetrics, though copy- and reconstruction-based extensions lead to noticeable\\nimprovements.\\n', 'publish_date': 'Tue, 25 Jul 2017 15:42:25 GMT'}\n",
    "\n",
    "Answer:\n",
    "\n",
    "==================================================\n",
    "生成嵌入中: 100%|██████████| 1/1 [00:00<00:00, 11.21it/s]\n",
    "[INFO] Query vector shape: (1, 1, 768)\n",
    "\n",
    "\n",
    "cost 0.1639 seconds\n",
    "\n",
    "\n",
    "You are a professional academic assistant specializing in the cs.CL domain. Strictly based on the provided literature information, answer the user's query. You may slightly expand and refine the response using existing knowledge, but avoid fabricating information. The response language should match the user's query language (English for English queries, Chinese for Chinese queries). Ensure the response is concise and limited to 500 words. Include a citation with the corresponding literature link (like [text](f\"https://arxiv.org/abs/{{id}}\")) and adjust the response style based on the temperature parameter (from 0.0 to 1.0). If the temperature is low (e.g., 0.2), provide a strict and precise answer. If the temperature is high (e.g., 0.8), provide a more creative and exploratory answer.\n",
    "\n",
    "Temperature: 0.2\n",
    "Question: 是否可以利用对抗训练等技术提高大模型生成文本的可检测性，如果可以应该如何实现？\n",
    "Relevant Literature: \n",
    "ID: 2004.08994, Distance: 0.33217766880989075, Content: {'title': 'Adversarial Training for Large Neural Language Models', 'doi': None, 'categories': 'cs.CL', 'abstract': '  Generalization and robustness are both key desiderata for designing machine\\nlearning methods. Adversarial training can enhance robustness, but past work\\noften finds it hurts generalization. In natural language processing (NLP),\\npre-training large neural language models such as BERT have demonstrated\\nimpressive gain in generalization for a variety of tasks, with further\\nimprovement from adversarial fine-tuning. However, these models are still\\nvulnerable to adversarial attacks. In this paper, we show that adversarial\\npre-training can improve both generalization and robustness. We propose a\\ngeneral algorithm ALUM (Adversarial training for large neural LangUage Models),\\nwhich regularizes the training objective by applying perturbations in the\\nembedding space that maximizes the adversarial loss. We present the first\\ncomprehensive study of adversarial training in all stages, including\\npre-training from scratch, continual pre-training on a well-trained model, and\\ntask-specific fine-tuning. ALUM obtains substantial gains over BERT on a wide\\nrange of NLP tasks, in both regular and adversarial scenarios. Even for models\\nthat have been well trained on extremely large text corpora, such as RoBERTa,\\nALUM can still produce significant gains from continual pre-training, whereas\\nconventional non-adversarial methods can not. ALUM can be further combined with\\ntask-specific fine-tuning to attain additional gains. The ALUM code is publicly\\navailable at https://github.com/namisan/mt-dnn.\\n', 'publish_date': 'Mon, 20 Apr 2020 00:07:18 GMT'}\n",
    "ID: 2406.08050, Distance: 0.3420890271663666, Content: {'title': 'Adversarial Evasion Attack Efficiency against Large Language Models', 'doi': None, 'categories': 'cs.CL cs.LG', 'abstract': '  Large Language Models (LLMs) are valuable for text classification, but their\\nvulnerabilities must not be disregarded. They lack robustness against\\nadversarial examples, so it is pertinent to understand the impacts of different\\ntypes of perturbations, and assess if those attacks could be replicated by\\ncommon users with a small amount of perturbations and a small number of queries\\nto a deployed LLM. This work presents an analysis of the effectiveness,\\nefficiency, and practicality of three different types of adversarial attacks\\nagainst five different LLMs in a sentiment classification task. The obtained\\nresults demonstrated the very distinct impacts of the word-level and\\ncharacter-level attacks. The word attacks were more effective, but the\\ncharacter and more constrained attacks were more practical and required a\\nreduced number of perturbations and queries. These differences need to be\\nconsidered during the development of adversarial defense strategies to train\\nmore robust LLMs for intelligent text classification applications.\\n', 'publish_date': 'Wed, 12 Jun 2024 10:02:27 GMT'}\n",
    "ID: 1910.04618, Distance: 0.3452152609825134, Content: {'title': 'Universal Adversarial Perturbation for Text Classification', 'doi': None, 'categories': 'cs.CL cs.LG stat.ML', 'abstract': '  Given a state-of-the-art deep neural network text classifier, we show the\\nexistence of a universal and very small perturbation vector (in the embedding\\nspace) that causes natural text to be misclassified with high probability.\\nUnlike images on which a single fixed-size adversarial perturbation can be\\nfound, text is of variable length, so we define the \"universality\" as\\n\"token-agnostic\", where a single perturbation is applied to each token,\\nresulting in different perturbations of flexible sizes at the sequence level.\\nWe propose an algorithm to compute universal adversarial perturbations, and\\nshow that the state-of-the-art deep neural networks are highly vulnerable to\\nthem, even though they keep the neighborhood of tokens mostly preserved. We\\nalso show how to use these adversarial perturbations to generate adversarial\\ntext samples. The surprising existence of universal \"token-agnostic\"\\nadversarial perturbations may reveal important properties of a text classifier.\\n', 'publish_date': 'Thu, 10 Oct 2019 14:48:22 GMT'}\n",
    "ID: 1906.03805, Distance: 0.3533552289009094, Content: {'title': 'Improving Neural Language Modeling via Adversarial Training', 'doi': None, 'categories': 'cs.LG cs.CL stat.ML', 'abstract': '  Recently, substantial progress has been made in language modeling by using\\ndeep neural networks. However, in practice, large scale neural language models\\nhave been shown to be prone to overfitting. In this paper, we present a simple\\nyet highly effective adversarial training mechanism for regularizing neural\\nlanguage models. The idea is to introduce adversarial noise to the output\\nembedding layer while training the models. We show that the optimal adversarial\\nnoise yields a simple closed-form solution, thus allowing us to develop a\\nsimple and time efficient algorithm. Theoretically, we show that our\\nadversarial mechanism effectively encourages the diversity of the embedding\\nvectors, helping to increase the robustness of models. Empirically, we show\\nthat our method improves on the single model state-of-the-art results for\\nlanguage modeling on Penn Treebank (PTB) and Wikitext-2, achieving test\\nperplexity scores of 46.01 and 38.07, respectively. When applied to machine\\ntranslation, our method improves over various transformer-based translation\\nbaselines in BLEU scores on the WMT14 English-German and IWSLT14 German-English\\ntasks.\\n', 'publish_date': 'Mon, 10 Jun 2019 05:55:08 GMT'}\n",
    "ID: 2311.01873, Distance: 0.35835573077201843, Content: {'title': 'Efficient Black-Box Adversarial Attacks on Neural Text Detectors', 'doi': None, 'categories': 'cs.CL', 'abstract': '  Neural text detectors are models trained to detect whether a given text was\\ngenerated by a language model or written by a human. In this paper, we\\ninvestigate three simple and resource-efficient strategies (parameter tweaking,\\nprompt engineering, and character-level mutations) to alter texts generated by\\nGPT-3.5 that are unsuspicious or unnoticeable for humans but cause\\nmisclassification by neural text detectors. The results show that especially\\nparameter tweaking and character-level mutations are effective strategies.\\n', 'publish_date': 'Fri, 3 Nov 2023 12:29:32 GMT'}\n",
    "\n",
    "Answer:\n",
    "\n",
    "==================================================\n",
    "生成嵌入中: 100%|██████████| 1/1 [00:00<00:00, 11.78it/s]\n",
    "[INFO] Query vector shape: (1, 1, 768)\n",
    "\n",
    "\n",
    "\n",
    "cost 0.1588 seconds\n",
    "\n",
    "\n",
    "You are a professional academic assistant specializing in the cs.CL domain. Strictly based on the provided literature information, answer the user's query. You may slightly expand and refine the response using existing knowledge, but avoid fabricating information. The response language should match the user's query language (English for English queries, Chinese for Chinese queries). Ensure the response is concise and limited to 500 words. Include a citation with the corresponding literature link (like [text](f\"https://arxiv.org/abs/{{id}}\")) and adjust the response style based on the temperature parameter (from 0.0 to 1.0). If the temperature is low (e.g., 0.2), provide a strict and precise answer. If the temperature is high (e.g., 0.8), provide a more creative and exploratory answer.\n",
    "\n",
    "Temperature: 0.2\n",
    "Question: 如何构建大规模、高质量的大模型生成文本检测数据集？\n",
    "Relevant Literature: \n",
    "ID: 1707.08052, Distance: 0.3967449963092804, Content: {'title': 'Challenges in Data-to-Document Generation', 'doi': None, 'categories': 'cs.CL', 'abstract': '  Recent neural models have shown significant progress on the problem of\\ngenerating short descriptive texts conditioned on a small number of database\\nrecords. In this work, we suggest a slightly more difficult data-to-text\\ngeneration task, and investigate how effective current approaches are on this\\ntask. In particular, we introduce a new, large-scale corpus of data records\\npaired with descriptive documents, propose a series of extractive evaluation\\nmethods for analyzing performance, and obtain baseline results using current\\nneural generation methods. Experiments show that these models produce fluent\\ntext, but fail to convincingly approximate human-generated documents. Moreover,\\neven templated baselines exceed the performance of these neural models on some\\nmetrics, though copy- and reconstruction-based extensions lead to noticeable\\nimprovements.\\n', 'publish_date': 'Tue, 25 Jul 2017 15:42:25 GMT'}\n",
    "ID: 1906.04043, Distance: 0.3976016044616699, Content: {'title': 'GLTR: Statistical Detection and Visualization of Generated Text', 'doi': None, 'categories': 'cs.CL cs.AI cs.HC cs.LG', 'abstract': '  The rapid improvement of language models has raised the specter of abuse of\\ntext generation systems. This progress motivates the development of simple\\nmethods for detecting generated text that can be used by and explained to\\nnon-experts. We develop GLTR, a tool to support humans in detecting whether a\\ntext was generated by a model. GLTR applies a suite of baseline statistical\\nmethods that can detect generation artifacts across common sampling schemes. In\\na human-subjects study, we show that the annotation scheme provided by GLTR\\nimproves the human detection-rate of fake text from 54% to 72% without any\\nprior training. GLTR is open-source and publicly deployed, and has already been\\nwidely used to detect generated outputs\\n', 'publish_date': 'Mon, 10 Jun 2019 14:52:41 GMT'}\n",
    "ID: 2401.03946, Distance: 0.40528881549835205, Content: {'title': 'TextMachina: Seamless Generation of Machine-Generated Text Datasets', 'doi': None, 'categories': 'cs.CL', 'abstract': '  Recent advancements in Large Language Models (LLMs) have led to high-quality\\nMachine-Generated Text (MGT), giving rise to countless new use cases and\\napplications. However, easy access to LLMs is posing new challenges due to\\nmisuse. To address malicious usage, researchers have released datasets to\\neffectively train models on MGT-related tasks. Similar strategies are used to\\ncompile these datasets, but no tool currently unifies them. In this scenario,\\nwe introduce TextMachina, a modular and extensible Python framework, designed\\nto aid in the creation of high-quality, unbiased datasets to build robust\\nmodels for MGT-related tasks such as detection, attribution, mixcase, or\\nboundary detection. It provides a user-friendly pipeline that abstracts away\\nthe inherent intricacies of building MGT datasets, such as LLM integrations,\\nprompt templating, and bias mitigation. The quality of the datasets generated\\nby TextMachina has been assessed in previous works, including shared tasks\\nwhere more than one hundred teams trained robust MGT detectors.\\n', 'publish_date': 'Mon, 8 Jan 2024 15:05:32 GMT'}\n",
    "ID: 1809.00582, Distance: 0.4073987901210785, Content: {'title': 'Data-to-Text Generation with Content Selection and Planning', 'doi': None, 'categories': 'cs.CL', 'abstract': '  Recent advances in data-to-text generation have led to the use of large-scale\\ndatasets and neural network models which are trained end-to-end, without\\nexplicitly modeling what to say and in what order. In this work, we present a\\nneural network architecture which incorporates content selection and planning\\nwithout sacrificing end-to-end training. We decompose the generation task into\\ntwo stages. Given a corpus of data records (paired with descriptive documents),\\nwe first generate a content plan highlighting which information should be\\nmentioned and in which order and then generate the document while taking the\\ncontent plan into account. Automatic and human-based evaluation experiments\\nshow that our model outperforms strong baselines improving the state-of-the-art\\non the recently released RotoWire dataset.\\n', 'publish_date': 'Mon, 3 Sep 2018 12:41:44 GMT'}\n",
    "ID: 2402.08496, Distance: 0.4106740951538086, Content: {'title': 'A Systematic Review of Data-to-Text NLG', 'doi': None, 'categories': 'cs.CL cs.AI cs.LG', 'abstract': '  This systematic review undertakes a comprehensive analysis of current\\nresearch on data-to-text generation, identifying gaps, challenges, and future\\ndirections within the field. Relevant literature in this field on datasets,\\nevaluation metrics, application areas, multilingualism, language models, and\\nhallucination mitigation methods is reviewed. Various methods for producing\\nhigh-quality text are explored, addressing the challenge of hallucinations in\\ndata-to-text generation. These methods include re-ranking, traditional and\\nneural pipeline architecture, planning architectures, data cleaning, controlled\\ngeneration, and modification of models and training techniques. Their\\neffectiveness and limitations are assessed, highlighting the need for\\nuniversally applicable strategies to mitigate hallucinations. The review also\\nexamines the usage, popularity, and impact of datasets, alongside evaluation\\nmetrics, with an emphasis on both automatic and human assessment. Additionally,\\nthe evolution of data-to-text models, particularly the widespread adoption of\\ntransformer models, is discussed. Despite advancements in text quality, the\\nreview emphasizes the importance of research in low-resourced languages and the\\nengineering of datasets in these languages to promote inclusivity. Finally,\\nseveral application domains of data-to-text are highlighted, emphasizing their\\nrelevance in such domains. Overall, this review serves as a guiding framework\\nfor fostering innovation and advancing data-to-text generation.\\n', 'publish_date': 'Tue, 13 Feb 2024 14:51:45 GMT'}\n",
    "\n",
    "Answer:\n",
    "\n",
    "==================================================\n",
    "生成嵌入中: 100%|██████████| 1/1 [00:00<00:00, 11.35it/s]\n",
    "[INFO] Query vector shape: (1, 1, 768)\n",
    "\n",
    "\n",
    "cost 0.1610 seconds\n",
    "\n",
    "\n",
    "You are a professional academic assistant specializing in the cs.CL domain. Strictly based on the provided literature information, answer the user's query. You may slightly expand and refine the response using existing knowledge, but avoid fabricating information. The response language should match the user's query language (English for English queries, Chinese for Chinese queries). Ensure the response is concise and limited to 500 words. Include a citation with the corresponding literature link (like [text](f\"https://arxiv.org/abs/{{id}}\")) and adjust the response style based on the temperature parameter (from 0.0 to 1.0). If the temperature is low (e.g., 0.2), provide a strict and precise answer. If the temperature is high (e.g., 0.8), provide a more creative and exploratory answer.\n",
    "\n",
    "Temperature: 0.2\n",
    "Question: 如何应对大模型生成文本不断优化带来的检测挑战？\n",
    "Relevant Literature: \n",
    "ID: 1603.07771, Distance: 0.37713485956192017, Content: {'title': 'Neural Text Generation from Structured Data with Application to the\\n  Biography Domain', 'doi': None, 'categories': 'cs.CL', 'abstract': '  This paper introduces a neural model for concept-to-text generation that\\nscales to large, rich domains. We experiment with a new dataset of biographies\\nfrom Wikipedia that is an order of magnitude larger than existing resources\\nwith over 700k samples. The dataset is also vastly more diverse with a 400k\\nvocabulary, compared to a few hundred words for Weathergov or Robocup. Our\\nmodel builds upon recent work on conditional neural language model for text\\ngeneration. To deal with the large vocabulary, we extend these models to mix a\\nfixed vocabulary with copy actions that transfer sample-specific words from the\\ninput database to the generated output sentence. Our neural model significantly\\nout-performs a classical Kneser-Ney language model adapted to this task by\\nnearly 15 BLEU.\\n', 'publish_date': 'Thu, 24 Mar 2016 22:40:00 GMT'}\n",
    "ID: 1706.09433, Distance: 0.3856700658798218, Content: {'title': 'Data-driven Natural Language Generation: Paving the Road to Success', 'doi': None, 'categories': 'cs.CL', 'abstract': '  We argue that there are currently two major bottlenecks to the commercial use\\nof statistical machine learning approaches for natural language generation\\n(NLG): (a) The lack of reliable automatic evaluation metrics for NLG, and (b)\\nThe scarcity of high quality in-domain corpora. We address the first problem by\\nthoroughly analysing current evaluation metrics and motivating the need for a\\nnew, more reliable metric. The second problem is addressed by presenting a\\nnovel framework for developing and evaluating a high quality corpus for NLG\\ntraining.\\n', 'publish_date': 'Wed, 28 Jun 2017 18:17:30 GMT'}\n",
    "ID: 1906.04043, Distance: 0.3857758343219757, Content: {'title': 'GLTR: Statistical Detection and Visualization of Generated Text', 'doi': None, 'categories': 'cs.CL cs.AI cs.HC cs.LG', 'abstract': '  The rapid improvement of language models has raised the specter of abuse of\\ntext generation systems. This progress motivates the development of simple\\nmethods for detecting generated text that can be used by and explained to\\nnon-experts. We develop GLTR, a tool to support humans in detecting whether a\\ntext was generated by a model. GLTR applies a suite of baseline statistical\\nmethods that can detect generation artifacts across common sampling schemes. In\\na human-subjects study, we show that the annotation scheme provided by GLTR\\nimproves the human detection-rate of fake text from 54% to 72% without any\\nprior training. GLTR is open-source and publicly deployed, and has already been\\nwidely used to detect generated outputs\\n', 'publish_date': 'Mon, 10 Jun 2019 14:52:41 GMT'}\n",
    "ID: 2504.08697, Distance: 0.39249658584594727, Content: {'title': 'Large Language Models as Span Annotators', 'doi': None, 'categories': 'cs.CL', 'abstract': '  For high-quality texts, single-score metrics seldom provide actionable\\nfeedback. In contrast, span annotation - pointing out issues in the text by\\nannotating their spans - can guide improvements and provide insights. Until\\nrecently, span annotation was limited to human annotators or fine-tuned encoder\\nmodels. In this study, we automate span annotation with large language models\\n(LLMs). We compare expert or skilled crowdworker annotators with open and\\nproprietary LLMs on three tasks: data-to-text generation evaluation, machine\\ntranslation evaluation, and propaganda detection in human-written texts. In our\\nexperiments, we show that LLMs as span annotators are straightforward to\\nimplement and notably more cost-efficient than human annotators. The LLMs\\nachieve moderate agreement with skilled human annotators, in some scenarios\\ncomparable to the average agreement among the annotators themselves.\\nQualitative analysis shows that reasoning models outperform their\\ninstruction-tuned counterparts and provide more valid explanations for\\nannotations. We release the dataset of more than 40k model and human\\nannotations for further research.\\n', 'publish_date': 'Fri, 11 Apr 2025 17:04:51 GMT'}\n",
    "ID: 2102.02723, Distance: 0.3936072289943695, Content: {'title': 'Data-to-text Generation with Macro Planning', 'doi': None, 'categories': 'cs.CL', 'abstract': '  Recent approaches to data-to-text generation have adopted the very successful\\nencoder-decoder architecture or variants thereof. These models generate text\\nwhich is fluent (but often imprecise) and perform quite poorly at selecting\\nappropriate content and ordering it coherently. To overcome some of these\\nissues, we propose a neural model with a macro planning stage followed by a\\ngeneration stage reminiscent of traditional methods which embrace separate\\nmodules for planning and surface realization. Macro plans represent high level\\norganization of important content such as entities, events and their\\ninteractions; they are learnt from data and given as input to the generator.\\nExtensive experiments on two data-to-text benchmarks (RotoWire and MLB) show\\nthat our approach outperforms competitive baselines in terms of automatic and\\nhuman evaluation.\\n', 'publish_date': 'Thu, 4 Feb 2021 16:32:57 GMT'}\n",
    "\n",
    "Answer:\n",
    "\n",
    "==================================================\n",
    "生成嵌入中: 100%|██████████| 1/1 [00:00<00:00, 10.13it/s]\n",
    "[INFO] Query vector shape: (1, 1, 768)\n",
    "\n",
    "\n",
    "cost 0.1676 seconds\n",
    "\n",
    "\n",
    "You are a professional academic assistant specializing in the cs.CL domain. Strictly based on the provided literature information, answer the user's query. You may slightly expand and refine the response using existing knowledge, but avoid fabricating information. The response language should match the user's query language (English for English queries, Chinese for Chinese queries). Ensure the response is concise and limited to 500 words. Include a citation with the corresponding literature link (like [text](f\"https://arxiv.org/abs/{{id}}\")) and adjust the response style based on the temperature parameter (from 0.0 to 1.0). If the temperature is low (e.g., 0.2), provide a strict and precise answer. If the temperature is high (e.g., 0.8), provide a more creative and exploratory answer.\n",
    "\n",
    "Temperature: 0.2\n",
    "Question: 如何构建一个能够自适应不同领域和风格的文本生成模型？\n",
    "Relevant Literature: \n",
    "ID: cs/9812018, Distance: 0.39264920353889465, Content: {'title': 'A Flexible Shallow Approach to Text Generation', 'doi': None, 'categories': 'cs.CL', 'abstract': '  In order to support the efficient development of NL generation systems, two\\northogonal methods are currently pursued with emphasis: (1) reusable, general,\\nand linguistically motivated surface realization components, and (2) simple,\\ntask-oriented template-based techniques. In this paper we argue that, from an\\napplication-oriented perspective, the benefits of both are still limited. In\\norder to improve this situation, we suggest and evaluate shallow generation\\nmethods associated with increased flexibility. We advise a close connection\\nbetween domain-motivated and linguistic ontologies that supports the quick\\nadaptation to new tasks and domains, rather than the reuse of general\\nresources. Our method is especially designed for generating reports with\\nlimited linguistic variations.\\n', 'publish_date': 'Wed, 16 Dec 1998 16:37:01 GMT'}\n",
    "ID: 2307.09702, Distance: 0.3937809467315674, Content: {'title': 'Efficient Guided Generation for Large Language Models', 'doi': None, 'categories': 'cs.CL cs.LG', 'abstract': \"  In this article we show how the problem of neural text generation can be\\nconstructively reformulated in terms of transitions between the states of a\\nfinite-state machine. This framework leads to an efficient approach to guiding\\ntext generation with regular expressions and context-free grammars by allowing\\nthe construction of an index over a language model's vocabulary. The approach\\nis model agnostic, allows one to enforce domain-specific knowledge and\\nconstraints, and enables the construction of reliable interfaces by\\nguaranteeing the structure of the generated text. It adds little overhead to\\nthe token sequence generation process and significantly outperforms existing\\nsolutions. An implementation is provided in the open source Python library\\nOutlines\\n\", 'publish_date': 'Wed, 19 Jul 2023 01:14:49 GMT'}\n",
    "ID: 2005.01822, Distance: 0.4125930070877075, Content: {'title': 'Exploring Controllable Text Generation Techniques', 'doi': None, 'categories': 'cs.CL', 'abstract': '  Neural controllable text generation is an important area gaining attention\\ndue to its plethora of applications. Although there is a large body of prior\\nwork in controllable text generation, there is no unifying theme. In this work,\\nwe provide a new schema of the pipeline of the generation process by\\nclassifying it into five modules. The control of attributes in the generation\\nprocess requires modification of these modules. We present an overview of\\ndifferent techniques used to perform the modulation of these modules. We also\\nprovide an analysis on the advantages and disadvantages of these techniques. We\\nfurther pave ways to develop new architectures based on the combination of the\\nmodules described in this paper.\\n', 'publish_date': 'Mon, 4 May 2020 20:04:47 GMT'}\n",
    "ID: 2404.07117, Distance: 0.42169976234436035, Content: {'title': 'Continuous Language Model Interpolation for Dynamic and Controllable\\n  Text Generation', 'doi': None, 'categories': 'cs.CL cs.LG', 'abstract': '  As large language models (LLMs) have gained popularity for a variety of use\\ncases, making them adaptable and controllable has become increasingly\\nimportant, especially for user-facing applications. While the existing\\nliterature on LLM adaptation primarily focuses on finding a model (or models)\\nthat optimizes a single predefined objective, here we focus on the challenging\\ncase where the model must dynamically adapt to diverse -- and often changing --\\nuser preferences. For this, we leverage adaptation methods based on linear\\nweight interpolation, casting them as continuous multi-domain interpolators\\nthat produce models with specific prescribed generation characteristics\\non-the-fly. Specifically, we use low-rank updates to fine-tune a base model to\\nvarious different domains, yielding a set of anchor models with distinct\\ngeneration profiles. Then, we use the weight updates of these anchor models to\\nparametrize the entire (infinite) class of models contained within their convex\\nhull. We empirically show that varying the interpolation weights yields\\npredictable and consistent change in the model outputs with respect to all of\\nthe controlled attributes. We find that there is little entanglement between\\nmost attributes and identify and discuss the pairs of attributes for which this\\nis not the case. Our results suggest that linearly interpolating between the\\nweights of fine-tuned models facilitates predictable, fine-grained control of\\nmodel outputs with respect to multiple stylistic characteristics\\nsimultaneously.\\n', 'publish_date': 'Wed, 10 Apr 2024 15:55:07 GMT'}\n",
    "ID: 1909.00734, Distance: 0.4312938153743744, Content: {'title': 'Sentence-Level Content Planning and Style Specification for Neural Text\\n  Generation', 'doi': None, 'categories': 'cs.CL', 'abstract': '  Building effective text generation systems requires three critical\\ncomponents: content selection, text planning, and surface realization, and\\ntraditionally they are tackled as separate problems. Recent all-in-one style\\nneural generation models have made impressive progress, yet they often produce\\noutputs that are incoherent and unfaithful to the input. To address these\\nissues, we present an end-to-end trained two-step generation model, where a\\nsentence-level content planner first decides on the keyphrases to cover as well\\nas a desired language style, followed by a surface realization decoder that\\ngenerates relevant and coherent text. For experiments, we consider three tasks\\nfrom domains with diverse topics and varying language styles: persuasive\\nargument construction from Reddit, paragraph generation for normal and simple\\nversions of Wikipedia, and abstract generation for scientific articles.\\nAutomatic evaluation shows that our system can significantly outperform\\ncompetitive comparisons. Human judges further rate our system generated text as\\nmore fluent and correct, compared to the generations by its variants that do\\nnot consider language style.\\n', 'publish_date': 'Mon, 2 Sep 2019 14:29:36 GMT'}\n",
    "\n",
    "Answer:\n",
    "\n",
    "==================================================\n",
    "生成嵌入中: 100%|██████████| 1/1 [00:00<00:00, 11.36it/s]\n",
    "[INFO] Query vector shape: (1, 1, 768)\n",
    "\n",
    "\n",
    "cost 0.1617 seconds\n",
    "\n",
    "\n",
    "You are a professional academic assistant specializing in the cs.CL domain. Strictly based on the provided literature information, answer the user's query. You may slightly expand and refine the response using existing knowledge, but avoid fabricating information. The response language should match the user's query language (English for English queries, Chinese for Chinese queries). Ensure the response is concise and limited to 500 words. Include a citation with the corresponding literature link (like [text](f\"https://arxiv.org/abs/{{id}}\")) and adjust the response style based on the temperature parameter (from 0.0 to 1.0). If the temperature is low (e.g., 0.2), provide a strict and precise answer. If the temperature is high (e.g., 0.8), provide a more creative and exploratory answer.\n",
    "\n",
    "Temperature: 0.2\n",
    "Question: 在文本分类任务中，如何处理文本数据中的噪声和异常值，以提高模型的性能与鲁棒性？\n",
    "Relevant Literature: \n",
    "ID: 1904.08067, Distance: 0.3456064760684967, Content: {'title': 'Text Classification Algorithms: A Survey', 'doi': '10.3390/info10040150', 'categories': 'cs.LG cs.AI cs.CL cs.IR stat.ML', 'abstract': '  In recent years, there has been an exponential growth in the number of\\ncomplex documents and texts that require a deeper understanding of machine\\nlearning methods to be able to accurately classify texts in many applications.\\nMany machine learning approaches have achieved surpassing results in natural\\nlanguage processing. The success of these learning algorithms relies on their\\ncapacity to understand complex models and non-linear relationships within data.\\nHowever, finding suitable structures, architectures, and techniques for text\\nclassification is a challenge for researchers. In this paper, a brief overview\\nof text classification algorithms is discussed. This overview covers different\\ntext feature extractions, dimensionality reduction methods, existing algorithms\\nand techniques, and evaluations methods. Finally, the limitations of each\\ntechnique and their application in the real-world problem are discussed.\\n', 'publish_date': 'Wed, 17 Apr 2019 03:29:05 GMT'}\n",
    "ID: 2502.19801, Distance: 0.4182180166244507, Content: {'title': 'Text classification using machine learning methods', 'doi': None, 'categories': 'cs.CL cs.LG', 'abstract': '  In this paper we present the results of an experiment aimed to use machine\\nlearning methods to obtain models that can be used for the automatic\\nclassification of products. In order to apply automatic classification methods,\\nwe transformed the product names from a text representation to numeric vectors,\\na process called word embedding. We used several embedding methods: Count\\nVectorization, TF-IDF, Word2Vec, FASTTEXT, and GloVe. Having the product names\\nin a form of numeric vectors, we proceeded with a set of machine learning\\nmethods for automatic classification: Logistic Regression, Multinomial Naive\\nBayes, kNN, Artificial Neural Networks, Support Vector Machines, and Decision\\ntrees with several variants. The results show an impressive accuracy of the\\nclassification process for Support Vector Machines, Logistic Regression, and\\nRandom Forests. Regarding the word embedding methods, the best results were\\nobtained with the FASTTEXT technique.\\n', 'publish_date': 'Thu, 27 Feb 2025 06:20:38 GMT'}\n",
    "ID: 2010.02458, Distance: 0.41830217838287354, Content: {'title': 'Identifying Spurious Correlations for Robust Text Classification', 'doi': None, 'categories': 'cs.LG cs.CL cs.IR', 'abstract': '  The predictions of text classifiers are often driven by spurious correlations\\n-- e.g., the term `Spielberg\\' correlates with positively reviewed movies, even\\nthough the term itself does not semantically convey a positive sentiment. In\\nthis paper, we propose a method to distinguish spurious and genuine\\ncorrelations in text classification. We treat this as a supervised\\nclassification problem, using features derived from treatment effect estimators\\nto distinguish spurious correlations from \"genuine\" ones. Due to the generic\\nnature of these features and their small dimensionality, we find that the\\napproach works well even with limited training examples, and that it is\\npossible to transport the word classifier to new domains. Experiments on four\\ndatasets (sentiment classification and toxicity detection) suggest that using\\nthis approach to inform feature selection also leads to more robust\\nclassification, as measured by improved worst-case accuracy on the samples\\naffected by spurious correlations.\\n', 'publish_date': 'Tue, 6 Oct 2020 03:49:22 GMT'}\n",
    "ID: 1710.09085, Distance: 0.4185680150985718, Content: {'title': 'Re-evaluating the need for Modelling Term-Dependence in Text\\n  Classification Problems', 'doi': None, 'categories': 'cs.IR cs.CL', 'abstract': '  A substantial amount of research has been carried out in developing machine\\nlearning algorithms that account for term dependence in text classification.\\nThese algorithms offer acceptable performance in most cases but they are\\nassociated with a substantial cost. They require significantly greater\\nresources to operate. This paper argues against the justification of the higher\\ncosts of these algorithms, based on their performance in text classification\\nproblems. In order to prove the conjecture, the performance of one of the best\\ndependence models is compared to several well established algorithms in text\\nclassification. A very specific collection of datasets have been designed,\\nwhich would best reflect the disparity in the nature of text data, that are\\npresent in real world applications. The results show that even one of the best\\nterm dependence models, performs decent at best when compared to other\\nindependence models. Coupled with their substantially greater requirement for\\nhardware resources for operation, this makes them an impractical choice for\\nbeing used in real world scenarios.\\n', 'publish_date': 'Wed, 25 Oct 2017 06:26:28 GMT'}\n",
    "ID: 2005.09198, Distance: 0.4193064868450165, Content: {'title': 'Quantifying the Uncertainty of Precision Estimates for Rule based Text\\n  Classifiers', 'doi': None, 'categories': 'cs.LG cs.AI cs.CL stat.ML', 'abstract': \"  Rule based classifiers that use the presence and absence of key sub-strings\\nto make classification decisions have a natural mechanism for quantifying the\\nuncertainty of their precision. For a binary classifier, the key insight is to\\ntreat partitions of the sub-string set induced by the documents as Bernoulli\\nrandom variables. The mean value of each random variable is an estimate of the\\nclassifier's precision when presented with a document inducing that partition.\\nThese means can be compared, using standard statistical tests, to a desired or\\nexpected classifier precision. A set of binary classifiers can be combined into\\na single, multi-label classifier by an application of the Dempster-Shafer\\ntheory of evidence. The utility of this approach is demonstrated with a\\nbenchmark problem.\\n\", 'publish_date': 'Tue, 19 May 2020 03:51:47 GMT'}\n",
    "\n",
    "Answer:\n",
    "\n",
    "==================================================\n",
    "生成嵌入中: 100%|██████████| 1/1 [00:00<00:00, 11.25it/s]\n",
    "[INFO] Query vector shape: (1, 1, 768)\n",
    "\n",
    "\n",
    "cost 0.1599 seconds\n",
    "\n",
    "\n",
    "You are a professional academic assistant specializing in the cs.CL domain. Strictly based on the provided literature information, answer the user's query. You may slightly expand and refine the response using existing knowledge, but avoid fabricating information. The response language should match the user's query language (English for English queries, Chinese for Chinese queries). Ensure the response is concise and limited to 500 words. Include a citation with the corresponding literature link (like [text](f\"https://arxiv.org/abs/{{id}}\")) and adjust the response style based on the temperature parameter (from 0.0 to 1.0). If the temperature is low (e.g., 0.2), provide a strict and precise answer. If the temperature is high (e.g., 0.8), provide a more creative and exploratory answer.\n",
    "\n",
    "Temperature: 0.2\n",
    "Question: 什么是知识图谱，如何对文本数据构建知识图谱？\n",
    "Relevant Literature: \n",
    "ID: 2211.10511, Distance: 0.34266549348831177, Content: {'title': 'Knowledge Graph Generation From Text', 'doi': None, 'categories': 'cs.CL cs.LG', 'abstract': '  In this work we propose a novel end-to-end multi-stage Knowledge Graph (KG)\\ngeneration system from textual inputs, separating the overall process into two\\nstages. The graph nodes are generated first using pretrained language model,\\nfollowed by a simple edge construction head, enabling efficient KG extraction\\nfrom the text. For each stage we consider several architectural choices that\\ncan be used depending on the available training resources. We evaluated the\\nmodel on a recent WebNLG 2020 Challenge dataset, matching the state-of-the-art\\nperformance on text-to-RDF generation task, as well as on New York Times (NYT)\\nand a large-scale TekGen datasets, showing strong overall performance,\\noutperforming the existing baselines. We believe that the proposed system can\\nserve as a viable KG construction alternative to the existing linearization or\\nsampling-based graph generation approaches. Our code can be found at\\nhttps://github.com/IBM/Grapher\\n', 'publish_date': 'Fri, 18 Nov 2022 21:27:13 GMT'}\n",
    "ID: 2401.07683, Distance: 0.35198739171028137, Content: {'title': 'Assisted Knowledge Graph Authoring: Human-Supervised Knowledge Graph\\n  Construction from Natural Language', 'doi': '10.1145/3627508.3638340', 'categories': 'cs.CL', 'abstract': '  Encyclopedic knowledge graphs, such as Wikidata, host an extensive repository\\nof millions of knowledge statements. However, domain-specific knowledge from\\nfields such as history, physics, or medicine is significantly underrepresented\\nin those graphs. Although few domain-specific knowledge graphs exist (e.g.,\\nPubmed for medicine), developing specialized retrieval applications for many\\ndomains still requires constructing knowledge graphs from scratch. To\\nfacilitate knowledge graph construction, we introduce WAKA: a Web application\\nthat allows domain experts to create knowledge graphs through the medium with\\nwhich they are most familiar: natural language.\\n', 'publish_date': 'Mon, 15 Jan 2024 13:51:00 GMT'}\n",
    "ID: 2409.03284, Distance: 0.3576383888721466, Content: {'title': 'iText2KG: Incremental Knowledge Graphs Construction Using Large Language\\n  Models', 'doi': None, 'categories': 'cs.AI cs.CL cs.IR', 'abstract': \"  Most available data is unstructured, making it challenging to access valuable\\ninformation. Automatically building Knowledge Graphs (KGs) is crucial for\\nstructuring data and making it accessible, allowing users to search for\\ninformation effectively. KGs also facilitate insights, inference, and\\nreasoning. Traditional NLP methods, such as named entity recognition and\\nrelation extraction, are key in information retrieval but face limitations,\\nincluding the use of predefined entity types and the need for supervised\\nlearning. Current research leverages large language models' capabilities, such\\nas zero- or few-shot learning. However, unresolved and semantically duplicated\\nentities and relations still pose challenges, leading to inconsistent graphs\\nand requiring extensive post-processing. Additionally, most approaches are\\ntopic-dependent. In this paper, we propose iText2KG, a method for incremental,\\ntopic-independent KG construction without post-processing. This plug-and-play,\\nzero-shot method is applicable across a wide range of KG construction scenarios\\nand comprises four modules: Document Distiller, Incremental Entity Extractor,\\nIncremental Relation Extractor, and Graph Integrator and Visualization. Our\\nmethod demonstrates superior performance compared to baseline methods across\\nthree scenarios: converting scientific papers to graphs, websites to graphs,\\nand CVs to graphs.\\n\", 'publish_date': 'Thu, 5 Sep 2024 06:49:14 GMT'}\n",
    "ID: 2101.06111, Distance: 0.35831788182258606, Content: {'title': 'Knowledge Graphs and Natural-Language Processing', 'doi': '10.1007/978-3-030-48099-8', 'categories': 'cs.CY cs.CL', 'abstract': '  Emergency-relevant data comes in many varieties. It can be high volume and\\nhigh velocity, and reaction times are critical, calling for efficient and\\npowerful techniques for data analysis and management. Knowledge graphs\\nrepresent data in a rich, flexible, and uniform way that is well matched with\\nthe needs of emergency management. They build on existing standards, resources,\\ntechniques, and tools for semantic data and computing. This chapter explains\\nthe most important semantic technologies and how they support knowledge graphs.\\nWe proceed to discuss their benefits and challenges and give examples of\\nrelevant semantic data sources and vocabularies. Natural-language texts -- in\\nparticular those collected from social media such as Twitter -- is a type of\\ndata source that poses particular analysis challenges. We therefore include an\\noverview of techniques for processing natural-language texts.\\n', 'publish_date': 'Tue, 15 Dec 2020 16:53:28 GMT'}\n",
    "ID: 2106.01167, Distance: 0.36182737350463867, Content: {'title': 'End-to-End NLP Knowledge Graph Construction', 'doi': None, 'categories': 'cs.CL', 'abstract': '  This paper studies the end-to-end construction of an NLP Knowledge Graph (KG)\\nfrom scientific papers. We focus on extracting four types of relations:\\nevaluatedOn between tasks and datasets, evaluatedBy between tasks and\\nevaluation metrics, as well as coreferent and related relations between the\\nsame type of entities. For instance, F1-score is coreferent with F-measure. We\\nintroduce novel methods for each of these relation types and apply our final\\nframework (SciNLP-KG) to 30,000 NLP papers from ACL Anthology to build a\\nlarge-scale KG, which can facilitate automatically constructing scientific\\nleaderboards for the NLP community. The results of our experiments indicate\\nthat the resulting KG contains high-quality information.\\n', 'publish_date': 'Wed, 2 Jun 2021 14:03:06 GMT'}\n",
    "\n",
    "Answer:\n",
    "\n",
    "==================================================\n",
    "avg time: 0.1504 seconds\"\"\"\n",
    "\n",
    "tokens = tokenizer.encode(input_str)\n",
    "print(f\"经过切分后共有{len(tokens)}个token\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
