{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "import os\n",
    "import json\n",
    "import faiss  # pip install faiss-cpu\n",
    "\n",
    "from bge import BGE\n",
    "\n",
    "full_dataset = \"./arxiv/arxiv-metadata-oai-snapshot.json\"\n",
    "selected_dataset = \"./arxiv/arxiv-metadata-cscl.json\"\n",
    "CATEGORY2IDS = \"./datasets/arxiv_category2ids.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 下载完整元数据并节选出cs.CL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download latest version  https://www.kaggle.com/api/v1/datasets/download/Cornell-University/arxiv?dataset_version_number=233\n",
    "# or > pip install kagglehub\n",
    "import kagglehub\n",
    "\n",
    "path = kagglehub.dataset_download(\"Cornell-University/arxiv\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)\n",
    "\n",
    "dest_path = \"./arxiv\"\n",
    "os.makedirs(dest_path, exist_ok=True)\n",
    "for file_name in os.listdir(path):\n",
    "    shutil.move(os.path.join(path, file_name), dest_path)\n",
    "\n",
    "print(\"Dataset files moved to:\", dest_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 筛选出包含指定分类的元数据\n",
    "def select(file_path, output_path, target=\"cs.CL\"):\n",
    "    selected_data = []\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        # kagglehub 数据集默认 jsonl\n",
    "        for line in file:\n",
    "            record = json.loads(line.strip())\n",
    "            if target in record.get(\"categories\", \"\"):\n",
    "                selected_data.append(record)\n",
    "\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as output_file:\n",
    "        json.dump(selected_data, output_file, ensure_ascii=False)\n",
    "\n",
    "    print(f\"共找到 {len(selected_data)} 条记录，已保存到 {output_path}\")\n",
    "\n",
    "    return selected_data\n",
    "\n",
    "\n",
    "# 筛选 cs.CL\n",
    "selected_data = select(full_dataset, selected_dataset, \"cs.CL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 对cs.CL元数据作进一步处理，分出过滤表与向量表\n",
    "\n",
    "前者（需要附带向量索引）负责进行如领域筛选（预处理阶段直接做好，多做点冗余分表）、年份范围（预排序，一次`lower_bound`二分，然后取到上界为止）筛选的工作；后者负责应对相似性搜索（Top 2N），得到的结果重新映射回过滤表，并剔除不在候选集中的部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lower_bound: 2\n"
     ]
    }
   ],
   "source": [
    "import bisect\n",
    "\n",
    "def lower_bound(sorted_list, value):\n",
    "    return bisect.bisect_left(sorted_list, value)\n",
    "def upper_bound(sorted_list, value):\n",
    "    return bisect.bisect_right(sorted_list, value)\n",
    "\n",
    "print(\"lower_bound:\", lower_bound([1, 2, 4, 5], 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "总共包含`142`种细分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(selected_dataset, \"r\", encoding=\"utf-8\") as file:\n",
    "    selected_data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': 'Natural Language Processing (almost) from Scratch', 'doi': None, 'categories': 'cs.LG cs.CL', 'abstract': '  We propose a unified neural network architecture and learning algorithm that\\ncan be applied to various natural language processing tasks including:\\npart-of-speech tagging, chunking, named entity recognition, and semantic role\\nlabeling. This versatility is achieved by trying to avoid task-specific\\nengineering and therefore disregarding a lot of prior knowledge. Instead of\\nexploiting man-made input features carefully optimized for each task, our\\nsystem learns internal representations on the basis of vast amounts of mostly\\nunlabeled training data. This work is then used as a basis for building a\\nfreely available tagging system with good performance and minimal computational\\nrequirements.\\n', 'publish_date': 'Wed, 2 Mar 2011 11:34:50 GMT'}\n"
     ]
    }
   ],
   "source": [
    "extracted_data = {}\n",
    "for line in selected_data:\n",
    "    extracted_data[line[\"id\"]] = {\n",
    "        \"title\": line.get(\"title\"),\n",
    "        \"doi\": line.get(\"doi\"),\n",
    "        \"categories\": line.get(\"categories\"),\n",
    "        \"abstract\": line.get(\"abstract\"),\n",
    "        \"publish_date\": next(\n",
    "            (\n",
    "                version[\"created\"]\n",
    "                for version in line.get(\"versions\", [])\n",
    "                if version[\"version\"] == \"v1\"\n",
    "            ),\n",
    "            None,\n",
    "        ),\n",
    "    }\n",
    "\n",
    "with open(\"./datasets/simplify_data.json\", \"w\", encoding=\"utf-8\") as output_file:\n",
    "    json.dump(extracted_data, output_file,indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "# 时间戳到 ID\n",
    "date_to_ids = defaultdict(list)\n",
    "for line in selected_data:\n",
    "    publish_date = next(\n",
    "        (\n",
    "            version[\"created\"]\n",
    "            for version in line.get(\"versions\", [])\n",
    "            if version[\"version\"] == \"v1\"\n",
    "        ),\n",
    "        None,\n",
    "    )\n",
    "    if publish_date:\n",
    "        timestamp = datetime.strptime(publish_date, \"%a, %d %b %Y %H:%M:%S %Z\").timestamp()\n",
    "        date_to_ids[timestamp].append(line[\"id\"])\n",
    "\n",
    "date_to_ids = dict(sorted(date_to_ids.items()))\n",
    "with open(\"./datasets/date_to_ids.json\", \"w\", encoding=\"utf-8\") as output_file:\n",
    "    json.dump(date_to_ids, output_file, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': 'Resolution of Verb Ellipsis in Japanese Sentence using Surface\\n  Expressions and Examples', 'doi': None, 'categories': 'cs.CL', 'abstract': '  Verbs are sometimes omitted in Japanese sentences. It is necessary to recover\\nomitted verbs for purposes of language understanding, machine translation, and\\nconversational processing. This paper describes a practical way to recover\\nomitted verbs by using surface expressions and examples. We experimented the\\nresolution of verb ellipses by using this information, and obtained a recall\\nrate of 73% and a precision rate of 66% on test sentences.\\n', 'publish_date': 'Mon, 13 Dec 1999 05:19:46 GMT'}\n"
     ]
    }
   ],
   "source": [
    "with open(\"./datasets/simplify_data.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "    extracted_data = json.load(file)\n",
    "    \n",
    "print(extracted_data[\"cs/9912006\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adap-org: ['cs/9902027']\n",
      "physics.class-ph: ['physics/0307117']\n",
      "处理正确：True\n"
     ]
    }
   ],
   "source": [
    "# 分类到 id\n",
    "category_to_ids = {}\n",
    "for record in selected_data:\n",
    "    categories = record.get(\"categories\", \"\").split()\n",
    "    doc_id = record.get(\"id\")\n",
    "\n",
    "    # 特殊处理 cs.CL 分类：仅包含单领域文献\n",
    "    if \"cs.CL\" in categories and len(categories) == 1:\n",
    "        if \"cs.CL\" not in category_to_ids:\n",
    "            category_to_ids[\"cs.CL\"] = []\n",
    "        category_to_ids[\"cs.CL\"].append(doc_id)\n",
    "        continue\n",
    "    \n",
    "    # 其他分类\n",
    "    for category in categories:\n",
    "        if category == \"cs.CL\" and len(categories) > 1:\n",
    "            continue  # 跳过跨领域文献的 cs.CL 标签\n",
    "        if category not in category_to_ids:\n",
    "            category_to_ids[category] = []\n",
    "        category_to_ids[category].append(doc_id)\n",
    "\n",
    "\n",
    "sorted_category_to_ids = dict(\n",
    "    sorted(category_to_ids.items(), key=lambda item: len(item[1]), reverse=True)\n",
    ")\n",
    "\n",
    "for i, (category, ids) in enumerate(list(category_to_ids.items())[-2:]):\n",
    "    print(f\"{category}: {ids}\")\n",
    "\n",
    "unique_ids = set()\n",
    "for ids in category_to_ids.values():\n",
    "    unique_ids.update(ids)\n",
    "print(f\"处理正确：{len(selected_data)==len(unique_ids)}\")\n",
    "\n",
    "os.makedirs(os.path.dirname(CATEGORY2IDS), exist_ok=True)\n",
    "with open(CATEGORY2IDS, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(sorted_category_to_ids, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(records):\n",
    "    texts, ids = [], []\n",
    "    for rec in records:\n",
    "        ids.append(rec[\"id\"])\n",
    "        \n",
    "        # 拼接文本（跨领域和发表时间放在外面）\n",
    "        text = (\n",
    "            f\"title: {rec['title'].strip()}\\n\"\n",
    "            f\"authors: {rec['authors'].strip()}\\n\"            \n",
    "            f\"abstract: {rec['abstract'].strip()}\\n\"\n",
    "        )\n",
    "        texts.append(text)\n",
    "    return ids, texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%git clone https://huggingface.co/BAAI/bge-base-en-v1.5 ./models/bge-base-en-v1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Preprocessing complete. Number of records: 83334\n",
      "[INFO] FAISS index and ID map saved.\n",
      "\n",
      "ID: 0704.2083\n",
      "Text:\n",
      "title: Introduction to Arabic Speech Recognition Using CMUSphinx System\n",
      "authors: H. Satori, M. Harti and N. Chenfour\n",
      "abstract: In this paper Arabic was investigated from the speech recognition problem\n",
      "point of view. We propose a novel approach to build an Arabic Automated Speech\n",
      "Recognition System (ASR). This system is based on the open source CMU Sphinx-4,\n",
      "from the Carnegie Mellon University. CMU Sphinx is a large-vocabulary;\n",
      "speaker-independent, continuous speech recognition system based on discrete\n",
      "Hidden Markov Models (HMMs). We build a model using utilities from the\n",
      "OpenSource CMU Sphinx. We will demonstrate the possible adaptability of this\n",
      "system to Arabic voice recognition.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ids, texts = preprocess(selected_data)\n",
    "print(\"[INFO] Preprocessing complete. Number of records:\", len(texts))\n",
    "# 保存 ID 列表以便检索结果映射\n",
    "with open(\"./datasets/bge_id_map_cscl.json\", \"w\") as f:\n",
    "    json.dump(ids, f)\n",
    "print(\"[INFO] FAISS index and ID map saved.\")\n",
    "\n",
    "for id, text in list(zip(ids, texts))[:1]:\n",
    "    print(f\"\\nID: {id}\\nText:\\n{text}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "生成嵌入中: 100%|██████████| 83334/83334 [4:36:33<00:00,  5.02it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Text embedding complete. Number of vectors: 83334\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "bge = BGE()\n",
    "vectors = bge.embed_texts(texts)\n",
    "np.save(\"./datasets/bge_vectors_cscl.npy\", np.vstack(vectors))\n",
    "print(\"[INFO] Text embedding complete. Number of vectors:\", len(vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Vectors loaded. Shape: (83334, 768)\n"
     ]
    }
   ],
   "source": [
    "vectors = np.array(np.load(\"./datasets/bge_vectors_cscl.npy\"), dtype=np.float32)\n",
    "print(\"[INFO] Vectors loaded. Shape:\", vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IVF_PQ 索引   PQ加速检索同时减少内存占用（略微降低一些准确性）\n",
    "d = vectors.shape[1]\n",
    "quantizer = faiss.IndexFlatL2(d)\n",
    "idx = faiss.IndexIVFPQ(quantizer, d, 300, 64, 16)\n",
    "idx.train(vectors)\n",
    "idx.add(vectors)\n",
    "\n",
    "faiss.write_index(idx, \"./datasets/faiss_cscl_ivfpq.idx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
